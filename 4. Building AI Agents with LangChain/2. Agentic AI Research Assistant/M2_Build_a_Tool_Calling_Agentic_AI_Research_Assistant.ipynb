{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kiH8lf1y4sD"
      },
      "source": [
        "# Build a Tool-Calling Agentic AI Research Assistant with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYBpZTjLnEXb"
      },
      "source": [
        "This demo will cover building AI Agents with the legacy LangChain `AgentExecutor`. These are fine for getting started, but for working with more advanced agents and having more finer control, LangChain recommends to use LangGraph, which we cover in other courses.\n",
        "\n",
        "Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determines whether more actions are needed, or whether it is okay to stop.\n",
        "\n",
        "![](https://i.imgur.com/1uVnBAm.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "93aa791d-4f4c-4167-e6d1-7491fbeb93e5"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain==0.3.14\n",
        "# !pip install langchain-openai==0.3.0\n",
        "# !pip install langchain-community==0.3.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AakmY6B_zYte",
        "outputId": "f206e910-8ac4-4423-c1af-c000a92f3429"
      },
      "outputs": [],
      "source": [
        "# !pip install markitdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9c37cLnSrbg"
      },
      "source": [
        "## Enter Open AI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv3JzCEx_PAd",
        "outputId": "3e7a5d36-2bd9-49d8-88bc-5753c36d235c"
      },
      "outputs": [],
      "source": [
        "# from getpass import getpass\n",
        "\n",
        "# OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucWRRI3QztL2"
      },
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK-1WLzOrJdb",
        "outputId": "143b9b1d-9ae9-4ab1-e830-27bab3836ee2"
      },
      "outputs": [],
      "source": [
        "# TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce5arICZEEov"
      },
      "source": [
        "## Enter WeatherAPI API Key\n",
        "\n",
        "Get a free API key from [here](https://www.weatherapi.com/signup.aspx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpAMz1XgEEov",
        "outputId": "da1143c0-4243-4799-d76e-79e41dd8ea5f"
      },
      "outputs": [],
      "source": [
        "# WEATHER_API_KEY = getpass('Enter WeatherAPI API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T0s0um5Svfa"
      },
      "source": [
        "## Setup Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "# os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# LOAD ENVIRONMENT VARIABLES\n",
        "# =============================================================================\n",
        "# This loads API keys from a .env file in the project directory.\n",
        "# Your .env file should contain:\n",
        "#   OPENAI_API_KEY=your_openai_key\n",
        "#   TAVILY_API_KEY=your_tavily_key  \n",
        "#   WEATHER_API_KEY=your_openweathermap_key\n",
        "# =============================================================================\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# find_dotenv() searches for .env file in current and parent directories\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howf-v0ARWbv"
      },
      "source": [
        "## Create Tools\n",
        "\n",
        "Here we create two custom tools which are wrappers on top of the [Tavily API](https://tavily.com/#api) and [WeatherAPI](https://www.weatherapi.com/)\n",
        "\n",
        "- Web Search tool with information extraction\n",
        "- Weather tool\n",
        "\n",
        "![](https://i.imgur.com/TyPAYXE.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue8xgu9WpuPi",
        "outputId": "5f312771-4696-4ee6-9d37-6a890b73086d"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TOOL CREATION SECTION\n",
        "# =============================================================================\n",
        "# In LangChain, \"tools\" are functions that agents can use to interact with \n",
        "# external systems (APIs, databases, web search, etc.).\n",
        "# \n",
        "# The @tool decorator converts a regular Python function into a LangChain-\n",
        "# compatible tool that can be used by agents for decision-making.\n",
        "# =============================================================================\n",
        "\n",
        "# --- Import Required Libraries ---\n",
        "from langchain_core.tools import tool  # Decorator to create tools from functions\n",
        "from markitdown import MarkItDown  # Converts web content to markdown format\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults  # Web search API\n",
        "from tqdm import tqdm  # Progress bar for visual feedback during processing\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError  # Parallel processing\n",
        "import requests\n",
        "import os\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# TOOL 1: Web Search and Information Extraction Tool\n",
        "# =============================================================================\n",
        "# This tool uses Tavily API for web search and MarkItDown for content extraction\n",
        "\n",
        "# Initialize Tavily search with advanced settings\n",
        "tavily_tool = TavilySearchResults(\n",
        "    max_results=5,           # Return top 5 search results\n",
        "    search_depth='advanced', # Use advanced search for better quality results\n",
        "    include_answer=False,    # Don't include AI-generated summary\n",
        "    include_raw_content=True # Include raw HTML content from pages\n",
        ")\n",
        "\n",
        "# Configure HTTP session with browser-like headers\n",
        "# This prevents websites from blocking our requests (anti-bot protection bypass)\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "})\n",
        "\n",
        "# Initialize MarkItDown for converting web pages to readable markdown text\n",
        "md = MarkItDown(requests_session=session)\n",
        "\n",
        "@tool\n",
        "def search_web_extract_info(query: str) -> list:\n",
        "    \"\"\"\n",
        "    Search the web for a query and extract useful information from the search links.\n",
        "    \n",
        "    This tool performs two main operations:\n",
        "    1. Searches the web using Tavily API to find relevant URLs\n",
        "    2. Extracts and converts content from each URL to readable text\n",
        "    \n",
        "    Args:\n",
        "        query (str): The search query to find information about\n",
        "        \n",
        "    Returns:\n",
        "        list: A list of extracted text content from relevant web pages\n",
        "    \"\"\"\n",
        "    print('Calling web search tool')\n",
        "    \n",
        "    # Step 1: Get search results from Tavily\n",
        "    results = tavily_tool.invoke(query)\n",
        "    docs = []\n",
        "\n",
        "    def extract_content(url):\n",
        "        \"\"\"Helper function to extract and format content from a single URL.\"\"\"\n",
        "        extracted_info = md.convert(url)\n",
        "        text_title = extracted_info.title.strip()\n",
        "        text_content = extracted_info.text_content.strip()\n",
        "        return text_title + '\\n' + text_content\n",
        "    \n",
        "    # Step 2: Process URLs in parallel for faster execution\n",
        "    # ThreadPoolExecutor allows concurrent URL fetching\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for result in tqdm(results):\n",
        "            try:\n",
        "                future = executor.submit(extract_content, result['url'])\n",
        "                # Set 15-second timeout to prevent hanging on slow websites\n",
        "                content = future.result(timeout=15)\n",
        "                docs.append(content)\n",
        "            except TimeoutError:\n",
        "                print(f\"Extraction timed out for url: {result['url']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting from url: {result['url']} - {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOOL 2: Weather Information Tool  \n",
        "# =============================================================================\n",
        "# This tool fetches real-time weather data using OpenWeatherMap API\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetch current weather data for a specified location using OpenWeatherMap API.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The city name to get weather information for\n",
        "        \n",
        "    Returns:\n",
        "        dict: Weather data including temperature, humidity, wind speed, etc.\n",
        "              Returns \"Weather Data Not Found\" if the location is invalid\n",
        "    \"\"\"\n",
        "    # Construct API URL with city name and API key\n",
        "    # Note: The ',IN' suffix restricts search to India - modify as needed\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={query},IN&appid={os.getenv('WEATHER_API_KEY')}&units=metric\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    \n",
        "    # Validate response by checking if 'name' field exists\n",
        "    if data.get(\"name\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2N5192vikJR"
      },
      "source": [
        "## Test Tool Calling with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B2EFrwTpuXB"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BINDING TOOLS TO THE LLM\n",
        "# =============================================================================\n",
        "# Before creating an agent, let's understand how LLMs interact with tools.\n",
        "# The .bind_tools() method tells the LLM about available tools and their schemas.\n",
        "# This allows the LLM to decide WHEN to use a tool and WHAT arguments to pass.\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize ChatGPT with temperature=0 for deterministic responses\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Create a list of tools the agent can use\n",
        "tools = [search_web_extract_info, get_weather]\n",
        "\n",
        "# Bind tools to the LLM - this creates tool-aware version of the model\n",
        "# The LLM can now \"call\" these tools when it determines they're needed\n",
        "chatgpt_with_tools = chatgpt.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0rVVBGDpuYw",
        "outputId": "95231440-d9de-4c58-a536-e05d67e0f279"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'search_web_extract_info',\n",
              "  'args': {'query': 'Microsoft earnings call Q4 2024 details'},\n",
              "  'id': 'call_hl9dhRJt5dgIPOSvTEe4i4oO',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 1: LLM Decides to Use Web Search Tool\n",
        "# =============================================================================\n",
        "# When we ask about real-time information (like earnings calls), the LLM\n",
        "# recognizes it needs external data and decides to call the search tool.\n",
        "\n",
        "prompt = \"Get details of Microsoft's earnings call Q4 2024\"\n",
        "response = chatgpt_with_tools.invoke(prompt)\n",
        "\n",
        "# The response contains 'tool_calls' - these are the tools the LLM wants to use\n",
        "# Notice how the LLM automatically formats the query for the search tool\n",
        "response.tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qfchhGEpuaj",
        "outputId": "d4674863-91c3-44ea-8a7e-67121e208f3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'get_weather',\n",
              "  'args': {'query': 'Bangalore'},\n",
              "  'id': 'call_PlNKoxc2iB93w2stVrhVQ0yS',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 2: LLM Decides to Use Weather Tool\n",
        "# =============================================================================\n",
        "# For weather-related queries, the LLM chooses the get_weather tool instead.\n",
        "# This demonstrates intelligent tool selection based on query context.\n",
        "\n",
        "prompt = \"how is the weather in Bangalore today\"\n",
        "response = chatgpt_with_tools.invoke(prompt)\n",
        "\n",
        "# The LLM has selected get_weather and extracted \"Bangalore\" as the location\n",
        "response.tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7kvwG3SmREW"
      },
      "source": [
        "## Build and Test AI Agent\n",
        "\n",
        "Now that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent to bind the tools to the agent with a prompt. We will also add in the capability to store historical conversations as memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grNq1I6_5dxC",
        "outputId": "7353b0b3-cb8d-4324-efc1-601180a6bc7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Act as a helpful assistant.\\n                You run in a loop of Thought, Action, PAUSE, Observation.\\n                At the end of the loop, you output an Answer.\\n                Use Thought to describe your thoughts about the question you have been asked.\\n                Use Action to run one of the actions available to you - then return PAUSE.\\n                Observation will be the result of running those actions.\\n                Repeat till you get to the answer for the given user query.\\n\\n                Use the following workflow format:\\n                  Question: the input task you must solve\\n                  Thought: you should always think about what to do\\n                  Action: the action to take which can be any of the following:\\n                            - break it into smaller steps if needed\\n                            - see if you can answer the given task with your trained knowledge\\n                            - call the most relevant tools at your disposal mentioned below in case you need more information\\n                  Action Input: the input to the action\\n                  Observation: the result of the action\\n                  ... (this Thought/Action/Action Input/Observation can repeat N times)\\n                  Thought: I now know the final answer\\n                  Final Answer: the final answer to the original input question\\n\\n                Tools at your disposal to perform tasks as needed:\\n                  - get_weather: whenever user asks get the weather of a place.\\n                  - search_web_extract_info: whenever user asks for specific information or if you don't know the answer.\\n             \"), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CREATING THE AGENT PROMPT TEMPLATE (ReAct Pattern)\n",
        "# =============================================================================\n",
        "# The prompt template defines how the agent should think and behave.\n",
        "# We use the ReAct (Reasoning + Acting) pattern which structures the agent's\n",
        "# thought process into: Thought → Action → Observation → (repeat) → Answer\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define the system prompt that guides agent behavior\n",
        "# This prompt implements the ReAct pattern for structured reasoning\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant.\n",
        "                You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "                At the end of the loop, you output an Answer.\n",
        "                Use Thought to describe your thoughts about the question you have been asked.\n",
        "                Use Action to run one of the actions available to you - then return PAUSE.\n",
        "                Observation will be the result of running those actions.\n",
        "                Repeat till you get to the answer for the given user query.\n",
        "\n",
        "                Use the following workflow format:\n",
        "                  Question: the input task you must solve\n",
        "                  Thought: you should always think about what to do\n",
        "                  Action: the action to take which can be any of the following:\n",
        "                            - break it into smaller steps if needed\n",
        "                            - see if you can answer the given task with your trained knowledge\n",
        "                            - call the most relevant tools at your disposal mentioned below in case you need more information\n",
        "                  Action Input: the input to the action\n",
        "                  Observation: the result of the action\n",
        "                  ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "                  Thought: I now know the final answer\n",
        "                  Final Answer: the final answer to the original input question\n",
        "\n",
        "                Tools at your disposal to perform tasks as needed:\n",
        "                  - get_weather: whenever user asks get the weather of a place.\n",
        "                  - search_web_extract_info: whenever user asks for specific information or if you don't know the answer.\n",
        "             \"\"\"\n",
        "\n",
        "# Build the prompt template with multiple components:\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),  # System instructions for agent behavior\n",
        "        MessagesPlaceholder(variable_name=\"history\", optional=True),  # Chat history (for memory)\n",
        "        (\"human\", \"{query}\"),  # User's input query\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),  # Agent's working memory\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display the prompt structure\n",
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlJwVepDmsZw"
      },
      "source": [
        "Now, we can initalize the agent with the LLM, the prompt, and the tools.\n",
        "\n",
        "The agent is responsible for taking in input and deciding what actions to take.\n",
        "\n",
        "REMEMBER the Agent does not execute those actions - that is done by the AgentExecutor\n",
        "\n",
        "Note that we are passing in the model `chatgpt`, not `chatgpt_with_tools`.\n",
        "\n",
        "That is because `create_tool_calling_agent` will call `.bind_tools` for us under the hood.\n",
        "\n",
        "This should ideally be used with an LLM which supports tool \\ function calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4NMA82HpueH"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE THE AGENT\n",
        "# =============================================================================\n",
        "# The agent is the \"brain\" that decides what actions to take based on input.\n",
        "# It uses the LLM to reason about the problem and determine which tools to use.\n",
        "# \n",
        "# IMPORTANT: The agent itself does NOT execute tools - it only DECIDES what to do.\n",
        "# The AgentExecutor (next step) handles the actual tool execution.\n",
        "# =============================================================================\n",
        "\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "# Re-initialize the LLM (we use a fresh instance for the agent)\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Define the tools available to this agent\n",
        "tools = [search_web_extract_info, get_weather]\n",
        "\n",
        "# Create the agent by combining: LLM + Tools + Prompt Template\n",
        "# Note: We pass 'chatgpt' not 'chatgpt_with_tools' because\n",
        "# create_tool_calling_agent calls .bind_tools() internally\n",
        "agent = create_tool_calling_agent(chatgpt, tools, prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiL70NCrmkkW"
      },
      "source": [
        "Finally, we combine the `agent` (the brains) with the `tools` inside the `AgentExecutor` (which will repeatedly call the agent and execute tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAuGe5G5pugJ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE THE AGENT EXECUTOR\n",
        "# =============================================================================\n",
        "# The AgentExecutor is the runtime that:\n",
        "# 1. Takes user input and passes it to the agent\n",
        "# 2. Executes the tools that the agent decides to use\n",
        "# 3. Passes tool results back to the agent\n",
        "# 4. Repeats until the agent has a final answer\n",
        "#\n",
        "# Think of it as: Agent = Brain, AgentExecutor = Body that takes action\n",
        "# =============================================================================\n",
        "\n",
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,                      # The agent (decision maker)\n",
        "    tools=tools,                      # Available tools to execute\n",
        "    early_stopping_method='force',    # Force stop if max iterations reached\n",
        "    max_iterations=10                 # Maximum reasoning loops (prevents infinite loops)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXqgT0Yth892",
        "outputId": "6c03e40b-832c-44b2-9318-77cbdb3f42ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As of my last update, Nvidia's Q4 2024 earnings call has not occurred, as my data only goes up to October 2023. Therefore, I cannot provide specific details about the call. However, in a typical Nvidia earnings call, you can expect discussions on the following key points:\n",
            "\n",
            "1. **Financial Performance**: Overview of revenue, net income, and earnings per share compared to previous quarters and analyst expectations.\n",
            "\n",
            "2. **Segment Performance**: Insights into the performance of different business segments, such as gaming, data center, professional visualization, and automotive.\n",
            "\n",
            "3. **Market Trends**: Commentary on market conditions affecting Nvidia's business, including demand for GPUs, AI, and data center products.\n",
            "\n",
            "4. **Product Updates**: Announcements or updates on new products, technologies, or partnerships.\n",
            "\n",
            "5. **Guidance**: Forward-looking statements regarding expected financial performance and strategic priorities for the upcoming quarters.\n",
            "\n",
            "6. **Challenges and Risks**: Discussion of any challenges faced, such as supply chain issues, competition, or regulatory concerns.\n",
            "\n",
            "7. **Q&A Session**: Responses to questions from analysts and investors, providing further insights into Nvidia's strategy and outlook.\n",
            "\n",
            "For the most accurate and up-to-date information, I recommend checking Nvidia's official investor relations website or financial news sources following the actual earnings call.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# COMPARISON: LLM WITHOUT Agent (No Tools)\n",
        "# =============================================================================\n",
        "# First, let's see what happens when we ask the LLM directly WITHOUT using\n",
        "# the agent. The LLM can only use its training data (which has a cutoff date)\n",
        "# and cannot access real-time information.\n",
        "# =============================================================================\n",
        "\n",
        "query = \"\"\"Summarize the key points discussed in Nvidia's Q4 2024 earnings call\"\"\"\n",
        "response = chatgpt.invoke(query)\n",
        "\n",
        "# Notice: The LLM admits it doesn't have current data - it cannot search the web!\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper import for rendering markdown output nicely in Jupyter\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07nRAXEwY7Ea",
        "outputId": "00785ea5-fcad-45bb-a491-0e287c184f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling web search tool\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:00<00:01,  2.61it/s]Cannot set gray non-stroke color because /'P25' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P46' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P137' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P172' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P205' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P238' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P271' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P306' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P339' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P374' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P442' is an invalid float value\n",
            " 40%|████      | 2/5 [00:02<00:04,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://s201.q4cdn.com/141608511/files/doc_financials/2024/q4/NVDA-F4Q24-Quarterly-Presentation-FINAL.pdf - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:03<00:02,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://www.youtube.com/watch?v=hV6WxM3S80g - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:06<00:00,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://nvidianews.nvidia.com/_gallery/download_pdf/65d669a33d63329bbf62672a/ - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Nvidia's Q4 2024 earnings call highlighted several key points:\n",
              "\n",
              "1. **Financial Performance**: Nvidia reported a record quarterly revenue of $22.1 billion, a 22% increase from the previous quarter and a 265% rise from the same period a year ago. The fiscal year 2024's revenue reached $60.9 billion, up by 126%. The GAAP EPS was $4.93, up 765% year-over-year, and Non-GAAP EPS was $5.16, reflecting a 486% increase.\n",
              "\n",
              "2. **Data Center and AI**: Approximately 40% of Nvidia's data center revenue, which hit a record $18.4 billion in Q4, was attributed to AI inference. Nvidia's AI enterprise software reached an annualized revenue run rate of $1 billion, indicating potential growth in leveraging AI across various industries.\n",
              "\n",
              "3. **Stock Performance**: Following the earnings announcement, Nvidia's stock surged by more than 8% in after-hours trading, reflecting positive market sentiment and investor confidence.\n",
              "\n",
              "4. **Strategic Initiatives**: Nvidia's strategic focus on AI and accelerated computing, along with its expansive ecosystem of partnerships, positions it well for sustained growth. Despite regulatory challenges in China, Nvidia adapted by offering alternative products, mitigating potential impacts on its business.\n",
              "\n",
              "5. **China Revenue**: Nvidia's data center revenue in China declined significantly due to U.S. government export control regulations. However, China represented a single-digit percentage of data center revenue for Q4, and Nvidia has started shipping alternatives that don't require a license for the China market.\n",
              "\n",
              "Overall, Nvidia's Q4 2024 earnings call underscored its strong financial performance, strategic focus on AI, and ability to navigate regulatory challenges, positioning it for continued growth and innovation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# COMPARISON: LLM WITH Agent (Has Access to Tools)\n",
        "# =============================================================================\n",
        "# Now let's ask the SAME question using the agent. The agent will:\n",
        "# 1. Recognize it needs real-time data\n",
        "# 2. Call the search_web_extract_info tool\n",
        "# 3. Process the results and provide an accurate answer\n",
        "# =============================================================================\n",
        "\n",
        "query = \"\"\"Summarize the key points discussed in Nvidia's Q4 2024 earnings call\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "\n",
        "# The agent successfully retrieves and summarizes real information!\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "riHUt00KE_3q",
        "outputId": "a45dcd11-12bf-43c7-da55-33b01c0fa057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling web search tool\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:01<00:02,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://morethanmoore.substack.com/p/intel-2024-q4-financials - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:02<00:01,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://www.fool.com/earnings/call-transcripts/2025/01/30/intel-intc-q4-2024-earnings-call-transcript/ - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:05<00:00,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://newsroom.intel.com/corporate/intel-reports-fourth-quarter-full-year-2024-financial-results - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Intel's Q4 2024 earnings call highlighted several key points:\n",
              "\n",
              "1. **Financial Performance**: Intel reported Q4 revenue of $14.3 billion, up 7% sequentially, with a non-GAAP gross margin of 42.1%, surpassing guidance. Earnings per share (EPS) were $0.13, slightly above the expected $0.12. However, the full-year 2024 revenue was $53.1 billion, down 2.1% year-over-year, with a negative EPS of $0.13.\n",
              "\n",
              "2. **Challenges and Guidance**: Despite surpassing guidance, Intel faces increased competition in the AI PC category and is not yet significantly participating in the cloud-based AI data center market. The company reported a significant operating loss in its foundry services and expects a sequential revenue decline in Q1 2025 due to macroeconomic uncertainty and seasonality.\n",
              "\n",
              "3. **Strategic Developments**: Intel remains a leader in AI PC CPUs and plans to ship over 100 million systems by the end of 2025. The launch of Panther Lake on Intel 18A is on track for the second half of 2025. Intel Foundry Services is focusing on building a world-class foundry, aligning with US government interests.\n",
              "\n",
              "4. **Leadership Changes**: The company appointed two interim co-CEOs, David Zinsner and Michelle Johnston Holthaus, following the departure of CEO Pat Gelsinger.\n",
              "\n",
              "5. **Product and Market Strategy**: Intel is focusing on its Jaguar Shores product for the AI data center market and has decided not to sell its Falcon Shores AI processor for servers, using it instead as a test chip.\n",
              "\n",
              "Overall, while Intel exceeded expectations for Q4 2024, it faces ongoing challenges in market competition and strategic execution."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST: Another Web Search Query\n",
        "# =============================================================================\n",
        "# Let's test with another company to verify the agent works consistently\n",
        "\n",
        "query = \"\"\"Summarize the key points discussed in Intel's Q4 2024 earnings call\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "GfBHtCzJFKdb",
        "outputId": "8bc3f413-9adf-48a7-88c5-6ea25be31108"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To provide an accurate assessment of a company's future outlook, I would need to know which specific companies you are interested in comparing. Please provide the names of the companies you want to evaluate, and I can help gather relevant information to assess their future prospects."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DEMONSTRATING THE MEMORY LIMITATION\n",
        "# =============================================================================\n",
        "# This query asks \"which company\" - referring to previous queries about\n",
        "# Nvidia and Intel. However, the agent has NO MEMORY of previous conversations!\n",
        "# It will ask for clarification because it doesn't remember the context.\n",
        "# (We'll address this limitation with session-based memory in future lessons)\n",
        "\n",
        "query = \"\"\"which company's future outlook looks to be better?\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "O1-HaMLoZAwO",
        "outputId": "88218ffb-eb4e-45f3-c530-68cbeb0e2f03"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The weather in Bangalore today is as follows:\n",
              "\n",
              "- **Condition**: Mist\n",
              "- **Temperature**: 14.84°C\n",
              "- **Feels Like**: 14.76°C\n",
              "- **Minimum Temperature**: 14.45°C\n",
              "- **Maximum Temperature**: 15.28°C\n",
              "- **Pressure**: 1015 hPa\n",
              "- **Humidity**: 91%\n",
              "- **Visibility**: 1800 meters\n",
              "- **Wind Speed**: 5.36 m/s, coming from 74° with gusts up to 12.96 m/s\n",
              "- **Cloudiness**: 20%\n",
              "\n",
              "Sunrise is at 05:43 AM and sunset will be at 05:51 PM local time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST: Weather Tool - Indian City (Bangalore)\n",
        "# =============================================================================\n",
        "# The agent should recognize this as a weather query and use the get_weather tool.\n",
        "# Our weather tool is configured for Indian cities (has ,IN suffix in API call).\n",
        "\n",
        "query = \"\"\"how is the weather in Bangalore today? show detailed statistics\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "2GosuSIoZrZm",
        "outputId": "a5212dd7-68f9-4820-934c-48a47a823c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling web search tool\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:00<00:01,  3.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://en.climate-data.org/asia/united-arab-emirates/dubai/dubai-705/t/december-12/ - 499 Client Error: <none> for url: https://en.climate-data.org/asia/united-arab-emirates/dubai/dubai-705/t/december-12/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:15<00:26,  8.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction timed out for url: https://www.makemytrip.com/tripideas/dubai/dubai-weather-in-december\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:16<00:10,  5.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://www.khaleejtimes.com/uae/weather/unstable-weather-heavy-rains-full-forecast-december-2025 - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:18<00:03,  3.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://www.khaleejtimes.com/uae/weather/december-13-2025 - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:18<00:00,  3.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error extracting from url: https://gulfnews.com/uae/weather/uae-authorities-warn-of-intense-rain-and-rough-seas-1.500378825 - 'NoneType' object has no attribute 'strip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I was unable to retrieve the current weather details for Dubai. You might want to check a reliable weather website or app for the most up-to-date information."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST: Weather Tool - Non-Indian City (Dubai) - LIMITATION DEMO\n",
        "# =============================================================================\n",
        "# Dubai is NOT in India, so our weather API (configured with ,IN suffix) won't\n",
        "# find it. The agent will fall back to web search, which may or may not work.\n",
        "# This demonstrates the importance of proper tool configuration!\n",
        "\n",
        "query = \"\"\"how is the weather in Dubai today? show detailed statistics\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "ElF1npM4ZvAo",
        "outputId": "0414a0b5-51dc-4183-813b-a6a37701fc6a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To determine which city is hotter, I need to know the names of the cities you are comparing. Could you please provide the names of the cities?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MEMORY LIMITATION DEMO (Again)\n",
        "# =============================================================================\n",
        "# Again, the agent doesn't remember previous queries about Bangalore and Dubai.\n",
        "# It will ask for clarification about which cities we're comparing.\n",
        "\n",
        "query = \"\"\"which city is hotter?\"\"\"\n",
        "response = agent_executor.invoke({\"query\": query})\n",
        "display(Markdown(response['output']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQQAx3B7xrl-"
      },
      "source": [
        "The agent is doing pretty well but unfortunately it doesn't remember conversations. We will use some user-session based memory to store this and dive deeper into this in the next video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Alternative Approach - Manual Tool Selection with LLMChain\n",
        "\n",
        "The following section demonstrates an **alternative approach** to building agents using manual LLMChain-based tool selection. This approach gives you more control over the tool selection process but requires more code.\n",
        "\n",
        "**Key Differences from AgentExecutor:**\n",
        "- More explicit control over the tool selection logic\n",
        "- Uses separate chains for: tool selection → input formatting → response generation\n",
        "- Good for understanding how agents work under the hood\n",
        "\n",
        "> ⚠️ **Note:** This is a simplified educational example using mock tools. The first example uses real API tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "tools = [search_web_extract_info, get_weather]\n",
        "\n",
        "# Create an LLM chain for deciding which tool to use\n",
        "tool_selection_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a tool selection assistant. \n",
        "    Based on the user's question, determine which tool would be most appropriate.\n",
        "    Respond with only the tool name: \"search_web_extract_info\", \"get_weather\" .\n",
        "    If none of these tools can help, respond with \"None\".\"\"\"),\n",
        "    (\"human\", \"{query}\")\n",
        "])\n",
        "\n",
        "tool_selection_chain = LLMChain(\n",
        "    llm=chatgpt,\n",
        "    prompt=tool_selection_prompt,\n",
        "    output_key=\"selected_tool\"\n",
        ")\n",
        "\n",
        "# Create an LLM chain for formatting tool inputs\n",
        "tool_input_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Extract the specific input needed for the tool from the user's query.\n",
        "    For WeatherService: Extract the location name.\n",
        "    for get_weather extract the keywords and search the info in web\n",
        "    Provide only the extracted information, nothing else.\"\"\"),\n",
        "    (\"human\", \"Tool to use: {selected_tool}\\nUser query: {query}\")\n",
        "])\n",
        "\n",
        "tool_input_chain = LLMChain(\n",
        "    llm=chatgpt,\n",
        "    prompt=tool_input_prompt,\n",
        "    output_key=\"tool_input\"\n",
        ")\n",
        "\n",
        "# Create an LLM chain for formatting the final response\n",
        "response_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Create a helpful response to the user's query using the tool's output.\"),\n",
        "    (\"human\", \"User query: {query}\\nTool used: {selected_tool}\\nTool output: {tool_output}\")\n",
        "])\n",
        "\n",
        "response_chain = LLMChain(\n",
        "    llm=chatgpt,\n",
        "    prompt=response_prompt,\n",
        "    output_key=\"response\"\n",
        ")\n",
        "\n",
        "# Function to run the full chain\n",
        "def process_query(query: str) -> str:\n",
        "    # First, select which tool to use\n",
        "    tool_selection_result = tool_selection_chain.invoke({\"query\": query})\n",
        "    selected_tool = tool_selection_result[\"selected_tool\"].strip()\n",
        "    print(f\"Selected tool: {selected_tool}\")\n",
        "    \n",
        "    if selected_tool == \"None\":\n",
        "        return \"I don't have a tool that can help with this query.\"\n",
        "    \n",
        "    # Next, format the input for the selected tool\n",
        "    tool_input_result = tool_input_chain.invoke({\n",
        "        \"selected_tool\": selected_tool,\n",
        "        \"query\": query\n",
        "    })\n",
        "    tool_input = tool_input_result[\"tool_input\"].strip()\n",
        "    print(f\"Tool input: {tool_input}\")\n",
        "    \n",
        "    # Find and run the appropriate tool\n",
        "    tool_output = \"Tool not found\"\n",
        "    for tool in tools:\n",
        "        if tool.name == selected_tool:\n",
        "            tool_output = tool.func(tool_input)\n",
        "            break\n",
        "    print(f\"Tool output: {tool_output}\")\n",
        "    \n",
        "    # Format the final response\n",
        "    response_result = response_chain.invoke({\n",
        "        \"query\": query,\n",
        "        \"selected_tool\": selected_tool,\n",
        "        \"tool_output\": tool_output\n",
        "    })\n",
        "    return response_result[\"response\"]\n",
        "\n",
        "examples = [\n",
        "        \"What is 234 * 78.5?\",\n",
        "        \"What's the weather like in Bangalore?\",\n",
        "        \"Tell me about LangChain.\",\n",
        "    ]\n",
        "    \n",
        "for example in examples:\n",
        "    print(f\"\\n\\nQUERY: {example}\")\n",
        "    print(\"-\" * 50)\n",
        "    response = process_query(example)\n",
        "    print(\"FINAL RESPONSE:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ALTERNATIVE APPROACH: Manual Tool Selection with LLMChain\n",
        "# =============================================================================\n",
        "# This example demonstrates how to build a simple agent-like system from scratch\n",
        "# using LLMChain instead of the built-in AgentExecutor.\n",
        "#\n",
        "# WORKFLOW:\n",
        "# 1. Tool Selection Chain → LLM decides which tool to use\n",
        "# 2. Tool Input Chain → LLM extracts the right input for the tool\n",
        "# 3. Tool Execution → Run the selected tool\n",
        "# 4. Response Chain → LLM formats the final response\n",
        "#\n",
        "# This approach gives you fine-grained control but requires more code.\n",
        "# Use AgentExecutor for production; use this for learning!\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Define Mock Tool Functions\n",
        "# =============================================================================\n",
        "# These are simple mock implementations for demonstration purposes\n",
        "\n",
        "def calculator(expression: str) -> float:\n",
        "    \"\"\"Evaluate a mathematical expression safely.\"\"\"\n",
        "    try:\n",
        "        return eval(expression)  # Note: eval() is unsafe in production!\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def get_weather_mock(location: str) -> str:\n",
        "    \"\"\"Mock weather function - returns fake data for demo.\"\"\"\n",
        "    return f\"The weather in {location} is sunny with a temperature of 72°F.\"\n",
        "\n",
        "def search_database(query: str) -> str:\n",
        "    \"\"\"Search a mock database for information.\"\"\"\n",
        "    # Simple key-value database for demonstration\n",
        "    database = {\n",
        "        \"python\": \"Python is a high-level programming language.\",\n",
        "        \"langchain\": \"LangChain is a framework for LLM applications.\",\n",
        "        \"llm\": \"LLM stands for Large Language Model.\"\n",
        "    }\n",
        "    query = query.lower()\n",
        "    return database.get(query, f\"No information found for '{query}'.\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Create Tool Objects\n",
        "# =============================================================================\n",
        "# Wrap our functions in LangChain Tool objects with names and descriptions\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=calculator,\n",
        "        description=\"Useful for performing mathematical calculations\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"WeatherService\",\n",
        "        func=get_weather_mock,\n",
        "        description=\"Get current weather for a location\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Database\",\n",
        "        func=search_database,\n",
        "        description=\"Search the database for information\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Initialize the LLM\n",
        "# =============================================================================\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Create the Tool Selection Chain\n",
        "# =============================================================================\n",
        "# This chain asks the LLM to decide which tool is most appropriate\n",
        "\n",
        "tool_selection_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a tool selection assistant. \n",
        "    Based on the user's question, determine which tool would be most appropriate.\n",
        "    Respond with only the tool name: \"Calculator\", \"WeatherService\", or \"Database\".\n",
        "    If none of these tools can help, respond with \"None\".\"\"\"),\n",
        "    (\"human\", \"{query}\")\n",
        "])\n",
        "\n",
        "tool_selection_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=tool_selection_prompt,\n",
        "    output_key=\"selected_tool\"\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Create the Tool Input Extraction Chain\n",
        "# =============================================================================\n",
        "# This chain extracts the specific input needed for the selected tool\n",
        "\n",
        "tool_input_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Extract the specific input needed for the tool from the user's query.\n",
        "    For Calculator: Extract the mathematical expression.\n",
        "    For WeatherService: Extract the location name.\n",
        "    For Database: Extract the search query.\n",
        "    Provide only the extracted information, nothing else.\"\"\"),\n",
        "    (\"human\", \"Tool to use: {selected_tool}\\nUser query: {query}\")\n",
        "])\n",
        "\n",
        "tool_input_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=tool_input_prompt,\n",
        "    output_key=\"tool_input\"\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Create the Response Formatting Chain\n",
        "# =============================================================================\n",
        "# This chain takes the tool output and creates a user-friendly response\n",
        "\n",
        "response_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Create a helpful response to the user's query using the tool's output.\"),\n",
        "    (\"human\", \"User query: {query}\\nTool used: {selected_tool}\\nTool output: {tool_output}\")\n",
        "])\n",
        "\n",
        "response_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=response_prompt,\n",
        "    output_key=\"response\"\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Create the Main Processing Function\n",
        "# =============================================================================\n",
        "# This function orchestrates the entire workflow, similar to what AgentExecutor\n",
        "# does automatically. This manual approach gives you visibility into each step.\n",
        "\n",
        "def process_query(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Process a user query using the manual tool selection approach.\n",
        "    \n",
        "    Steps:\n",
        "    1. Select appropriate tool\n",
        "    2. Extract tool input from query\n",
        "    3. Execute the tool\n",
        "    4. Format and return response\n",
        "    \"\"\"\n",
        "    \n",
        "    # STEP 7a: Ask LLM to select which tool to use\n",
        "    tool_selection_result = tool_selection_chain.invoke({\"query\": query})\n",
        "    selected_tool = tool_selection_result[\"selected_tool\"].strip()\n",
        "    print(f\"Selected tool: {selected_tool}\")\n",
        "    \n",
        "    # Handle case when no tool is applicable\n",
        "    if selected_tool == \"None\":\n",
        "        return \"I don't have a tool that can help with this query.\"\n",
        "    \n",
        "    # STEP 7b: Ask LLM to extract the specific input for the tool\n",
        "    tool_input_result = tool_input_chain.invoke({\n",
        "        \"selected_tool\": selected_tool,\n",
        "        \"query\": query\n",
        "    })\n",
        "    tool_input = tool_input_result[\"tool_input\"].strip()\n",
        "    print(f\"Tool input: {tool_input}\")\n",
        "    \n",
        "    # STEP 7c: Execute the selected tool\n",
        "    tool_output = \"Tool not found\"\n",
        "    for tool in tools:\n",
        "        if tool.name == selected_tool:\n",
        "            tool_output = tool.func(tool_input)\n",
        "            break\n",
        "    print(f\"Tool output: {tool_output}\")\n",
        "    \n",
        "    # STEP 7d: Ask LLM to format a user-friendly response\n",
        "    response_result = response_chain.invoke({\n",
        "        \"query\": query,\n",
        "        \"selected_tool\": selected_tool,\n",
        "        \"tool_output\": tool_output\n",
        "    })\n",
        "    return response_result[\"response\"]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Test with Example Queries\n",
        "# =============================================================================\n",
        "def run_examples():\n",
        "    \"\"\"Run example queries to demonstrate the manual agent approach.\"\"\"\n",
        "    examples = [\n",
        "        \"What is 234 * 78.5?\",           # Should use Calculator\n",
        "        \"What's the weather like in San Francisco?\",  # Should use WeatherService\n",
        "        \"Tell me about LangChain.\",       # Should use Database\n",
        "        \"What's the capital of France?\"   # None of the tools can help\n",
        "    ]\n",
        "    \n",
        "    for example in examples:\n",
        "        print(f\"\\n\\nQUERY: {example}\")\n",
        "        print(\"-\" * 50)\n",
        "        response = process_query(example)\n",
        "        print(\"FINAL RESPONSE:\")\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "QUERY: What is 234 * 78.5?\n",
            "--------------------------------------------------\n",
            "Selected tool: Calculator\n",
            "Tool input: 234 * 78.5\n",
            "Tool output: 18369.0\n",
            "FINAL RESPONSE:\n",
            "The result of multiplying 234 by 78.5 is 18369.0.\n",
            "\n",
            "\n",
            "QUERY: What's the weather like in San Francisco?\n",
            "--------------------------------------------------\n",
            "Selected tool: WeatherService\n",
            "Tool input: San Francisco\n",
            "Tool output: The weather in San Francisco is sunny with a temperature of 72°F.\n",
            "FINAL RESPONSE:\n",
            "The current weather in San Francisco is sunny with a temperature of 72°F. It seems like a pleasant day to enjoy outdoor activities or explore the city. Remember to check for any updates in case the weather changes throughout the day.\n",
            "\n",
            "\n",
            "QUERY: Tell me about LangChain.\n",
            "--------------------------------------------------\n",
            "Selected tool: None\n",
            "FINAL RESPONSE:\n",
            "I don't have a tool that can help with this query.\n",
            "\n",
            "\n",
            "QUERY: What's the capital of France?\n",
            "--------------------------------------------------\n",
            "Selected tool: None\n",
            "FINAL RESPONSE:\n",
            "I don't have a tool that can help with this query.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# RUN THE EXAMPLES\n",
        "# =============================================================================\n",
        "# Execute the examples to see how the manual tool selection works.\n",
        "# Notice how the LLM correctly identifies which tool to use for each query!\n",
        "\n",
        "run_examples()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
