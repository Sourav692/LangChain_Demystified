{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# LangChain Runnables: A Comprehensive Guide\n",
    "\n",
    "## Overview\n",
    "LangChain Runnables are the fundamental building blocks that enable you to create complex, composable AI workflows. This notebook explores the core runnable types and demonstrates how to build sophisticated data processing pipelines.\n",
    "\n",
    "## What You'll Learn:\n",
    "- **RunnablePassthrough**: Preserving input context\n",
    "- **RunnableLambda**: Converting functions to runnables\n",
    "- **RunnableParallel**: Concurrent execution patterns\n",
    "- **Sequential Chaining**: Building step-by-step workflows\n",
    "- **Conditional Logic**: Dynamic routing based on content\n",
    "- **Streaming & Batch Processing**: Handling different execution modes\n",
    "\n",
    "## Key Concepts:\n",
    "- **Composability**: Combining simple runnables to create complex workflows\n",
    "- **Type Safety**: Ensuring data flows correctly between components\n",
    "- **Performance**: Parallel execution for independent operations\n",
    "- **Flexibility**: Adapting to different input types and processing needs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Initial Configuration\n",
    "\n",
    "Let's start by clearing any previous outputs and importing the essential runnable components from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK INITIALIZATION\n",
    "# ============================================================================\n",
    "# Clear any previous output from the notebook for a clean start.\n",
    "# This ensures we have a fresh execution environment without cached outputs.\n",
    "# The wait=True parameter ensures the screen clears only after new output is ready.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORE RUNNABLE IMPORTS\n",
    "# ============================================================================\n",
    "# LangChain Runnables are the fundamental building blocks for creating \n",
    "# composable AI workflows. Each runnable implements a common interface with\n",
    "# methods like invoke(), batch(), stream(), and ainvoke() (async).\n",
    "#\n",
    "# Key Runnable Types:\n",
    "# -------------------\n",
    "# 1. RunnablePassthrough: Acts as an identity function - passes input unchanged.\n",
    "#    Use case: Preserving original data while also transforming it in parallel.\n",
    "#\n",
    "# 2. RunnableLambda: Converts any Python callable (function/lambda) into a Runnable.\n",
    "#    Use case: Integrating custom business logic into LangChain pipelines.\n",
    "#\n",
    "# 3. RunnableParallel: Executes multiple runnables concurrently on the same input.\n",
    "#    Use case: Extracting multiple insights from data simultaneously.\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. RunnablePassthrough: Basic Input Handling\n",
    "\n",
    "**RunnablePassthrough** is the simplest runnable that takes an input and returns it unchanged. This is useful when you want to preserve the original input in a chain while also performing other operations on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE: Basic RunnablePassthrough Usage\n",
    "# ============================================================================\n",
    "# RunnablePassthrough is the simplest runnable - it returns its input unchanged.\n",
    "# \n",
    "# WHY IS THIS USEFUL?\n",
    "# In complex chains, you often need to:\n",
    "# 1. Pass the original input alongside transformed versions\n",
    "# 2. Preserve data for later stages in the pipeline\n",
    "# 3. Create \"branches\" where one path keeps data intact\n",
    "#\n",
    "# Think of it as a \"copy\" operation in a data pipeline.\n",
    "# ============================================================================\n",
    "\n",
    "# Create a RunnablePassthrough instance\n",
    "chain = RunnablePassthrough()\n",
    "\n",
    "# Test with a simple string - notice it returns exactly what was passed in\n",
    "# Input: \"abcd\" ‚Üí Output: \"abcd\" (unchanged)\n",
    "print(chain.invoke(\"abcd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. RunnableLambda: Converting Functions to Runnables\n",
    "\n",
    "**RunnableLambda** allows you to convert any Python function into a runnable that can be chained with other components. This is extremely powerful for integrating custom business logic into your LangChain workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE: Converting Functions to Runnables with RunnableLambda\n",
    "# ============================================================================\n",
    "# RunnableLambda wraps ANY Python function to make it work in LangChain chains.\n",
    "# This is powerful because you can integrate any custom logic into your AI pipelines.\n",
    "#\n",
    "# IMPORTANT RULE: invoke() always takes a SINGLE input argument!\n",
    "# If your function needs multiple values, you must pass them as:\n",
    "#   - A tuple: (value1, value2)\n",
    "#   - A dictionary: {\"key1\": value1, \"key2\": value2}\n",
    "#   - A custom object\n",
    "# ============================================================================\n",
    "\n",
    "# ----- Example 1: Simple single-input function -----\n",
    "def output_length(input: str):\n",
    "    \"\"\"Calculate the length of the input string.\n",
    "    \n",
    "    Args:\n",
    "        input: A string to measure\n",
    "    Returns:\n",
    "        Integer length of the string\n",
    "    \"\"\"\n",
    "    output = len(input)\n",
    "    return output\n",
    "\n",
    "# ----- Example 2: Function expecting multiple values via tuple -----\n",
    "def sum_two(inputs: tuple):\n",
    "    \"\"\"Add two numbers passed as a tuple.\n",
    "    \n",
    "    Args:\n",
    "        inputs: A tuple of (number1, number2)\n",
    "    Returns:\n",
    "        Sum of the two numbers\n",
    "    \"\"\"\n",
    "    input1, input2 = inputs  # Unpack the tuple\n",
    "    output = input1 + input2\n",
    "    return output\n",
    "\n",
    "# ----- Example 3: Function expecting multiple values via dictionary -----\n",
    "def sum_dict(inputs: dict):\n",
    "    \"\"\"Add two numbers from a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        inputs: A dictionary with keys \"a\" and \"b\"\n",
    "    Returns:\n",
    "        Sum of inputs[\"a\"] and inputs[\"b\"]\n",
    "    \"\"\"\n",
    "    return inputs[\"a\"] + inputs[\"b\"]\n",
    "\n",
    "\n",
    "# Convert each function to a runnable using RunnableLambda\n",
    "chain = RunnableLambda(output_length)           # Wraps the length function\n",
    "chain_sum = RunnableLambda(sum_two)             # Wraps the tuple-based sum\n",
    "chain_sum_dict = RunnableLambda(sum_dict)       # Wraps the dict-based sum\n",
    "\n",
    "# ----- Testing the runnables -----\n",
    "# Test 1: String length - \"input to output\" has 15 characters\n",
    "print(chain.invoke(\"input to output\"))  # Output: 15\n",
    "\n",
    "# Test 2: Sum via tuple - invoke() takes ONE argument, so we pass a tuple\n",
    "print(chain_sum.invoke((1, 2)))  # Output: 3\n",
    "\n",
    "# Test 3: Sum via dictionary - another way to pass multiple values\n",
    "print(chain_sum_dict.invoke({\"a\": 1, \"b\": 2}))  # Output: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATTERN: Dictionary Input for Clean Multi-Value Handling\n",
    "# ============================================================================\n",
    "# Using dictionaries as input is a common pattern in LangChain because:\n",
    "# 1. Keys make the code self-documenting (you know what each value means)\n",
    "# 2. PromptTemplates use dictionaries to fill in template variables\n",
    "# 3. RunnableParallel outputs dictionaries, making chains composable\n",
    "# ============================================================================\n",
    "\n",
    "def add_dict(item: dict):\n",
    "    \"\"\"Add two numbers from a dictionary.\n",
    "    \n",
    "    This pattern is preferred when values have semantic meaning,\n",
    "    as the keys document what each value represents.\n",
    "    \"\"\"\n",
    "    return item[\"a\"] + item[\"b\"]\n",
    "\n",
    "# Convert to runnable and test with dictionary input\n",
    "chain = RunnableLambda(add_dict)\n",
    "chain.invoke({\"a\": 1, \"b\": 2})  # Returns: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATTERN: Adapting Existing Functions with Lambda Wrappers\n",
    "# ============================================================================\n",
    "# Sometimes you have existing functions with specific signatures that don't\n",
    "# match LangChain's single-input requirement. Use a lambda wrapper to adapt!\n",
    "#\n",
    "# This is useful when:\n",
    "# - Reusing library functions that expect multiple positional arguments\n",
    "# - Integrating legacy code into LangChain pipelines\n",
    "# - Creating flexible adapters between different data formats\n",
    "# ============================================================================\n",
    "\n",
    "# Existing function with separate parameters (can't be used directly with invoke)\n",
    "def sum_values(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers passed as separate parameters.\n",
    "    \n",
    "    Note: This function signature is NOT directly compatible with invoke()\n",
    "    because invoke() only passes a single argument.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# SOLUTION: Use a lambda to \"adapt\" the function signature\n",
    "# The lambda receives the dictionary and extracts values for the original function\n",
    "chain = RunnableLambda(lambda item: sum_values(item[\"a\"], item[\"b\"]))\n",
    "#                      ‚Üë This lambda bridges the gap between:\n",
    "#                        - LangChain's single-input invoke() call\n",
    "#                        - The original function's multi-parameter signature\n",
    "\n",
    "# Test the adapted function\n",
    "result = chain.invoke({\"a\": 1, \"b\": 2})\n",
    "print(result)  # Output: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "This code demonstrates a powerful pattern: **parallel processing with context preservation** before sending data to an LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Components Breakdown\n",
    "\n",
    "**1. Keyword Extraction Function**\n",
    "```python\n",
    "def extract_keywords(text):\n",
    "    words = text.lower().split()\n",
    "    keywords = [word for word in words if len(word) > 4]\n",
    "    return keywords[:3]\n",
    "```\n",
    "A simple function that extracts the first 3 words longer than 4 characters.\n",
    "\n",
    "**2. Prompt Template**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this text: {original_text}\\nKeywords found: {keywords}\"\n",
    ")\n",
    "```\n",
    "Expects two variables: `original_text` and `keywords`.\n",
    "\n",
    "**3. The Chain (key part)**\n",
    "```python\n",
    "chain = {\n",
    "    \"original_text\": RunnablePassthrough(),\n",
    "    \"keywords\": RunnableLambda(extract_keywords)\n",
    "} | prompt | chatgpt\n",
    "```\n",
    "\n",
    "The dictionary `{...}` is **shorthand for `RunnableParallel`**. Both branches receive the same input simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Flow Visualization\n",
    "\n",
    "```\n",
    "                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                            ‚îÇ  RunnablePassthrough()      ‚îÇ\n",
    "                            ‚îÇ  \"original_text\"            ‚îÇ\n",
    "                            ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫   ‚îÇ ‚Üí \"Building scalable...\"\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                         ‚ñ≤\n",
    "\"Building scalable data                  ‚îÇ\n",
    "platforms requires careful    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   (PARALLEL)\n",
    "architecture planning\"                   ‚îÇ\n",
    "                                         ‚ñº\n",
    "                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                            ‚îÇ  RunnableLambda(extract_    ‚îÇ\n",
    "                            ‚îÇ  keywords)                  ‚îÇ\n",
    "                            ‚îÇ  \"keywords\"                 ‚îÇ ‚Üí [\"building\", \"scalable\", \"platforms\"]\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "                                         ‚îÇ\n",
    "                                         ‚ñº\n",
    "                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                            ‚îÇ  Combined Dictionary        ‚îÇ\n",
    "                            ‚îÇ  {                          ‚îÇ\n",
    "                            ‚îÇ    \"original_text\": \"...\",  ‚îÇ\n",
    "                            ‚îÇ    \"keywords\": [...]        ‚îÇ\n",
    "                            ‚îÇ  }                          ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                         ‚îÇ\n",
    "                                         ‚ñº\n",
    "                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                            ‚îÇ  ChatPromptTemplate         ‚îÇ\n",
    "                            ‚îÇ  Fills in {original_text}   ‚îÇ\n",
    "                            ‚îÇ  and {keywords}             ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                         ‚îÇ\n",
    "                                         ‚ñº\n",
    "                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                            ‚îÇ  chatgpt (LLM)              ‚îÇ\n",
    "                            ‚îÇ  Generates analysis         ‚îÇ\n",
    "                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                         ‚îÇ\n",
    "                                         ‚ñº\n",
    "                                   AI Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Execution\n",
    "\n",
    "| Step | Component | Input | Output |\n",
    "|------|-----------|-------|--------|\n",
    "| 1 | `invoke(sample_text)` | `\"Building scalable data...\"` | ‚Äî |\n",
    "| 2a | `RunnablePassthrough()` | `\"Building scalable data...\"` | `\"Building scalable data...\"` (unchanged) |\n",
    "| 2b | `RunnableLambda(extract_keywords)` | `\"Building scalable data...\"` | `[\"building\", \"scalable\", \"platforms\"]` |\n",
    "| 3 | Dictionary merge | Both outputs | `{\"original_text\": \"...\", \"keywords\": [...]}` |\n",
    "| 4 | `prompt` | Dictionary | Formatted string: `\"Analyze this text: Building scalable...\\nKeywords found: ['building', 'scalable', 'platforms']\"` |\n",
    "| 5 | `chatgpt` | Prompt message | AI analysis response |\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Pattern Matters\n",
    "\n",
    "- **Parallel execution** ‚Äî Both branches run concurrently (faster than sequential)\n",
    "- **Context preservation** ‚Äî `RunnablePassthrough` keeps the original input available\n",
    "- **Enrichment** ‚Äî Add computed data (keywords) alongside original data\n",
    "- **Clean composition** ‚Äî The `|` pipe operator chains everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. RunnableParallel: Concurrent Processing\n",
    "\n",
    "**RunnableParallel** executes multiple runnables concurrently on the same input, collecting their outputs into a dictionary. This is perfect for scenarios where you need to extract different types of information from the same input simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': 'start-tech academy', 'length': 18}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parallel chain that:\n",
    "# 1. Preserves the original text using RunnablePassthrough\n",
    "# 2. Calculates the length using our custom function\n",
    "chain = RunnableParallel(\n",
    "    text1=RunnablePassthrough(),      # Keep original input\n",
    "    length=RunnableLambda(output_length)  # Calculate length\n",
    ")\n",
    "     \n",
    "# Test with a sample text - returns both original text and its length\n",
    "chain.invoke(\"start-tech academy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Integration with Large Language Models (LLMs)\n",
    "\n",
    "Now let's explore how to integrate runnables with LLMs like OpenAI's GPT models. This section demonstrates how to set up the OpenAI client and create chains that combine custom processing with AI capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and OpenAI API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Make sure you have a .env file with OPENAI_API_KEY=your_api_key_here\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from environment variables\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key as an environment variable\n",
    "# This is required for the OpenAI client to authenticate\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI chat model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a ChatOpenAI instance with specific configuration\n",
    "# gpt-4o-mini: A cost-effective model for most tasks\n",
    "# temperature=0: Ensures deterministic, consistent responses\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template and chain it with the LLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a simple prompt template that accepts user queries\n",
    "prompt_txt = \"{query}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# Create a chain by connecting the prompt template to the LLM\n",
    "# The pipe operator (|) creates a sequential chain\n",
    "# Data flows: input ‚Üí prompt_template ‚Üí chatgpt ‚Üí output\n",
    "llmchain = (prompt_template\n",
    "              |\n",
    "           chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to algorithms that can create new content, such as text, images, or music, by learning patterns from existing data.\n"
     ]
    }
   ],
   "source": [
    "# Test the chain with a sample query\n",
    "response = llmchain.invoke({'query': 'Explain Generative AI in 1 line'})\n",
    "\n",
    "# Print the AI's response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RunnableLambda Patterns\n",
    "\n",
    "### Custom Data Processing with RunnableLambda\n",
    "\n",
    "This section demonstrates how to create more sophisticated data processing pipelines using RunnableLambda to handle complex data transformations before sending to the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Sourav | Role: Solution Architect | Location: Bengaluru\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for custom function wrapping\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a custom function to format user data\n",
    "def format_user_data(user_info):\n",
    "    \"\"\"Process user information for downstream components\"\"\"\n",
    "    # Create a formatted string with user details\n",
    "    return f\"User: {user_info['name']} | Role: {user_info['role']} | Location: {user_info['location']}\"\n",
    "\n",
    "# Convert the function to a Runnable for use in chains\n",
    "formatter = RunnableLambda(format_user_data)\n",
    "\n",
    "# Test the formatter with sample user data\n",
    "user_data = {\"name\": \"Sourav\", \"role\": \"Solution Architect\", \"location\": \"Bengaluru\"}\n",
    "result = formatter.invoke(user_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RunnablePassthrough - Preserving Input Context\n",
    "\n",
    "### Complex Data Processing with Context Preservation\n",
    "\n",
    "This example shows how to use RunnablePassthrough to maintain the original input while simultaneously processing it through other functions. This is crucial for maintaining context in complex AI workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text \"Building scalable data platforms requires careful architecture planning\" emphasizes the importance of strategic design in the development of data platforms that can grow and adapt to increasing demands. \n",
      "\n",
      "### Analysis:\n",
      "\n",
      "1. **Keywords**:\n",
      "   - **Building**: This suggests an active process of creation and development, indicating that constructing a data platform is not a passive task but requires effort and expertise.\n",
      "   - **Scalable**: This is a critical term in technology and data management, referring to the ability of a system to handle growth, whether in terms of data volume, user load, or functionality. Scalability is essential for ensuring that a platform can evolve without requiring a complete redesign.\n",
      "   - **Platforms**: This term indicates a foundational technology or system that supports various applications or services. In the context of data, it implies a comprehensive environment for data storage, processing, and analysis.\n",
      "\n",
      "2. **Key Themes**:\n",
      "   - **Architecture Planning**: The phrase highlights the necessity of thoughtful design and planning in the architecture of data platforms. This involves considering various factors such as data flow, storage solutions, processing capabilities, and user access.\n",
      "   - **Scalability Challenges**: The text implies that without careful planning, a data platform may struggle to scale effectively, leading to performance issues or the need for costly overhauls in the future.\n",
      "\n",
      "3. **Implications**:\n",
      "   - Organizations looking to build data platforms must invest time and resources into the architectural phase to ensure long-term success and adaptability.\n",
      "   - The focus on scalability suggests that businesses should anticipate future growth and design their platforms accordingly, rather than merely addressing current needs.\n",
      "\n",
      "### Conclusion:\n",
      "The statement serves as a reminder of the complexities involved in creating data platforms and the critical role of architecture in ensuring that these platforms can scale effectively. It underscores the need for a proactive approach to design, which can ultimately lead to more robust and efficient data management solutions.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary components for context preservation\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a function to extract keywords from text\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Simple keyword extraction that filters words longer than 4 characters\"\"\"\n",
    "    words = text.lower().split()\n",
    "    keywords = [word for word in words if len(word) > 4]\n",
    "    return keywords[:3]  # Return top 3 keywords\n",
    "\n",
    "# Create a prompt template that uses both original text and keywords\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this text: {original_text}\\nKeywords found: {keywords}\"\n",
    ")\n",
    "\n",
    "# Build a chain that preserves original input and adds keyword analysis\n",
    "# This demonstrates parallel processing: \n",
    "# - RunnablePassthrough keeps the original text\n",
    "# - RunnableLambda processes the text to extract keywords\n",
    "chain = {\n",
    "    \"original_text\": RunnablePassthrough(),      # Preserve original input\n",
    "    \"keywords\": RunnableLambda(extract_keywords)    # Extract keywords\n",
    "} | prompt | chatgpt\n",
    "\n",
    "# Test the chain with a technical text\n",
    "sample_text = \"Building scalable data platforms requires careful architecture planning\"\n",
    "result = chain.invoke(sample_text)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text \"Building scalable data platforms requires careful architecture planning\" emphasizes the importance of strategic design in the development of data platforms that can grow and adapt to increasing demands. \n",
      "\n",
      "### Analysis:\n",
      "\n",
      "1. **Keywords**:\n",
      "   - **Building**: This suggests an active process of creation and development, indicating that constructing data platforms is not a passive task but requires effort and expertise.\n",
      "   - **Scalable**: This term highlights the need for flexibility and the ability to handle growth. A scalable platform can accommodate increasing amounts of data and users without a significant drop in performance.\n",
      "   - **Platforms**: Refers to the foundational systems or frameworks that support data management and analytics. This implies a focus on infrastructure that can support various applications and services.\n",
      "\n",
      "2. **Core Message**:\n",
      "   - The statement underscores that simply creating a data platform is not enough; it must be designed with scalability in mind. This involves thoughtful architecture planning to ensure that the platform can evolve and expand as needs change.\n",
      "\n",
      "3. **Implications**:\n",
      "   - **Architecture Planning**: The mention of \"careful architecture planning\" suggests that there are specific methodologies and best practices that should be followed to ensure the platform's success. This could involve considerations of data storage, processing capabilities, and integration with other systems.\n",
      "   - **Future-Proofing**: By focusing on scalability, organizations can future-proof their data platforms, ensuring they remain relevant and effective as technology and business needs evolve.\n",
      "\n",
      "4. **Target Audience**:\n",
      "   - This text is likely aimed at data engineers, architects, and decision-makers in organizations that rely on data-driven strategies. It speaks to professionals who understand the complexities of data management and the importance of strategic planning.\n",
      "\n",
      "In summary, the text conveys a critical message about the necessity of thoughtful design in building data platforms that can scale effectively, highlighting the interplay between architecture and operational success.\n"
     ]
    }
   ],
   "source": [
    "# Another way to do the same thing\n",
    "sample_text = \"Building scalable data platforms requires careful architecture planning\"\n",
    "chain = {\n",
    "    \"original_text\": RunnablePassthrough(),      # Preserve original input\n",
    "    \"keywords\": RunnableLambda(extract_keywords)    # Extract keywords\n",
    "}|prompt\n",
    "resp = chain.invoke(sample_text)\n",
    "response = chatgpt.invoke(resp)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequential Chaining with Pipe Operator (|)\n",
    "\n",
    "### Building Multi-Step Data Processing Pipelines\n",
    "\n",
    "Sequential chaining allows you to build complex workflows where data flows through multiple processing steps in order. Each step receives the output of the previous step as its input, creating a powerful data transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Analysis Summary:\n",
      "- Characters: 119\n",
      "- Words: 18  \n",
      "- Sentences: 3\n",
      "- Avg words per sentence: 6.0\n",
      "Original: \"Data engineering is crucial for ML success. It inv...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for creating processing steps\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Step 1: Input validation and cleaning\n",
    "def validate_input(data):\n",
    "    \"\"\"Validate and clean input data\"\"\"\n",
    "    if not isinstance(data, str) or len(data.strip()) == 0:\n",
    "        raise ValueError(\"Input must be a non-empty string\")\n",
    "    return data.strip()\n",
    "\n",
    "# Step 2: Extract various text metrics\n",
    "def extract_metrics(text):\n",
    "    \"\"\"Extract comprehensive text metrics from the input\"\"\"\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"character_count\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?')\n",
    "    }\n",
    "\n",
    "# Step 3: Generate a formatted summary report\n",
    "def generate_summary(metrics):\n",
    "    \"\"\"Generate a comprehensive summary report from metrics\"\"\"\n",
    "    avg_words_per_sentence = metrics['word_count'] / max(metrics['sentence_count'], 1)\n",
    "    return f\"\"\"\n",
    "Text Analysis Summary:\n",
    "- Characters: {metrics['character_count']}\n",
    "- Words: {metrics['word_count']}  \n",
    "- Sentences: {metrics['sentence_count']}\n",
    "- Avg words per sentence: {avg_words_per_sentence:.1f}\n",
    "Original: \"{metrics['original_text'][:50]}...\"\n",
    "\"\"\"\n",
    "\n",
    "# Create a sequential processing chain\n",
    "# Data flows: input ‚Üí validate ‚Üí extract_metrics ‚Üí generate_summary ‚Üí output\n",
    "analysis_chain = (\n",
    "    RunnableLambda(validate_input) |     # Step 1: Clean input\n",
    "    RunnableLambda(extract_metrics) |    # Step 2: Extract metrics\n",
    "    RunnableLambda(generate_summary)     # Step 3: Generate report\n",
    ")\n",
    "\n",
    "# Test the sequential chain with sample text\n",
    "sample_text = \"Data engineering is crucial for ML success. It involves building robust pipelines. Quality data leads to better models.\"\n",
    "result = analysis_chain.invoke(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Parallel Execution with RunnableParallel\n",
    "\n",
    "### Concurrent Multi-Analysis Pipeline\n",
    "\n",
    "This advanced example demonstrates how to perform multiple types of analysis simultaneously on the same input. This is particularly useful for extracting different insights from text data in parallel, significantly improving performance compared to sequential processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Analysis Results:\n",
      "  technical_analysis: {'technical_terms': ['data', 'platform', 'architecture', 'pipeline', 'model'], 'technical_density': 27.77777777777778}\n",
      "  sentiment: Positive\n",
      "  entities: ['Databricks', 'The']\n",
      "  word_count: 18\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for parallel processing\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "import re\n",
    "\n",
    "# Analysis Function 1: Technical Content Analysis\n",
    "def analyze_technical_content(text):\n",
    "    \"\"\"Analyze technical aspects of text and calculate technical density\"\"\"\n",
    "    tech_terms = ['data', 'platform', 'architecture', 'pipeline', 'model', 'algorithm']\n",
    "    found_terms = [term for term in tech_terms if term.lower() in text.lower()]\n",
    "    technical_density = len(found_terms) / len(text.split()) * 100\n",
    "    return {\n",
    "        \"technical_terms\": found_terms,\n",
    "        \"technical_density\": technical_density\n",
    "    }\n",
    "\n",
    "# Analysis Function 2: Sentiment Analysis\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Simple rule-based sentiment analysis\"\"\"\n",
    "    positive_words = ['good', 'great', 'excellent', 'success', 'efficient', 'robust']\n",
    "    negative_words = ['bad', 'poor', 'failed', 'problem', 'issue', 'difficult']\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    pos_count = sum(1 for word in positive_words if word in text.lower())\n",
    "    neg_count = sum(1 for word in negative_words if word in text.lower())\n",
    "    \n",
    "    # Determine overall sentiment\n",
    "    if pos_count > neg_count:\n",
    "        return \"Positive\"\n",
    "    elif neg_count > pos_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Analysis Function 3: Entity Extraction\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract potential named entities using simple pattern matching\"\"\"\n",
    "    # Find capitalized words (potential proper nouns/entities)\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "    return list(set(entities))  # Remove duplicates\n",
    "\n",
    "# Create a parallel analysis pipeline that runs all analyses simultaneously\n",
    "# All functions receive the same input text and execute concurrently\n",
    "parallel_analyzer = RunnableParallel(\n",
    "    technical_analysis=RunnableLambda(analyze_technical_content),  # Technical analysis\n",
    "    sentiment=RunnableLambda(analyze_sentiment),                   # Sentiment analysis\n",
    "    entities=RunnableLambda(extract_entities),                     # Entity extraction\n",
    "    word_count=RunnableLambda(lambda x: len(x.split()))            # Simple word count\n",
    ")\n",
    "\n",
    "# Test the parallel execution with sample text\n",
    "sample_text = \"Databricks provides an excellent platform for building robust data pipelines. The architecture supports efficient model training and deployment.\"\n",
    "\n",
    "# Execute all analyses in parallel\n",
    "result = parallel_analyzer.invoke(sample_text)\n",
    "\n",
    "# Display results from all parallel analyses\n",
    "print(\"Parallel Analysis Results:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conditional Logic with RunnableBranch\n",
    "\n",
    "### Intelligent Query Routing System\n",
    "\n",
    "RunnableBranch enables dynamic routing of inputs based on conditions. This example demonstrates how to create an intelligent system that routes different types of technical queries to specialized handlers, similar to how a support system might route tickets to different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I optimize Spark pipelines for better performance?\n",
      "Response: üîß Data Engineering Response: This query about 'How do I optimize Spark pipelines for better performance?' relates to building and maintaining data pipelines and infrastructure.\n",
      "\n",
      "Query: What's the best algorithm for classification problems?\n",
      "Response: ü§ñ ML Response: This query about 'What's the best algorithm for classification problems?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: How to deploy models on Azure ML?\n",
      "Response: ü§ñ ML Response: This query about 'How to deploy models on Azure ML?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: What's the weather like today?\n",
      "Response: üí° General Response: This is a general query about 'What's the weather like today?'. Please provide more specific context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for conditional logic\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "# Condition Functions: These determine which branch to take\n",
    "def is_data_engineering_query(text):\n",
    "    \"\"\"Check if query is about data engineering topics\"\"\"\n",
    "    de_keywords = ['pipeline', 'etl', 'data warehouse', 'spark', 'kafka', 'airflow']\n",
    "    return any(keyword in text.lower() for keyword in de_keywords)\n",
    "\n",
    "def is_ml_query(text):\n",
    "    \"\"\"Check if query is about machine learning topics\"\"\"\n",
    "    ml_keywords = ['model', 'algorithm', 'training', 'prediction', 'classification', 'regression']\n",
    "    return any(keyword in text.lower() for keyword in ml_keywords)\n",
    "\n",
    "def is_cloud_query(text):\n",
    "    \"\"\"Check if query is about cloud platforms and services\"\"\"\n",
    "    cloud_keywords = ['aws', 'azure', 'gcp', 'cloud', 'kubernetes', 'docker']\n",
    "    return any(keyword in text.lower() for keyword in cloud_keywords)\n",
    "\n",
    "# Handler Functions: These process queries based on their type\n",
    "def handle_data_engineering(text):\n",
    "    \"\"\"Specialized handler for data engineering queries\"\"\"\n",
    "    return f\"üîß Data Engineering Response: This query about '{text}' relates to building and maintaining data pipelines and infrastructure.\"\n",
    "\n",
    "def handle_ml_query(text):\n",
    "    \"\"\"Specialized handler for ML queries\"\"\"\n",
    "    return f\"ü§ñ ML Response: This query about '{text}' involves machine learning models and algorithms.\"\n",
    "\n",
    "def handle_cloud_query(text):\n",
    "    \"\"\"Specialized handler for cloud queries\"\"\"\n",
    "    return f\"‚òÅÔ∏è Cloud Response: This query about '{text}' concerns cloud platforms and services.\"\n",
    "\n",
    "def handle_general_query(text):\n",
    "    \"\"\"Default handler for general queries that don't match specific categories\"\"\"\n",
    "    return f\"üí° General Response: This is a general query about '{text}'. Please provide more specific context.\"\n",
    "\n",
    "# Create conditional routing system using RunnableBranch\n",
    "# Structure: (condition, handler) pairs, with default handler at the end\n",
    "query_router = RunnableBranch(\n",
    "    (RunnableLambda(is_data_engineering_query), RunnableLambda(handle_data_engineering)),\n",
    "    (RunnableLambda(is_ml_query), RunnableLambda(handle_ml_query)),\n",
    "    (RunnableLambda(is_cloud_query), RunnableLambda(handle_cloud_query)),\n",
    "    RunnableLambda(handle_general_query)  # Default case when no conditions match\n",
    ")\n",
    "\n",
    "# Test the routing system with different types of queries\n",
    "test_queries = [\n",
    "    \"How do I optimize Spark pipelines for better performance?\",  # Data Engineering\n",
    "    \"What's the best algorithm for classification problems?\",      # Machine Learning\n",
    "    \"How to deploy models on Azure ML?\",                          # Cloud (contains both ML and cloud keywords)\n",
    "    \"What's the weather like today?\"                              # General (no specific keywords)\n",
    "]\n",
    "\n",
    "# Execute routing for each test query\n",
    "for query in test_queries:\n",
    "    result = query_router.invoke(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Streaming Processing\n",
    "\n",
    "### Real-time Data Processing with Streaming\n",
    "\n",
    "Streaming allows you to process data incrementally and yield results as they become available. This is particularly useful for long-running processes or when you want to provide progressive updates to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming processing:\n",
      "  Processed: BUILDING\n",
      "  Processed: BUILDING SCALABLE\n",
      "  Processed: BUILDING SCALABLE DATA\n",
      "  Processed: BUILDING SCALABLE DATA SOLUTIONS\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for streaming\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import time\n",
    "\n",
    "def streaming_processor(text):\n",
    "    \"\"\"Process text word by word and yield incremental results\"\"\"\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    # Process each word incrementally\n",
    "    for word in words:\n",
    "        processed_words.append(word.upper())\n",
    "        \n",
    "        # Simulate processing time (remove in production)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Yield intermediate result after processing each word\n",
    "        yield f\"Processed: {' '.join(processed_words)}\"\n",
    "\n",
    "# Create a streaming runnable from the generator function\n",
    "streaming_runnable = RunnableLambda(streaming_processor)\n",
    "\n",
    "# Execute streaming processing - results are yielded incrementally\n",
    "print(\"Streaming processing:\")\n",
    "for chunk in streaming_runnable.stream(\"building scalable data solutions\"):\n",
    "    print(f\"  {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing\n",
    "\n",
    "### Efficient Processing of Multiple Inputs\n",
    "\n",
    "Batch processing allows you to process multiple inputs simultaneously, which is more efficient than processing them one by one. This is particularly useful when you need to analyze large volumes of data or multiple queries at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Analysis Results:\n",
      "  Query 1: {'query': 'Hello', 'word_count': 1, 'complexity': 'Simple'}\n",
      "  Query 2: {'query': 'How do I setup Databricks?', 'word_count': 5, 'complexity': 'Medium'}\n",
      "  Query 3: {'query': 'What are the best practices fo...', 'word_count': 21, 'complexity': 'Complex'}\n",
      "  Query 4: {'query': 'Spark optimization tips', 'word_count': 3, 'complexity': 'Simple'}\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for batch processing\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def analyze_query_complexity(query):\n",
    "    \"\"\"Analyze the complexity of a user query based on word count\"\"\"\n",
    "    word_count = len(query.split())\n",
    "    char_count = len(query)\n",
    "    \n",
    "    # Categorize complexity based on word count\n",
    "    if word_count <= 3:\n",
    "        complexity = \"Simple\"\n",
    "    elif word_count <= 10:\n",
    "        complexity = \"Medium\"\n",
    "    else:\n",
    "        complexity = \"Complex\"\n",
    "    \n",
    "    return {\n",
    "        \"query\": query[:30] + \"...\" if len(query) > 30 else query,  # Truncate long queries\n",
    "        \"word_count\": word_count,\n",
    "        \"complexity\": complexity\n",
    "    }\n",
    "\n",
    "# Create analyzer runnable\n",
    "complexity_analyzer = RunnableLambda(analyze_query_complexity)\n",
    "\n",
    "# Prepare multiple queries for batch processing\n",
    "queries = [\n",
    "    \"Hello\",                                    # Simple query\n",
    "    \"How do I setup Databricks?\",              # Medium query\n",
    "    \"What are the best practices for designing a data lake architecture that can handle both batch and streaming data processing workloads?\",  # Complex query\n",
    "    \"Spark optimization tips\"                   # Simple query\n",
    "]\n",
    "\n",
    "# Batch process all queries simultaneously\n",
    "# This is more efficient than processing them one by one\n",
    "results = complexity_analyzer.batch(queries)\n",
    "\n",
    "# Display results from batch processing\n",
    "print(\"Batch Analysis Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  Query {i}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Examples with LLM Integration\n",
    "\n",
    "### Building Production-Ready AI Workflows\n",
    "\n",
    "This section demonstrates how to combine all the concepts we've learned to build sophisticated AI workflows that integrate custom processing with Large Language Models. These examples show real-world patterns for building production-ready AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Custom Data Processing with LLM Integration\n",
    "\n",
    "This example demonstrates how to clean and format user input before sending it to the LLM. Data preprocessing is crucial for getting consistent, high-quality responses from AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Approach 1: Dictionary Input Processing\n",
    "\n",
    "This approach processes dictionary input, useful when your function expects structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In this approach, the `clean_user_input()` function expects a dictionary input with a \"topic\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned output: {'topic': 'apache spark'}\n",
      "AI Response: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Here are the key concepts and features of Apache Spark:\n",
      "\n",
      "### 1. **Speed and Performance**\n",
      "   - **In-Memory Processing**: Spark processes data in memory, which significantly speeds up data processing tasks compared to traditional disk-based processing systems like Hadoop MapReduce.\n",
      "   - **Optimized Execution**: Spark uses a directed acyclic graph (DAG) execution engine that optimizes the execution plan for data processing tasks.\n",
      "\n",
      "### 2. **Ease of Use**\n",
      "   - **High-Level APIs**: Spark provides high-level APIs in multiple programming languages, including Scala, Java, Python, and R, making it accessible to a wide range of developers.\n",
      "   - **Interactive Shell**: It offers an interactive shell for data analysis, allowing users to run commands and see results immediately.\n",
      "\n",
      "### 3. **Unified Engine**\n",
      "   - **Multiple Workloads**: Spark supports various workloads, including batch processing, interactive queries, streaming data, machine learning, and graph processing, all within a single framework.\n",
      "   - **Libraries**: It comes with built-in libraries like Spark SQL (for structured data processing), Spark Streaming (for real-time data processing), MLlib (for machine learning), and GraphX (for graph processing).\n",
      "\n",
      "### 4. **Scalability**\n",
      "   - **Cluster Computing**: Spark can run on a cluster of machines, allowing it to scale horizontally. It can handle large datasets by distributing the data and computation across multiple nodes.\n",
      "   - **Resource Management**: It can run on various cluster managers, including Apache Mesos, Hadoop YARN, and Kubernetes, which manage resources and scheduling.\n",
      "\n",
      "### 5. **Fault Tolerance**\n",
      "   - **Resilient Distributed Datasets (RDDs)**: Spark introduces RDDs, which are immutable collections of objects that can be processed in parallel. RDDs provide fault tolerance by allowing Spark to recompute lost data using lineage information.\n",
      "\n",
      "### 6. **Data Sources**\n",
      "   - **Integration with Various Data Sources**: Spark can read data from various sources, including HDFS, Apache Cassandra, Apache HBase, Amazon S3, and more, making it versatile for different data environments.\n",
      "\n",
      "### 7. **Community and Ecosystem**\n",
      "   - **Active Community**: Being an open-source project, Spark has a large and active community that contributes to its development, documentation, and support.\n",
      "   - **Ecosystem**: Spark integrates well with other big data tools and frameworks, enhancing its capabilities and making it a popular choice for data processing tasks.\n",
      "\n",
      "### Use Cases\n",
      "Apache Spark is widely used in various industries for tasks such as:\n",
      "- Data processing and ETL (Extract, Transform, Load)\n",
      "- Real-time data analytics and monitoring\n",
      "- Machine learning model training and deployment\n",
      "- Graph processing and analysis\n",
      "\n",
      "In summary, Apache Spark is a powerful tool for big data processing that combines speed, ease of use, and versatility, making it a popular choice for data engineers and data scientists.\n"
     ]
    }
   ],
   "source": [
    "# Import string output parser for clean text responses\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Function that processes dictionary input\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    clean_text = text[\"topic\"].strip().lower()  # Extract and clean the topic\n",
    "    return {\"topic\": clean_text}\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "\n",
    "# Test the cleaner with dictionary input\n",
    "print(\"Cleaned output:\", input_cleaner.invoke({\"topic\": \"  APACHE SPARK  \"}))\n",
    "\n",
    "# Create a complete AI chain with input cleaning\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = input_cleaner | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the complete chain with dictionary input\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke({\"topic\": user_input})\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Direct String Input Processing\n",
    "\n",
    "This approach processes string input directly, which is more common when dealing with simple text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned output: apache spark\n",
      "AI Response: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Here are the key concepts and features of Apache Spark:\n",
      "\n",
      "### 1. **Speed and Performance**\n",
      "   - **In-Memory Processing**: Spark processes data in memory, which significantly speeds up data processing tasks compared to traditional disk-based processing systems like Hadoop MapReduce.\n",
      "   - **Optimized Execution**: Spark uses a directed acyclic graph (DAG) execution engine that optimizes the execution plan for data processing tasks.\n",
      "\n",
      "### 2. **Ease of Use**\n",
      "   - **High-Level APIs**: Spark provides high-level APIs in multiple programming languages, including Scala, Java, Python, and R, making it accessible to a wide range of developers.\n",
      "   - **Interactive Shell**: It offers an interactive shell for data analysis, allowing users to run commands and see results immediately.\n",
      "\n",
      "### 3. **Unified Engine**\n",
      "   - **Multiple Workloads**: Spark supports various workloads, including batch processing, interactive queries, streaming data, machine learning, and graph processing, all within a single framework.\n",
      "   - **Libraries**: It comes with built-in libraries for SQL (Spark SQL), machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n",
      "\n",
      "### 4. **Distributed Computing**\n",
      "   - **Cluster Management**: Spark can run on various cluster managers, including Hadoop YARN, Apache Mesos, and Kubernetes, allowing it to scale across many machines.\n",
      "   - **Data Distribution**: It automatically distributes data across the cluster and handles task scheduling, fault tolerance, and resource management.\n",
      "\n",
      "### 5. **Resilient Distributed Datasets (RDDs)**\n",
      "   - **Core Abstraction**: RDDs are the fundamental data structure in Spark, representing an immutable distributed collection of objects that can be processed in parallel.\n",
      "   - **Fault Tolerance**: RDDs provide fault tolerance through lineage, meaning that if a partition of an RDD is lost, it can be recomputed from the original data.\n",
      "\n",
      "### 6. **Data Sources**\n",
      "   - **Integration**: Spark can read data from various sources, including HDFS, Apache Cassandra, Apache HBase, Amazon S3, and many more, making it versatile for different data environments.\n",
      "\n",
      "### 7. **Community and Ecosystem**\n",
      "   - **Active Development**: As an open-source project, Spark has a large and active community that contributes to its development, ensuring continuous improvement and a rich ecosystem of tools and libraries.\n",
      "\n",
      "### Use Cases\n",
      "Apache Spark is widely used in various industries for tasks such as:\n",
      "- Data processing and ETL (Extract, Transform, Load)\n",
      "- Real-time data analytics and monitoring\n",
      "- Machine learning model training and deployment\n",
      "- Data warehousing and business intelligence\n",
      "\n",
      "In summary, Apache Spark is a powerful tool for big data processing that combines speed, ease of use, and versatility, making it a popular choice for organizations looking to analyze large datasets efficiently.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import string output parser for clean text responses\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Function that processes string input directly\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    return text.strip().lower()  # Remove whitespace and convert to lowercase\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "\n",
    "# Test the cleaner with string input\n",
    "print(\"Cleaned output:\", input_cleaner.invoke(\"  APACHE SPARK  \"))\n",
    "\n",
    "# Create a complete AI chain with input cleaning\n",
    "# Note: We use dictionary syntax to map the cleaned input to the \"topic\" key\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = {\"topic\": input_cleaner} | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the complete chain with string input\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke(user_input)\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Context Preservation with RunnablePassthrough\n",
    "\n",
    "This example demonstrates how to preserve the original input context while simultaneously processing it through other functions. This pattern is essential for maintaining context in complex AI workflows where you need both the original data and processed insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: How do I build data pipelines using Spark for cloud architecture?\n",
      "Keywords found: ['data', 'spark', 'pipeline', 'cloud', 'architecture']\n",
      "AI Analysis: Building data pipelines using Apache Spark in a cloud architecture involves several key steps and considerations. Here‚Äôs a focused technical explanation based on the identified terms:\n",
      "\n",
      "### 1. **Understanding Data Pipelines**\n",
      "A data pipeline is a series of data processing steps that involve the collection, transformation, and storage of data. In a cloud architecture, these pipelines can leverage cloud services for scalability, reliability, and performance.\n",
      "\n",
      "### 2. **Apache Spark Overview**\n",
      "Apache Spark is an open-source distributed computing system designed for fast processing of large datasets...\n"
     ]
    }
   ],
   "source": [
    "# Function to extract technical keywords from text\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract key technical terms from text\"\"\"\n",
    "    technical_terms = ['data', 'spark', 'pipeline', 'model', 'cloud', 'architecture']\n",
    "    words = text.lower().split()\n",
    "    found_terms = [term for term in technical_terms if term in ' '.join(words)]\n",
    "    return found_terms\n",
    "\n",
    "# Create a prompt template that uses both original text and extracted keywords\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Original query: \"{original_text}\"\n",
    "Technical terms found: {keywords}\n",
    "\n",
    "Provide a focused technical explanation based on these key terms.\n",
    "\"\"\")\n",
    "\n",
    "# Create a chain that preserves original input and adds keyword analysis\n",
    "# RunnablePassthrough preserves the original text\n",
    "# RunnableLambda extracts keywords from the same text\n",
    "context_chain = {\n",
    "    \"original_text\": RunnablePassthrough(),      # Preserve original input\n",
    "    \"keywords\": RunnableLambda(extract_keywords)    # Extract keywords\n",
    "} | analysis_prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the context preservation chain\n",
    "tech_query = \"How do I build data pipelines using Spark for cloud architecture?\"\n",
    "result = context_chain.invoke(tech_query)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Original: {tech_query}\")\n",
    "print(f\"Keywords found: {extract_keywords(tech_query)}\")\n",
    "print(f\"AI Analysis: {result[:600]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chaining with Pipe Operator (|)\n",
    "The pipe operator creates sequential workflows where data flows from left to right through multiple processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: Optimizing Spark job performance is crucial for ensuring efficient data processing and resource utilization. Here are several strategies and best prac...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: Choosing the \"best\" algorithm for classification in machine learning depends on various factors, including the nature of the data, the problem at hand...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: Deploying applications on Azure Kubernetes Service (AKS) involves several steps, from setting up the AKS cluster to deploying your application. Below ...\n",
      "\n",
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: Optimizing Spark job performance involves a combination of configuration tuning, efficient data handling, and leveraging Spark's built-in capabilities...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: Choosing the \"best\" algorithm for classification in machine learning depends on various factors, including the nature of the data, the problem at hand...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: Deploying applications on Azure Kubernetes Service (AKS) involves several steps, from setting up the AKS cluster to deploying your application. Below ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules for sequential chaining\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def categorize_query(text):\n",
    "    \"\"\"Categorize technical queries based on keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in ['spark', 'hadoop', 'etl', 'pipeline']):\n",
    "        return \"Data Engineering\"\n",
    "    elif any(word in text_lower for word in ['model', 'algorithm', 'training']):\n",
    "        return \"Machine Learning\"\n",
    "    elif any(word in text_lower for word in ['azure', 'aws', 'cloud']):\n",
    "        return \"Cloud Platform\"\n",
    "    return \"General\"\n",
    "\n",
    "def create_expert_prompt(data):\n",
    "    \"\"\"Create specialized prompts based on category\"\"\"\n",
    "    category = data['category']\n",
    "    question = data['question']\n",
    "    \n",
    "    expert_prompts = {\n",
    "        \"Data Engineering\": f\"As a data engineering expert, provide technical guidance for: {question}\",\n",
    "        \"Machine Learning\": f\"As an ML specialist, explain the concepts related to: {question}\",\n",
    "        \"Cloud Platform\": f\"As a cloud architect, describe the solution for: {question}\",\n",
    "        \"General\": f\"Provide a comprehensive answer to: {question}\"\n",
    "    }\n",
    "    \n",
    "    return expert_prompts[category]\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    RunnableLambda(lambda x: {\"question\": x, \"category\": categorize_query(x)}) |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: Optimizing Spark job performance involves a combination of configuration tuning, efficient data handling, and leveraging Spark's built-in capabilities...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: Choosing the \"best\" algorithm for classification in machine learning depends on various factors, including the nature of the data, the problem at hand...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m test_questions:\n\u001b[32m     19\u001b[39m     category = categorize_query(question)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     result = \u001b[43mexpert_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCategory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3025\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3024\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m                 \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:925\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/openai/_base_client.py:955\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    952\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    961\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Another way to create the chain\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    {\"question\": RunnablePassthrough(),\"category\": RunnableLambda(categorize_query)} |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing with RunnableParallel\n",
    "RunnableParallel executes multiple operations simultaneously on the same input, perfect for extracting different types of information concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Simple word count function\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of this text (positive/negative/neutral): {text}\"\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Provide a one-sentence summary of: {text}\"\n",
    ")\n",
    "\n",
    "# Create parallel analysis chains\n",
    "sentiment_chain = sentiment_prompt | chatgpt | StrOutputParser()\n",
    "summary_chain = summary_prompt | chatgpt | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Databricks provides excellent tools for building scalable data pipelines and machine learning solutions efficiently.\"\n",
    "# RunnableLambda(count_words).invoke({\"text\": sample_text})\n",
    "\n",
    "RunnableLambda(count_words).invoke( sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand RunnableLambda\n",
    "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def add_three(a):\n",
    "    return a + 3\n",
    "\n",
    "print(RunnableLambda(add_three).invoke(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "print(RunnableLambda(add_one).invoke(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
     ]
    }
   ],
   "source": [
    "print(type(RunnableLambda(add_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for elem in RunnableLambda(add_one).stream(2):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for elem in RunnableLambda(add_one).batch([1,2,3]):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand RunnablePassthrough\n",
    "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin': 1, 'modified': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "print(runnable.invoke(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return prompt+\" completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parsed': 'noitelpmoc olleh'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain that combines a fake LLM with text reversal\n",
    "# Step 1: RunnableLambda(fake_llm) - Wraps our fake LLM function\n",
    "# Step 2: {'parsed': lambda text: text[::-1]} - Creates a dictionary with a key 'parsed'\n",
    "#         that contains a lambda function to reverse the text\n",
    "# The pipe operator (|) chains these operations together\n",
    "chain = RunnableLambda(fake_llm) | {'parsed': lambda text: text[::-1]}\n",
    "\n",
    "# Invoke the chain with input \"hello\"\n",
    "# This will: 1) Pass \"hello\" to fake_llm -> returns \"hello completion\"\n",
    "#           2) Pass \"hello completion\" to the lambda function -> reverses it to \"noitelpmoc olleh\"\n",
    "#           3) Return result as {'parsed': 'noitelpmoc olleh'}\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'hello completion', 'parsed': 'noitelpmoc olleh'}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] # Reverse the output\n",
    "}\n",
    "\n",
    "print(chain.invoke('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above code\n",
    "\n",
    "1. RunnableLambda(fake_llm) Converts the fake_llm function into a LangChain Runnable. This makes it compatible with LangChain pipelines\n",
    "2. This pipes the output of RunnableLambda(fake_llm) into the next processing step.\n",
    "3. The output of fake_llm (\"completion\") is passed into both\n",
    "    * 'original': RunnablePassthrough(), which simply forwards the input as is.\n",
    "    * 'parsed': lambda text: text[::-1], which reverses the output string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input 'hello' is passed into fake_llm\n",
    "    * fake_llm('hello')  # Returns \"completion\"\n",
    "2. \"completion\" is passed into the dictionary processor, which does:\n",
    "    * 'original': Keeps \"completion\" as is.\n",
    "    * 'parsed': Reverses \"completion\" to \"noitelpmoc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello completion'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableLambda(fake_llm).invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Assign in RunnablePassThrough\n",
    "# In some cases, it may be useful to pass the input through while adding some keys to the output. In this case, you can use the assign method.\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "This notebook covered the fundamental concepts of LangChain Runnables and how to build sophisticated AI workflows:\n",
    "\n",
    "#### Core Runnable Types:\n",
    "1. **RunnablePassthrough**: Preserves input unchanged - useful for maintaining context\n",
    "2. **RunnableLambda**: Wraps Python functions for custom processing\n",
    "3. **RunnableParallel**: Executes multiple operations simultaneously\n",
    "4. **RunnableBranch**: Enables conditional logic and routing\n",
    "\n",
    "#### Key Patterns:\n",
    "- **Sequential Processing**: Using the pipe operator (`|`) to chain operations\n",
    "- **Parallel Processing**: Executing multiple analyses simultaneously\n",
    "- **Context Preservation**: Maintaining original input while processing\n",
    "- **Conditional Routing**: Directing inputs to specialized handlers\n",
    "\n",
    "#### Best Practices:\n",
    "- Use parallel processing for independent operations to improve performance\n",
    "- Preserve context when building complex workflows\n",
    "- Implement proper input validation and cleaning\n",
    "- Design modular, reusable components\n",
    "- Combine custom logic with LLM capabilities effectively\n",
    "\n",
    "#### Real-world Applications:\n",
    "- Content analysis and categorization\n",
    "- Technical query routing systems\n",
    "- Data preprocessing pipelines\n",
    "- Multi-stage AI workflows\n",
    "- Batch processing of large datasets\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand the fundamentals of LangChain Runnables, you can:\n",
    "- Build more complex multi-agent systems\n",
    "- Implement sophisticated data processing pipelines\n",
    "- Create intelligent routing systems\n",
    "- Develop production-ready AI applications\n",
    "\n",
    "**Remember**: The power of LangChain Runnables lies in their composability - you can combine these simple building blocks to create highly sophisticated AI workflows that solve real-world problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
