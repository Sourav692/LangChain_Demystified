{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3377e45",
   "metadata": {},
   "source": [
    "# üîó Introduction to LangChain Expression Language (LCEL)\n",
    "\n",
    "## What is LCEL?\n",
    "\n",
    "**LangChain Expression Language (LCEL)** is a declarative way to compose chains in LangChain. It provides a simple, intuitive syntax using the **pipe operator (`|`)** to connect different components together.\n",
    "\n",
    "### Key Benefits of LCEL:\n",
    "- **üöÄ Simplified Syntax**: Chain components using the intuitive `|` operator\n",
    "- **‚ö° First-class Streaming Support**: Get tokens as soon as they're available from the LLM\n",
    "- **üîÑ Async Support**: Same chain works for both sync and async operations\n",
    "- **üîß Optimized Parallel Execution**: Automatically runs independent steps in parallel\n",
    "- **üìù Transparent Tracing**: Easy debugging with LangSmith integration\n",
    "\n",
    "### How LCEL Works:\n",
    "```\n",
    "Input ‚Üí Component1 | Component2 | Component3 ‚Üí Output\n",
    "```\n",
    "\n",
    "Each component in the chain:\n",
    "1. Receives input from the previous component\n",
    "2. Processes it\n",
    "3. Passes the output to the next component\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives:\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the basic LCEL syntax\n",
    "2. Create a simple LLM chain using LCEL\n",
    "3. Execute the chain with custom inputs\n",
    "4. See a practical example with SQL query generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì¶ STEP 1: Install Required Libraries (Run once if needed)\n",
    "# ============================================================\n",
    "# \n",
    "# langchain: The core LangChain library for building LLM applications\n",
    "# langchain-openai: Integration package for OpenAI models with LangChain\n",
    "#\n",
    "# Uncomment the lines below if you haven't installed these packages:\n",
    "\n",
    "# !pip install -qq langchain==0.3.11\n",
    "# !pip install -qq langchain-openai==0.2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa486f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_community: Contains community-contributed integrations\n",
    "# (Required for SQLDatabase utility used later in this notebook)\n",
    "\n",
    "# !pip install -qq langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce031c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîë Step 2: Setup OpenAI API Credentials\n",
    "\n",
    "Before using OpenAI models, you need to configure your API key. We use the `python-dotenv` library to securely load environment variables from a `.env` file.\n",
    "\n",
    "**Important**: Create a `.env` file in your project root with:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Security Tip**: Never hardcode API keys in your notebooks or commit them to version control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv() reads the .env file and loads all key-value pairs\n",
    "# as environment variables. Returns True if successful.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414e3ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Step 3: Initialize the LLM (Chat Model)\n",
    "\n",
    "LangChain provides the `ChatOpenAI` class to interact with OpenAI's chat models. This is a **Runnable** component - the building block of LCEL chains.\n",
    "\n",
    "### Key Parameters:\n",
    "- `model_name`: The specific model to use (e.g., \"gpt-4o-mini\", \"gpt-4\", \"gpt-3.5-turbo\")\n",
    "- `temperature`: Controls randomness (0 = deterministic, 1 = creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatOpenAI class from langchain_openai package\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the Chat Model\n",
    "# - model_name: \"gpt-4o-mini\" is a cost-effective, fast model good for learning\n",
    "# - temperature=0: Makes responses deterministic and focused (no randomness)\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# The 'chatgpt' object is now a Runnable that can be used in LCEL chains\n",
    "print(f\"Model initialized: {chatgpt.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800882e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚õìÔ∏è Step 4: Create an LCEL Chain\n",
    "\n",
    "Now comes the exciting part - building our first LCEL chain!\n",
    "\n",
    "### The Pipe Operator (`|`)\n",
    "In LCEL, the `|` operator connects components together:\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "```\n",
    "\n",
    "### Our Chain Structure:\n",
    "```\n",
    "User Input ‚Üí PromptTemplate ‚Üí ChatModel ‚Üí Output\n",
    "     ‚Üì              ‚Üì              ‚Üì\n",
    "  {\"topic\":    Formats the     Generates\n",
    "   \"AI\"}       prompt with      response\n",
    "               the topic\n",
    "```\n",
    "\n",
    "### What is a PromptTemplate?\n",
    "A `PromptTemplate` is a reusable template that:\n",
    "1. Accepts variables (like `{topic}`)\n",
    "2. Formats them into a proper prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ChatPromptTemplate - used to create structured prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4a: Define the Prompt Template\n",
    "# ============================================================\n",
    "# The {topic} is a placeholder that will be replaced with actual values\n",
    "# when we invoke the chain\n",
    "\n",
    "prompt_txt = \"Explain {topic} in 1 line\"\n",
    "\n",
    "# Create a ChatPromptTemplate from the string template\n",
    "# This automatically converts our string into a proper chat message format\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# Let's see what the template looks like\n",
    "print(\"üìã Prompt Template Variables:\", prompt_template.input_variables)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4b: Build the LCEL Chain using the Pipe Operator\n",
    "# ============================================================\n",
    "# The \"|\" operator chains components together:\n",
    "#   - Input flows from LEFT to RIGHT\n",
    "#   - Each component processes and passes data to the next\n",
    "\n",
    "# Method 1: Multi-line format (better readability for complex chains)\n",
    "llmchain = (\n",
    "    prompt_template  # First: Format the prompt with input variables\n",
    "    |                # Pipe operator: Pass output to next component\n",
    "    chatgpt          # Second: Send formatted prompt to LLM\n",
    ")\n",
    "\n",
    "# Method 2: Single-line format (equivalent, more compact)\n",
    "# llmchain = prompt_template | chatgpt\n",
    "\n",
    "print(\"‚úÖ Chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a767d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Step 5: Execute the Chain\n",
    "\n",
    "Now let's run our chain! Every LCEL chain has these key methods:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `.invoke()` | Run the chain synchronously (waits for complete response) |\n",
    "| `.stream()` | Stream tokens as they're generated |\n",
    "| `.batch()` | Process multiple inputs in parallel |\n",
    "| `.ainvoke()` | Async version of invoke |\n",
    "\n",
    "### Using `.invoke()`\n",
    "The `invoke()` method:\n",
    "1. Takes a dictionary with input variables\n",
    "2. Passes them through each component\n",
    "3. Returns the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58849480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Execute the chain with .invoke()\n",
    "# ============================================================\n",
    "# Pass a dictionary with the required variable(s)\n",
    "# The key 'topic' matches the {topic} placeholder in our template\n",
    "\n",
    "response = llmchain.invoke({'topic': 'Generative AI'})\n",
    "\n",
    "# The response is an AIMessage object from LangChain\n",
    "# - response.content: The actual text response from the LLM\n",
    "# - response.response_metadata: Additional info (tokens used, model, etc.)\n",
    "\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.content)\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nüìä Response Type: {type(response).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e807d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Bonus: Real-World Example - SQL Query Generation\n",
    "\n",
    "Let's see LCEL in action with a practical use case: **generating SQL queries from natural language questions**.\n",
    "\n",
    "LangChain provides a pre-built chain `create_sql_query_chain` that:\n",
    "1. Connects to your database\n",
    "2. Understands the schema\n",
    "3. Converts natural language to SQL\n",
    "\n",
    "This demonstrates how LCEL enables complex, production-ready chains!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0597c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SQL Query Generation Example using LCEL\n",
    "# ============================================================\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# Step 1: Connect to the database\n",
    "# SQLDatabase.from_uri() creates a connection to your database\n",
    "# Here we're using the Chinook sample database (a music store DB)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# Step 2: Initialize a new LLM instance for SQL generation\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Step 3: Create the SQL query chain\n",
    "# This pre-built chain uses LCEL internally and:\n",
    "#   - Analyzes the database schema\n",
    "#   - Understands table relationships\n",
    "#   - Generates appropriate SQL queries\n",
    "sql_chain = create_sql_query_chain(llm, db)\n",
    "\n",
    "# Step 4: Invoke the chain with a natural language question\n",
    "response = sql_chain.invoke({\"question\": \"How many employees are there\"})\n",
    "\n",
    "print(\"üí¨ Question: How many employees are there?\")\n",
    "print(\"üìù Generated SQL Query:\")\n",
    "print(response)\n",
    "\n",
    "# ============================================================\n",
    "# üí° Key Insight:\n",
    "# The LLM understood our natural language question and:\n",
    "# 1. Identified the relevant table (Employees)\n",
    "# 2. Generated the correct SQL syntax\n",
    "# 3. Used COUNT() aggregate function appropriately\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ede80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üí° Key Insight:\n",
    "# ============================================================\n",
    "# The LLM understood our natural language question and:\n",
    "# 1. Identified the relevant table (Employees)\n",
    "# 2. Generated the correct SQL syntax\n",
    "# 3. Used COUNT() aggregate function appropriately\n",
    "#\n",
    "# Try asking different questions like:\n",
    "# - \"What are the names of all artists?\"\n",
    "# - \"Show me the top 5 most expensive tracks\"\n",
    "# - \"How many albums does each artist have?\"\n",
    "# ============================================================\n",
    "\n",
    "# Execute the generated query on the database to verify it works\n",
    "result = db.run(response)\n",
    "print(f\"‚úÖ Query Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4043a",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of LCEL. Here's what we covered:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **LCEL** | LangChain Expression Language - a declarative way to compose chains |\n",
    "| **Pipe Operator (`\\|`)** | Connects components, data flows left to right |\n",
    "| **Runnable** | Any component that can be part of an LCEL chain |\n",
    "| **PromptTemplate** | Reusable template with variable placeholders |\n",
    "| **invoke()** | Method to execute a chain with input data |\n",
    "\n",
    "### The LCEL Pattern:\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "result = chain.invoke({\"input_key\": \"input_value\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Modify the prompt**: Change the prompt to \"Explain {topic} to a 5-year-old\" and test with different topics\n",
    "\n",
    "2. **Add more variables**: Create a prompt with multiple variables like \"Explain {topic} in {language} language\"\n",
    "\n",
    "3. **Try different models**: Replace `gpt-4o-mini` with `gpt-3.5-turbo` and compare responses\n",
    "\n",
    "4. **Experiment with temperature**: Try `temperature=0.7` or `temperature=1.0` and observe the differences\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "Continue your LCEL journey by exploring:\n",
    "- **Runnables**: Learn about RunnablePassthrough, RunnableLambda, and more\n",
    "- **Output Parsers**: Structure LLM outputs into Python objects\n",
    "- **Chains**: Build more complex multi-step chains\n",
    "- **Streaming**: Get real-time token-by-token responses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
