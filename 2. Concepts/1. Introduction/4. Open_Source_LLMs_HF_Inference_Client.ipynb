{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d779e870",
   "metadata": {},
   "source": [
    "## Using LLMs via Hugging Face Inference Client\n",
    "\n",
    "### What is the Hugging Face Inference Client?\n",
    "\n",
    "The **Hugging Face Inference Client** is a powerful Python library that allows you to interact with Large Language Models (LLMs) hosted on Hugging Face's servers ‚Äî **without needing to download or run the models locally**.\n",
    "\n",
    "### Why Use It?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Free Tier Available** | HuggingFace offers a [free inference API](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client) with basic rate limits |\n",
    "| **No Infrastructure Needed** | Access 150,000+ models without GPU/hardware requirements |\n",
    "| **Easy to Use** | Simple Python API similar to OpenAI's client |\n",
    "| **Wide Model Selection** | Access to latest open-source models like Llama, Mistral, etc. |\n",
    "\n",
    "### Prerequisites\n",
    "- A Hugging Face account (free)\n",
    "- A Hugging Face API token (get it from [Settings > Access Tokens](https://huggingface.co/settings/tokens))\n",
    "- `huggingface_hub` library installed (`pip install huggingface_hub`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Import the Hugging Face Hub Library\n",
    "# =============================================================================\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "# Print the version to ensure compatibility\n",
    "# IMPORTANT: Version should be >= 0.36.0 for Inference Providers to work properly\n",
    "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "\n",
    "# Import the InferenceClient class\n",
    "# This is the main class we'll use to interact with HuggingFace's hosted models\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897019eb",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up Authentication\n",
    "\n",
    "To use the Inference API, you need to authenticate with your Hugging Face API token. We'll load it securely from environment variables using the `python-dotenv` library.\n",
    "\n",
    "> üîê **Security Best Practice**: Never hardcode your API tokens directly in code. Always use environment variables or secret management tools.\n",
    "\n",
    "üìö **Documentation**: Feel free to refer to the [official InferenceClient documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) for more details on available methods and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a05cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Load API Token from Environment Variables\n",
    "# =============================================================================\n",
    "\n",
    "from dotenv import load_dotenv  # Library to load variables from .env file\n",
    "import os  # Standard library for OS operations\n",
    "\n",
    "# Load environment variables from a .env file in the project root\n",
    "# Your .env file should contain: HF_TOKEN=your_huggingface_api_token_here\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Hugging Face API token from environment variables\n",
    "# This token authenticates your requests to the HuggingFace Inference API\n",
    "hf_key = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Optional: Verify the token was loaded (don't print the actual token!)\n",
    "if hf_key:\n",
    "    print(\"‚úÖ HuggingFace API token loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: HF_TOKEN not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6aa82",
   "metadata": {},
   "source": [
    "### Step 3: Making Your First API Call\n",
    "\n",
    "Now let's use the `InferenceClient` to interact with a Large Language Model. We'll use **Meta's Llama 3.1 8B Instruct** model, which is:\n",
    "- An open-source model available for free\n",
    "- Instruction-tuned (optimized to follow instructions)\n",
    "- 8 billion parameters (good balance between quality and speed)\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Chat Completion**: A conversation-style API where you send messages with roles (user, assistant, system)\n",
    "- **Messages Format**: A list of dictionaries with `role` and `content` keys\n",
    "- **max_tokens**: Controls the maximum length of the generated response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Here are 2 bullet points explaining what Generative AI is:\\n\\n‚Ä¢ **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\\n\\n‚Ä¢ **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.', reasoning=None, tool_call_id=None, tool_calls=None), logprobs=None, content_filter_results={'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}})], created=1765329641, id='a01f72feefbb4acdbf0915ca0c1d559c', model='meta-llama/llama-3.1-8b-instruct', system_fingerprint='', usage=ChatCompletionOutputUsage(completion_tokens=148, prompt_tokens=47, total_tokens=195, prompt_tokens_details=None, completion_tokens_details=None), object='chat.completion')\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Create the Inference Client and Make a Chat Completion Request\n",
    "# =============================================================================\n",
    "\n",
    "# Define the model to use\n",
    "# Format: \"organization/model-name\"\n",
    "# Note: Only models with \"warm\" inference status work with the free API\n",
    "# You can find available models at: https://huggingface.co/models?inference=warm\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Initialize the InferenceClient with your API token\n",
    "# This client handles all communication with HuggingFace's servers\n",
    "client = InferenceClient(token=hf_key)\n",
    "\n",
    "# Define the conversation as a list of messages\n",
    "# Each message has:\n",
    "#   - \"role\": Who is speaking (\"system\", \"user\", or \"assistant\")\n",
    "#   - \"content\": The actual message text\n",
    "# \n",
    "# Common roles:\n",
    "#   - \"system\": Sets the behavior/personality of the AI (optional)\n",
    "#   - \"user\": Messages from the human user\n",
    "#   - \"assistant\": Previous responses from the AI (for multi-turn conversations)\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain what is Generative AI in 2 bullet points\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Make the API call using chat_completion()\n",
    "# Parameters:\n",
    "#   - messages: The conversation history (our 'chat' list)\n",
    "#   - model: Which model to use for generation\n",
    "#   - max_tokens: Maximum number of tokens in the response (1 token ‚âà 4 characters)\n",
    "response = client.chat_completion(chat, model=model_name, max_tokens=1000)\n",
    "\n",
    "# Print the full response object to see its structure\n",
    "print(\"Full API Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8736a",
   "metadata": {},
   "source": [
    "### Step 4: Extracting the Response\n",
    "\n",
    "The API returns a `ChatCompletionOutput` object with several fields:\n",
    "- `choices`: List of generated responses (usually just one)\n",
    "- `id`: Unique identifier for this request\n",
    "- `model`: The model that was used\n",
    "- `usage`: Token usage statistics (prompt_tokens, completion_tokens, total_tokens)\n",
    "\n",
    "To get just the text response, we need to navigate: `response.choices[0].message.content`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc33d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 bullet points explaining what Generative AI is:\n",
      "\n",
      "‚Ä¢ **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\n",
      "\n",
      "‚Ä¢ **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Extract the Generated Text from the Response\n",
    "# =============================================================================\n",
    "\n",
    "# The response structure is:\n",
    "# response\n",
    "#   ‚îî‚îÄ‚îÄ choices (list of completions)\n",
    "#       ‚îî‚îÄ‚îÄ [0] (first/only choice)\n",
    "#           ‚îî‚îÄ‚îÄ message\n",
    "#               ‚îî‚îÄ‚îÄ content (the actual generated text)\n",
    "\n",
    "# Extract just the text content\n",
    "generated_text = response.choices[0].message.content\n",
    "print(\"Generated Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(generated_text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Bonus: Let's also look at the token usage\n",
    "print(f\"\\nüìä Token Usage Statistics:\")\n",
    "print(f\"   - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   - Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"   - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7cd26",
   "metadata": {},
   "source": [
    "### Bonus: Advanced Usage with System Prompt\n",
    "\n",
    "You can customize the AI's behavior using a **system prompt**. This is especially useful for:\n",
    "- Setting a specific persona or role\n",
    "- Defining output format requirements\n",
    "- Establishing constraints or guidelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f076af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with System Prompt:\n",
      "--------------------------------------------------\n",
      "Imagine you're at a restaurant and you want to order food. You can't just walk into the kitchen and start making your own food, right? You need to tell the waiter what you want, and they'll order it for you.\n",
      "\n",
      "An API (Application Programming Interface) is like the waiter. You give the waiter (API) instructions (requests), and they go to the kitchen (server) to get what you need (data). The waiter then brings back the data (response) to you.\n",
      "\n",
      "In code, you send a request to the API, and it returns data that you can use in your program. APIs help different apps and systems talk to each other and share data.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Bonus: Using System Prompts to Customize AI Behavior\n",
    "# =============================================================================\n",
    "\n",
    "# Define a conversation with a system prompt\n",
    "# The system prompt sets the AI's persona and behavior rules\n",
    "chat_with_system = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful coding tutor. Explain concepts simply and use analogies. Keep responses concise.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is an API?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make the request with the system prompt\n",
    "response_with_system = client.chat_completion(\n",
    "    chat_with_system,\n",
    "    model=model_name,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"Response with System Prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_with_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5fa08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Set up** the Hugging Face Inference Client\n",
    "2. **Authenticate** using API tokens stored in environment variables\n",
    "3. **Make API calls** to open-source LLMs hosted on HuggingFace\n",
    "4. **Parse responses** to extract the generated text\n",
    "5. **Use system prompts** to customize AI behavior\n",
    "\n",
    "## üîó Additional Resources\n",
    "\n",
    "- [HuggingFace Inference Client Documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n",
    "- [Available Models for Inference](https://huggingface.co/models?inference=warm)\n",
    "- [Chat Completion API Reference](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient.chat_completion)\n",
    "\n",
    "## üí° Try It Yourself\n",
    "\n",
    "Experiment with:\n",
    "- Different models (e.g., `mistralai/Mistral-7B-Instruct-v0.2`)\n",
    "- Different `max_tokens` values\n",
    "- Adding multi-turn conversations\n",
    "- Using different system prompts to change the AI's personality\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
