{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d779e870",
   "metadata": {},
   "source": [
    "## Using LLMs via Hugging Face Inference Client\n",
    "\n",
    "Thankfully HuggingFace has made its new [__Inference Client__](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on its servers.\n",
    "\n",
    "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure. Similar to the inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/Codebases/2. AI ENGINEERING/LangChain_Demystified/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "# Version should be >= 0.36.0 for Inference Providers to work\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897019eb",
   "metadata": {},
   "source": [
    "Feel free to refer to the [documentation](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) at any time as needed for more details on function names, arguments and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a05cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_key = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7901175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Here are 2 bullet points explaining what Generative AI is:\\n\\n• **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\\n\\n• **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.', reasoning=None, tool_call_id=None, tool_calls=None), logprobs=None, content_filter_results={'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}})], created=1765329641, id='a01f72feefbb4acdbf0915ca0c1d559c', model='meta-llama/llama-3.1-8b-instruct', system_fingerprint='', usage=ChatCompletionOutputUsage(completion_tokens=148, prompt_tokens=47, total_tokens=195, prompt_tokens_details=None, completion_tokens_details=None), object='chat.completion')\n"
     ]
    }
   ],
   "source": [
    "# Using a model available via HuggingFace Inference Providers\n",
    "# Note: Only models with \"warm\" inference status work with the free API\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "client = InferenceClient(token=hf_key)\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\" },\n",
    "]\n",
    "\n",
    "response = client.chat_completion(chat, model=model_name, max_tokens=1000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc33d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 bullet points explaining what Generative AI is:\n",
      "\n",
      "• **Definition**: Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos using algorithms and machine learning models. These models are trained on large datasets and can learn patterns, styles, and structures to generate new content that is often indistinguishable from human-created work.\n",
      "\n",
      "• **Applications**: Generative AI has numerous applications across various industries, including art and design, music and audio production, writing and content creation, and even product design. Some examples of generative AI include generating realistic images of people, creating new music tracks, or producing automated content such as news articles or social media posts.\n"
     ]
    }
   ],
   "source": [
    "# Extract the message content from the ChatCompletion response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7cd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
