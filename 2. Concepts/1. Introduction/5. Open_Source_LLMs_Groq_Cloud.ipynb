{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320018f9",
   "metadata": {},
   "source": [
    "# Using Open Source LLMs with Groq Cloud\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll learn how to use **Groq Cloud** to access and run open-source Large Language Models (LLMs) like Meta's Llama models.\n",
    "\n",
    "### What is Groq?\n",
    "Groq is a cloud platform that provides ultra-fast inference for open-source LLMs. It uses custom hardware called **Language Processing Units (LPUs)** designed specifically for AI inference, making it one of the fastest ways to run LLMs.\n",
    "\n",
    "### Key Benefits of Groq:\n",
    "- **Blazing Fast Inference**: LPU technology provides exceptionally low latency\n",
    "- **Free Tier Available**: Generous free tier for experimentation and learning\n",
    "- **Access to Latest Models**: Run state-of-the-art open-source models like Llama, Mixtral, and more\n",
    "- **Simple API**: OpenAI-compatible API for easy integration\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How to set up Groq API authentication\n",
    "2. How to create a Groq client\n",
    "3. How to send prompts and receive completions\n",
    "4. How to use different Llama model variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521b51a",
   "metadata": {},
   "source": [
    "## Get Groq API\n",
    "\n",
    "Here you need to get an access token to be able to access models using Grok's platform via APIs:\n",
    "\n",
    "- Groq API Key: Go [here](https://console.groq.com/keys) and create an API key. You need to setup an account which is totally free of cost. Also while Groq has a generous free tier, there are also paid plans if you are interested.\n",
    "\n",
    "\n",
    "1. Go to [Groq Cloud -> Create API Key](https://console.groq.com/keys) after creating your account and make sure to create a new API Key as shown\n",
    "\n",
    "![](https://i.imgur.com/tgHXlcV.png)\n",
    "\n",
    "2. Remember to __Save__ your key somewhere safe as it will just be shown once as shown below. So copy and save it in a local secure file to use it later on. If you forget, just create a new key anytime.\n",
    "\n",
    "![](https://i.imgur.com/Q27AgA1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5e247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Load Environment Variables\n",
    "# ============================================================================\n",
    "# We use python-dotenv to securely load our API key from a .env file\n",
    "# This is a best practice to keep sensitive credentials out of your code\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load all environment variables from .env file into the environment\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Groq API key from environment variables\n",
    "# Your .env file should contain: GROQ_API_KEY=your_api_key_here\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Verify the key was loaded (optional - for debugging)\n",
    "# print(\"API Key loaded:\", \"Yes\" if groq_key else \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9665",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Groq Client\n",
    "\n",
    "The Groq Python SDK provides a simple interface to interact with Groq's API. The client handles:\n",
    "- Authentication with your API key\n",
    "- Request/response serialization\n",
    "- Error handling and retries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c91eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Create the Groq Client\n",
    "# ============================================================================\n",
    "# The Groq SDK follows a similar pattern to OpenAI's SDK, making it easy\n",
    "# to switch between providers\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client with your API key\n",
    "# This client will be used to make all API calls to Groq's servers\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "\n",
    "# Note: You can also set GROQ_API_KEY as an environment variable\n",
    "# and initialize without passing the key: groq_client = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87816374",
   "metadata": {},
   "source": [
    "## Step 3: Create a Helper Function for Chat Completions\n",
    "\n",
    "Let's create a reusable function that wraps the Groq API call. This function will:\n",
    "- Accept a user prompt and optional model parameter\n",
    "- Format the message in the chat completion format\n",
    "- Return just the text content from the response\n",
    "\n",
    "### Understanding the Chat Completion Format\n",
    "The chat completion API uses a **messages** array where each message has:\n",
    "- `role`: Can be \"system\", \"user\", or \"assistant\"\n",
    "- `content`: The actual text content of the message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Define Helper Function for Chat Completions\n",
    "# ============================================================================\n",
    "\n",
    "def get_completion_chatgroq(prompt, model=\"meta-llama/llama-4-scout-17b-16e-instruct\"):\n",
    "    \"\"\"\n",
    "    Send a prompt to Groq's chat completion API and get a response.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prompt : str\n",
    "        The user's input/question to send to the model\n",
    "    model : str\n",
    "        The model identifier to use (default: Llama 4 Scout)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The model's response text\n",
    "        \n",
    "    Available Models on Groq (as of 2024):\n",
    "    - meta-llama/llama-4-scout-17b-16e-instruct  : Llama 4 Scout (efficient)\n",
    "    - meta-llama/llama-4-maverick-17b-128e-instruct : Llama 4 Maverick (more capable)\n",
    "    - llama-3.3-70b-versatile : Llama 3.3 70B\n",
    "    - mixtral-8x7b-32768 : Mixtral 8x7B\n",
    "    - gemma2-9b-it : Google's Gemma 2 9B\n",
    "    \n",
    "    Check https://console.groq.com/docs/models for the latest available models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt as a chat message\n",
    "    # The messages array can contain multiple messages for multi-turn conversations\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Make the API call to Groq\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,        # Specify which LLM to use\n",
    "        messages=messages,  # The conversation history/prompt\n",
    "        temperature=0,      # Controls randomness (0 = deterministic, 1 = creative)\n",
    "        # Other optional parameters:\n",
    "        # max_tokens=1024,  # Maximum length of the response\n",
    "        # top_p=1,          # Nucleus sampling parameter\n",
    "        # stream=False,     # Whether to stream the response\n",
    "    )\n",
    "    \n",
    "    # Extract and return just the text content from the response\n",
    "    # The response structure: response.choices[0].message.content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322fab3",
   "metadata": {},
   "source": [
    "## Step 4: Test with Different Models\n",
    "\n",
    "Now let's test our function with different Llama 4 model variants. Groq provides access to various open-source models, each with different capabilities:\n",
    "\n",
    "### Llama 4 Scout vs Maverick\n",
    "- **Scout (16 experts)**: Faster, more efficient, good for most tasks\n",
    "- **Maverick (128 experts)**: More capable, better for complex reasoning tasks\n",
    "\n",
    "Let's compare their outputs for the same prompt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09856e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 bullet points explaining Generative AI:\n",
      "\n",
      "* **Creates new content**: Generative AI is a type of artificial intelligence that can generate new, original content, such as images, videos, music, text, and more. It uses complex algorithms and machine learning techniques to create this content, often based on patterns and structures learned from large datasets.\n",
      "* **Learns from data, not human input**: Unlike traditional AI systems that rely on human input and rules to generate output, Generative AI models learn from vast amounts of data and can produce novel, diverse, and often surprising results that may not be immediately recognizable as related to the training data.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 1: Using Llama 4 Scout Model\n",
    "# ============================================================================\n",
    "# Scout is the lighter, faster variant with 16 experts\n",
    "# Great for quick responses and standard tasks\n",
    "\n",
    "prompt = 'Explain Generative AI in 2 bullet points'\n",
    "\n",
    "# Call our helper function with the Scout model\n",
    "response = get_completion_chatgroq(\n",
    "    prompt=prompt, \n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LLAMA 4 SCOUT RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1512eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 bullet points explaining Generative AI:\n",
      "\n",
      "* **Generative AI creates new content**: Generative AI uses complex algorithms to generate new, original content, such as images, videos, music, text, or code, that is similar in style and structure to the data it was trained on.\n",
      "* **Trained on existing data to learn patterns**: Generative AI models are trained on large datasets to learn patterns, relationships, and structures within the data, allowing them to generate new content that is often realistic and coherent, and sometimes even creative or surprising.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 2: Using Llama 4 Maverick Model\n",
    "# ============================================================================\n",
    "# Maverick is the more powerful variant with 128 experts\n",
    "# Better for complex reasoning and nuanced responses\n",
    "\n",
    "prompt = 'Explain Generative AI in 2 bullet points'\n",
    "\n",
    "# Call our helper function with the Maverick model\n",
    "response = get_completion_chatgroq(\n",
    "    prompt=prompt, \n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LLAMA 4 MAVERICK RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "\n",
    "# Notice how both models give similar but slightly different responses\n",
    "# The Maverick model may provide more nuanced explanations for complex topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ed86a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned how to:\n",
    "\n",
    "1. **Set up Groq API authentication** using environment variables\n",
    "2. **Initialize the Groq client** for making API calls\n",
    "3. **Create a reusable helper function** for chat completions\n",
    "4. **Use different Llama 4 model variants** (Scout vs Maverick)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Speed** | Groq's LPU technology provides ultra-fast inference |\n",
    "| **Cost** | Free tier available for learning and experimentation |\n",
    "| **Models** | Access to latest open-source models (Llama, Mixtral, Gemma) |\n",
    "| **API** | OpenAI-compatible format makes migration easy |\n",
    "\n",
    "### Next Steps\n",
    "- Try different models available on Groq\n",
    "- Experiment with the `temperature` parameter for more creative responses\n",
    "- Build multi-turn conversations using the messages array\n",
    "- Explore streaming responses for real-time output\n",
    "\n",
    "### Resources\n",
    "- [Groq Documentation](https://console.groq.com/docs)\n",
    "- [Available Models](https://console.groq.com/docs/models)\n",
    "- [Rate Limits & Pricing](https://console.groq.com/docs/rate-limits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
