{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ea846c",
   "metadata": {},
   "source": [
    "# Using Open Source LLMs Natively with Hugging Face Transformers\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. **Understand** what Hugging Face Transformers library is and why it's important\n",
    "2. **Load** pre-trained Large Language Models (LLMs) locally on your machine\n",
    "3. **Use** tokenizers to prepare text input for LLMs\n",
    "4. **Generate** text responses using the model's `generate()` method\n",
    "5. **Simplify** the workflow using Hugging Face Pipelines\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of what LLMs are\n",
    "- A Hugging Face account (free)\n",
    "\n",
    "## üîß What is Hugging Face Transformers?\n",
    "\n",
    "**Hugging Face Transformers** is an open-source library that provides:\n",
    "- Access to thousands of pre-trained models for NLP, computer vision, and audio tasks\n",
    "- Easy-to-use APIs for downloading and using these models\n",
    "- Tools for fine-tuning models on your own data\n",
    "\n",
    "**Key Advantage**: Run models locally without sending data to external servers (privacy-friendly!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3d5f2",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885ff15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üì¶ INSTALLING REQUIRED PACKAGES\n",
    "# ============================================================================\n",
    "# Uncomment and run these lines if you haven't installed the packages yet\n",
    "#\n",
    "# transformers: The main Hugging Face library for working with pre-trained models\n",
    "# accelerate: Enables efficient model loading and GPU/CPU optimization\n",
    "# groq: SDK for Groq Cloud inference (alternative to local inference)\n",
    "#\n",
    "# The -qq flag suppresses output for cleaner installation logs\n",
    "# ============================================================================\n",
    "\n",
    "# !pip install -qq transformers==4.47.0\n",
    "# !pip install -qq accelerate==1.1.0\n",
    "# !pip install -qq groq==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937dc9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üî• PYTORCH INSTALLATION\n",
    "# ============================================================================\n",
    "# PyTorch is the deep learning framework that powers Hugging Face Transformers.\n",
    "# It handles tensor operations and GPU computations under the hood.\n",
    "#\n",
    "# Choose ONE of these options based on your setup:\n",
    "# - Option 1: Install specific version (for reproducibility)\n",
    "# - Option 2: Install with all components (torch, torchvision, torchaudio)\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Install specific version\n",
    "# pip install -qq torch==2.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402af1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Install all PyTorch components (recommended for full functionality)\n",
    "# - torch: Core deep learning library\n",
    "# - torchvision: Computer vision utilities (not required for text LLMs)\n",
    "# - torchaudio: Audio processing utilities (not required for text LLMs)\n",
    "# pip install -qq torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb9c4f",
   "metadata": {},
   "source": [
    "## Get Hugging Face Access Token\n",
    "\n",
    "Here you need to get an access token to be able to download or access models using Hugging Face's platform:\n",
    "\n",
    "- Hugging Face Access Token: Go [here](https://huggingface.co/settings/tokens) and create a key with write permissions. You need to setup an account which is totally free of cost.\n",
    "\n",
    "\n",
    "1. Go to [Settings -> Access Tokens](https://huggingface.co/settings/tokens) after creating your account and make sure to create a new access token with write permissions\n",
    "\n",
    "![](https://i.imgur.com/dtS6tFr.png)\n",
    "\n",
    "2. Remember to __Save__ your key somewhere safe as it will just be shown once as shown below. So copy and save it in a local secure file to use it later on. If you forget, just create a new key anytime.\n",
    "\n",
    "![](https://i.imgur.com/NmZmpmw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e08b4b",
   "metadata": {},
   "source": [
    "## Load Hugging Face Access Token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f9a5e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üîê LOADING ENVIRONMENT VARIABLES\n",
    "# ============================================================================\n",
    "# We use python-dotenv to load sensitive information (like API keys) from a \n",
    "# .env file. This keeps your credentials secure and out of your code.\n",
    "#\n",
    "# Your .env file should contain:\n",
    "#   HUGGINGFACE_API_KEY=your_token_here\n",
    "#\n",
    "# ‚ö†Ô∏è  NEVER commit your .env file to version control (add it to .gitignore)\n",
    "# ============================================================================\n",
    "\n",
    "from dotenv import load_dotenv  # Library to load environment variables from .env file\n",
    "import os                        # Standard library for OS operations\n",
    "\n",
    "# load_dotenv() searches for a .env file and loads its contents as environment variables\n",
    "# Returns True if .env file was found and loaded successfully\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3564916",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üñ•Ô∏è Part 1: Using LLMs Locally with Hugging Face\n",
    "\n",
    "### Why Run Models Locally?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Privacy** | Your data never leaves your machine |\n",
    "| **No API Costs** | Once downloaded, use unlimited times for free |\n",
    "| **Offline Access** | Works without internet connection |\n",
    "| **Customization** | Full control over model parameters |\n",
    "\n",
    "### ‚ö†Ô∏è Hardware Requirements\n",
    "\n",
    "Running LLMs locally requires significant computational resources:\n",
    "\n",
    "- **GPU Recommended**: Even small models (1-3B parameters) benefit greatly from GPU acceleration\n",
    "- **RAM**: At least 8GB for small models, 16GB+ for medium models\n",
    "- **Storage**: Models can range from 2GB to 100GB+ depending on size\n",
    "\n",
    "> **üí° Tip**: If you don't have a GPU, consider using Hugging Face Inference API or Groq Cloud (covered in other notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7efae1",
   "metadata": {},
   "source": [
    "### üîí Understanding Gated Models\n",
    "\n",
    "Some LLMs on Hugging Face are **\"gated\"** - meaning you need to accept terms and conditions before accessing them.\n",
    "\n",
    "**Examples of Gated Models:**\n",
    "- [Meta Llama 3.2 1B Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "- [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "**How to Request Access:**\n",
    "1. Go to the model page on Hugging Face\n",
    "2. Click on \"Request Access\" or \"Agree and access repository\"\n",
    "3. Fill out the required form (usually just accept terms)\n",
    "4. Wait for approval (usually instant for most models)\n",
    "\n",
    "![](https://i.imgur.com/M88MOu5.png)\n",
    "\n",
    "> **Note**: For this tutorial, we'll use **TinyLlama** - an open (non-gated) model that works well for learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1a558",
   "metadata": {},
   "source": [
    "### Step 1: Load the LLM and Tokenizer\n",
    "\n",
    "Every LLM requires two main components:\n",
    "\n",
    "1. **Tokenizer**: Converts text to numbers (tokens) that the model can understand\n",
    "2. **Model**: The actual neural network that generates predictions\n",
    "\n",
    "```\n",
    "Text Input ‚Üí [Tokenizer] ‚Üí Token IDs ‚Üí [Model] ‚Üí Token IDs ‚Üí [Tokenizer] ‚Üí Text Output\n",
    "```\n",
    "\n",
    "**About TinyLlama:**\n",
    "- Size: 1.1 Billion parameters\n",
    "- Based on Llama 2 architecture\n",
    "- Trained on 3 trillion tokens\n",
    "- Great for learning and experimentation due to small size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6cdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üöÄ LOADING THE MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "# Import required libraries from Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Define the Model ID\n",
    "# ============================================================================\n",
    "# The model_id is the unique identifier on Hugging Face Hub\n",
    "# Format: \"organization_name/model_name\" or \"username/model_name\"\n",
    "# You can find model IDs by browsing: https://huggingface.co/models\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load the Tokenizer\n",
    "# ============================================================================\n",
    "# AutoTokenizer automatically detects and loads the correct tokenizer class\n",
    "# based on the model. It handles:\n",
    "#   - Vocabulary loading\n",
    "#   - Special tokens (like <|user|>, </s>, etc.)\n",
    "#   - Text encoding/decoding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Load the Model\n",
    "# ============================================================================\n",
    "# AutoModelForCausalLM loads models designed for text generation (causal LM)\n",
    "# \n",
    "# Key Parameters:\n",
    "#   - model_id: The Hugging Face model identifier\n",
    "#   - torch_dtype: Data type for model weights\n",
    "#       ‚Ä¢ torch.float32: Full precision (more accurate, uses more memory)\n",
    "#       ‚Ä¢ torch.float16: Half precision (good balance)\n",
    "#       ‚Ä¢ torch.bfloat16: Brain float (best for modern GPUs, handles larger ranges)\n",
    "#\n",
    "# üí° Using bfloat16 reduces memory usage by ~50% with minimal quality loss\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16  # Use bfloat16 for better memory efficiency\n",
    ")\n",
    "\n",
    "# Note: First run will download the model (~2GB for TinyLlama)\n",
    "# Subsequent runs will load from cache (~/.cache/huggingface/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefb229",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Your Prompt Using Chat Templates\n",
    "\n",
    "Modern chat LLMs expect input in a specific format with **special tokens** to distinguish between:\n",
    "- User messages\n",
    "- Assistant responses\n",
    "- System instructions\n",
    "\n",
    "**Why Chat Templates Matter:**\n",
    "- Each model family (Llama, Mistral, etc.) uses different formatting\n",
    "- Using wrong format = poor quality responses\n",
    "- `apply_chat_template()` automatically formats your messages correctly\n",
    "\n",
    "**Example Format (TinyLlama/Llama style):**\n",
    "```\n",
    "<|user|>\n",
    "Your question here</s>\n",
    "<|assistant|>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad0aef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FORMATTED PROMPT:\n",
      "==================================================\n",
      "<|user|>\n",
      "Explain what is Generative AI in 2 bullet points</s>\n",
      "<|assistant|>\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üìù CREATING THE CHAT MESSAGE\n",
    "# ============================================================================\n",
    "# Chat messages are structured as a list of dictionaries\n",
    "# Each message has:\n",
    "#   - \"role\": Who is speaking (\"user\", \"assistant\", or \"system\")\n",
    "#   - \"content\": The actual message text\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\"},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# üîÑ APPLYING THE CHAT TEMPLATE\n",
    "# ============================================================================\n",
    "# apply_chat_template() converts your structured messages into the format\n",
    "# the model expects\n",
    "#\n",
    "# Parameters:\n",
    "#   - chat: The list of message dictionaries\n",
    "#   - tokenize: False = return string, True = return token IDs\n",
    "#   - add_generation_prompt: True = add the assistant turn start token\n",
    "#                           (signals the model to start generating)\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    chat, \n",
    "    tokenize=False,            # Return human-readable string (not token IDs)\n",
    "    add_generation_prompt=True  # Add \"<|assistant|>\\n\" to prompt generation\n",
    ")\n",
    "\n",
    "# Let's see what the formatted prompt looks like:\n",
    "print(\"=\" * 50)\n",
    "print(\"FORMATTED PROMPT:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6f606",
   "metadata": {},
   "source": [
    "### Step 3: Generate Text with the Model\n",
    "\n",
    "Now we'll use the model's `generate()` method to produce a response. Understanding the generation parameters is crucial for controlling output quality.\n",
    "\n",
    "üìö **[Full Documentation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate)**\n",
    "\n",
    "#### Key Generation Parameters:\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `max_length` | Maximum total length (input + output) | 512, 1024, 2048 |\n",
    "| `max_new_tokens` | Maximum tokens to generate (output only) | 100, 500, 1000 |\n",
    "| `do_sample` | Enable random sampling | `True` (creative) / `False` (deterministic) |\n",
    "| `temperature` | Controls randomness | 0.0-1.0 (higher = more creative) |\n",
    "| `top_p` | Nucleus sampling threshold | 0.9-0.95 |\n",
    "| `top_k` | Limit vocabulary choices | 50 |\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: Use either `max_new_tokens` OR `max_length`, not both!\n",
    "\n",
    "#### Temperature Explained:\n",
    "- **0.0**: Greedy decoding (always picks highest probability token) - deterministic\n",
    "- **0.3-0.5**: More focused, consistent outputs\n",
    "- **0.7-0.9**: Balanced creativity and coherence\n",
    "- **1.0+**: Very creative but potentially incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9bf642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Input shape: torch.Size([1, 29])\n",
      "üìä Number of input tokens: 29\n",
      "\n",
      "============================================================\n",
      "ü§ñ MODEL RESPONSE:\n",
      "============================================================\n",
      "<|user|>\n",
      "Explain what is Generative AI in 2 bullet points</s> \n",
      "<|assistant|>\n",
      "1. Generative AI is a type of artificial intelligence that can generate new ideas, concepts, and solutions based on data. It is a form of machine learning that uses algorithms to analyze large amounts of data and generate new insights or solutions.\n",
      "\n",
      "2. Generative AI can be used in various industries, including finance, healthcare, marketing, and education. It can help businesses to identify new products or services, improve marketing campaigns, and develop new educational programs.\n",
      "\n",
      "3. Generative AI can also be used to create new forms of art, such as music or visual art. It can generate new melodies or paintings based on user input or data.\n",
      "\n",
      "4. Generative AI is still in its early stages of development, and there are still many challenges to overcome. One of the biggest challenges is the creation of a universal language for generative AI, which would allow machines to understand and generate new ideas in multiple languages.\n",
      "\n",
      "5. Generative AI has the potential to revolutionize many industries, but it also has the potential to create new problems and challenges. For example, if generative AI is used to create new products or services without considering the needs and preferences of end-users, it could lead to negative consequences.\n",
      "\n",
      "6. Overall, generative AI has the potential to transform many industries and create new opportunities for businesses and individuals. However, it also requires careful consideration and regulation to ensure that it is used responsibly and for the benefit of society as a whole.</s>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üî¢ STEP 3A: TOKENIZE THE INPUT (Convert Text ‚Üí Numbers)\n",
    "# ============================================================================\n",
    "# The model can only process numbers, so we need to convert our text prompt\n",
    "# into token IDs using the tokenizer's encode() method\n",
    "#\n",
    "# Parameters:\n",
    "#   - prompt: The formatted text string\n",
    "#   - add_special_tokens: False because chat template already added them\n",
    "#   - return_tensors: \"pt\" = PyTorch tensor format (required for model)\n",
    "\n",
    "inputs = tokenizer.encode(\n",
    "    prompt, \n",
    "    add_special_tokens=False,  # Chat template already includes special tokens\n",
    "    return_tensors=\"pt\"        # Return as PyTorch tensor\n",
    ")\n",
    "\n",
    "print(f\"üìä Input shape: {inputs.shape}\")  # [batch_size, sequence_length]\n",
    "print(f\"üìä Number of input tokens: {inputs.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ü§ñ STEP 3B: GENERATE OUTPUT TOKENS\n",
    "# ============================================================================\n",
    "# model.generate() produces new tokens based on the input\n",
    "#\n",
    "# Key Steps (under the hood):\n",
    "#   1. Process input tokens through the model\n",
    "#   2. Get probability distribution for next token\n",
    "#   3. Select next token (based on sampling/greedy strategy)\n",
    "#   4. Repeat until max_new_tokens or end token reached\n",
    "#\n",
    "# Note: .to(model.device) moves input to same device as model (CPU/GPU)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.to(model.device),  # Move input to model's device\n",
    "    max_new_tokens=1000                  # Generate up to 1000 new tokens\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# üìù STEP 3C: DECODE OUTPUT (Convert Numbers ‚Üí Text)\n",
    "# ============================================================================\n",
    "# tokenizer.decode() converts the generated token IDs back to readable text\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü§ñ MODEL RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d46b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 2: The Easier Way - Using Pipelines\n",
    "\n",
    "The manual process above (encode ‚Üí generate ‚Üí decode) works but is verbose. Hugging Face **Pipelines** simplify this significantly!\n",
    "\n",
    "### What are Pipelines?\n",
    "\n",
    "Pipelines are high-level abstractions that:\n",
    "- ‚úÖ Handle tokenization automatically\n",
    "- ‚úÖ Manage device placement (CPU/GPU)\n",
    "- ‚úÖ Decode outputs for you\n",
    "- ‚úÖ Support batching for efficiency\n",
    "- ‚úÖ Work with chat message format directly\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "| Manual Approach | Pipeline Approach |\n",
    "|-----------------|-------------------|\n",
    "| `tokenizer.encode()` | Just pass your message! |\n",
    "| `model.generate()` | Pipeline handles it |\n",
    "| `tokenizer.decode()` | Returns clean text |\n",
    "| ~10 lines of code | ~3 lines of code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e889207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üõ†Ô∏è CREATING A TEXT GENERATION PIPELINE\n",
    "# ============================================================================\n",
    "# transformers.pipeline() is a factory function that creates an easy-to-use\n",
    "# interface for various NLP tasks\n",
    "#\n",
    "# Common task types:\n",
    "#   - \"text-generation\": Generate text continuations (what we need for chat)\n",
    "#   - \"text-classification\": Sentiment analysis, categorization\n",
    "#   - \"question-answering\": Extract answers from context\n",
    "#   - \"summarization\": Condense long text\n",
    "#   - \"translation\": Translate between languages\n",
    "# ============================================================================\n",
    "\n",
    "llama_pipe = transformers.pipeline(\n",
    "    \"text-generation\",           # Task type: generate text\n",
    "    model=model,                 # Our loaded TinyLlama model\n",
    "    tokenizer=tokenizer,         # Matching tokenizer\n",
    "    torch_dtype=torch.bfloat16,  # Keep memory-efficient dtype\n",
    "    trust_remote_code=True,      # Allow model's custom code (if any)\n",
    "    device_map=\"auto\",           # Automatically choose best device (GPU if available)\n",
    ")\n",
    "\n",
    "# üí° Note: \"auto\" device_map will use:\n",
    "#   - CUDA GPU if available (fastest)\n",
    "#   - Apple MPS if on Mac with M-series chip\n",
    "#   - CPU as fallback (slowest)\n",
    "print(\"‚úÖ Pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba80d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üí¨ PREPARING CHAT MESSAGES FOR THE PIPELINE\n",
    "# ============================================================================\n",
    "# With pipelines, you can pass the chat messages directly!\n",
    "# No need to manually apply chat templates - the pipeline handles it.\n",
    "#\n",
    "# The pipeline accepts the same message format we used before:\n",
    "# A list of dictionaries with \"role\" and \"content\" keys\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\"},\n",
    "]\n",
    "\n",
    "# üí° You can also include conversation history:\n",
    "# chat = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Explain what is Generative AI in 2 bullet points\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2421fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì¶ RAW RESPONSE STRUCTURE:\n",
      "============================================================\n",
      "[{'generated_text': [{'role': 'user', 'content': 'Explain what is Generative AI in 2 bullet points'}, {'role': 'assistant', 'content': '1. Generative AI is a type of artificial intelligence that can generate new ideas, concepts, and solutions based on data. It is a form of machine learning that uses algorithms to analyze large amounts of data and generate new insights or solutions.\\n\\n2. Generative AI can be used in various industries, including finance, healthcare, marketing, and education. It can help businesses to identify new products, services, and marketing strategies, as well as improve customer experience and reduce costs.\\n\\n3. Generative AI can also be used to create new content, such as blog posts, social media posts, and videos. It can generate content based on user data, such as browsing history or search queries, and can create content that is tailored to the specific needs and interests of the user.\\n\\n4. Generative AI can also be used to create new products, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\\n\\n5. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\\n\\n6. Generative AI can also be used to create new business models, such as peer-to-peer lending or crowdfunding platforms. These AI-powered tools can help businesses to reach new audiences and generate revenue.\\n\\n7. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\\n\\n8. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\\n\\n9. Generative AI can also be used to create new business models, such as peer-to-peer lending or crowdfunding platforms. These AI-powered tools can help businesses to reach new audiences and generate revenue.\\n\\n10. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\\n\\n11. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\\n\\n12. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\\n\\n13. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\\n\\n14. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\\n\\n15. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\\n\\n16. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\\n\\n17. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\\n\\n18. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\\n\\n19. Generative AI can also be used to create new products or services, such as virtual assistants or'}]}]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üéØ GENERATING TEXT WITH THE PIPELINE\n",
    "# ============================================================================\n",
    "# Simply call the pipeline like a function!\n",
    "# It handles all the complexity (tokenization, generation, decoding)\n",
    "#\n",
    "# The pipeline accepts the same generation parameters as model.generate()\n",
    "# Common parameters:\n",
    "#   - max_new_tokens: Maximum tokens to generate\n",
    "#   - temperature: Creativity control (0.0-1.0)\n",
    "#   - do_sample: Enable/disable random sampling\n",
    "#   - top_p, top_k: Fine-tune sampling behavior\n",
    "\n",
    "response = llama_pipe(\n",
    "    chat,                    # Our chat messages\n",
    "    max_new_tokens=1000      # Generate up to 1000 new tokens\n",
    ")\n",
    "\n",
    "# Let's examine the raw response structure:\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ RAW RESPONSE STRUCTURE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "701c0f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ ASSISTANT'S RESPONSE (CLEAN):\n",
      "============================================================\n",
      "1. Generative AI is a type of artificial intelligence that can generate new ideas, concepts, and solutions based on data. It is a form of machine learning that uses algorithms to analyze large amounts of data and generate new insights or solutions.\n",
      "\n",
      "2. Generative AI can be used in various industries, including finance, healthcare, marketing, and education. It can help businesses to identify new products, services, and marketing strategies, as well as improve customer experience and reduce costs.\n",
      "\n",
      "3. Generative AI can also be used to create new content, such as blog posts, social media posts, and videos. It can generate content based on user data, such as browsing history or search queries, and can create content that is tailored to the specific needs and interests of the user.\n",
      "\n",
      "4. Generative AI can also be used to create new products, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\n",
      "\n",
      "5. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\n",
      "\n",
      "6. Generative AI can also be used to create new business models, such as peer-to-peer lending or crowdfunding platforms. These AI-powered tools can help businesses to reach new audiences and generate revenue.\n",
      "\n",
      "7. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\n",
      "\n",
      "8. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\n",
      "\n",
      "9. Generative AI can also be used to create new business models, such as peer-to-peer lending or crowdfunding platforms. These AI-powered tools can help businesses to reach new audiences and generate revenue.\n",
      "\n",
      "10. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\n",
      "\n",
      "11. Generative AI can also be used to create new products or services, such as virtual reality experiences or augmented reality apps. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\n",
      "\n",
      "12. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\n",
      "\n",
      "13. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\n",
      "\n",
      "14. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\n",
      "\n",
      "15. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\n",
      "\n",
      "16. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to improve customer service, reduce costs, and increase efficiency.\n",
      "\n",
      "17. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to create immersive and engaging experiences for their customers.\n",
      "\n",
      "18. Generative AI can also be used to create new products or services, such as virtual assistants or chatbots. These AI-powered tools can help businesses to reach new audiences and generate revenue.\n",
      "\n",
      "19. Generative AI can also be used to create new products or services, such as virtual assistants or\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì§ EXTRACTING THE ASSISTANT'S RESPONSE\n",
    "# ============================================================================\n",
    "# The pipeline returns a nested structure:\n",
    "#   response[0][\"generated_text\"] = list of all messages (input + generated)\n",
    "#   [-1] gets the last message (the assistant's response)\n",
    "#   ['content'] extracts just the text content\n",
    "#\n",
    "# Structure breakdown:\n",
    "#   response = [\n",
    "#       {\n",
    "#           \"generated_text\": [\n",
    "#               {\"role\": \"user\", \"content\": \"...\"},        # Original input\n",
    "#               {\"role\": \"assistant\", \"content\": \"...\"}   # Generated response ‚Üê We want this!\n",
    "#           ]\n",
    "#       }\n",
    "#   ]\n",
    "\n",
    "# Extract just the assistant's message content:\n",
    "assistant_response = response[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ ASSISTANT'S RESPONSE (CLEAN):\")\n",
    "print(\"=\" * 60)\n",
    "print(assistant_response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3dedb",
   "metadata": {},
   "source": [
    "## üìù Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Hugging Face Transformers** provides easy access to thousands of pre-trained models\n",
    "2. **Loading models** requires two components: **Tokenizer** + **Model**\n",
    "3. **Chat templates** format messages correctly for each model family\n",
    "4. **Manual workflow**: encode ‚Üí generate ‚Üí decode\n",
    "5. **Pipelines** simplify everything into a single function call\n",
    "\n",
    "### Key Code Patterns:\n",
    "\n",
    "```python\n",
    "# Loading a model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model_id\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model_id\")\n",
    "\n",
    "# Using pipelines (recommended for simplicity)\n",
    "pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "response = pipe([{\"role\": \"user\", \"content\": \"Your question\"}])\n",
    "```\n",
    "\n",
    "### üéØ When to Use Each Approach:\n",
    "\n",
    "| Approach | Use Case |\n",
    "|----------|----------|\n",
    "| **Manual** (encode/generate/decode) | Fine-grained control, custom generation logic |\n",
    "| **Pipeline** | Quick prototyping, standard use cases |\n",
    "| **API/Cloud** (Groq, HF Inference) | No local GPU, production deployments |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "- Explore **notebook 4**: Using Hugging Face Inference Client (API-based)\n",
    "- Explore **notebook 5**: Using Groq Cloud for faster inference\n",
    "- Explore **notebook 6**: Integrating with LangChain\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Hugging Face Hub](https://huggingface.co/models) - Browse models\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers) - Official docs\n",
    "- [Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies) - Deep dive into text generation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
