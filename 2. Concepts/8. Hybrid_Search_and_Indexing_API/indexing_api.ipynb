{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Indexing API - A Complete Guide\n",
    "\n",
    "### What is the Indexing API?\n",
    "\n",
    "The **LangChain Indexing API** helps you efficiently sync your documents into a vector store. It solves critical problems when working with document embeddings:\n",
    "\n",
    "1. **Avoids duplicate content** - Prevents writing the same document multiple times\n",
    "2. **Saves embedding costs** - Skips re-computing embeddings for unchanged content\n",
    "3. **Handles updates intelligently** - Only re-embeds content that has actually changed\n",
    "4. **Manages deletions** - Removes stale documents that no longer exist in your source\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `RecordManager` | Tracks which documents have been indexed (stores hashes & timestamps) |\n",
    "| `VectorStore` | Stores the actual document embeddings |\n",
    "| `index()` function | Orchestrates the sync process between source documents and vector store |\n",
    "\n",
    "### Cleanup Modes\n",
    "\n",
    "The `cleanup` parameter controls how the indexer handles document changes:\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `None` | Only adds new documents, never deletes anything |\n",
    "| `\"incremental\"` | Deletes outdated versions when source documents are re-indexed |\n",
    "| `\"full\"` | Deletes ALL documents not in the current batch (complete sync) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup\n",
    "\n",
    "Load environment variables (like `OPENAI_API_KEY`) from a `.env` file. This keeps sensitive credentials out of your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Vector Store and Embeddings\n",
    "\n",
    "We'll use:\n",
    "- **OpenAIEmbeddings**: Converts text into numerical vectors using OpenAI's embedding model\n",
    "- **PGVector**: A PostgreSQL-based vector store that supports similarity search\n",
    "\n",
    "> **Note**: Make sure PostgreSQL with pgvector extension is running. You can use the provided `docker-compose.yaml` to spin up a local instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required components\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# Initialize the embedding model (uses OpenAI's text-embedding-ada-002 by default)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# PostgreSQL connection string format: postgresql+psycopg://user:password@host:port/database\n",
    "CONNECTION_STRING = \"postgresql+psycopg://admin:admin@127.0.0.1:5432/vectordb\"\n",
    "\n",
    "# Collection name acts like a \"table\" within the vector database\n",
    "COLLECTION_NAME = \"vectordb\"\n",
    "\n",
    "# Initialize PGVector - this creates the necessary tables if they don't exist\n",
    "vectorstore = PGVector(\n",
    "    collection_name=COLLECTION_NAME,  # Logical grouping of vectors\n",
    "    connection=CONNECTION_STRING,      # Database connection\n",
    "    embeddings=embeddings,             # Embedding model to use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and Split Documents\n",
    "\n",
    "Before indexing, we need to:\n",
    "1. **Load** the source document (a text file about \"Bella Vista\" restaurant)\n",
    "2. **Split** it into smaller chunks for better retrieval\n",
    "\n",
    "The `CharacterTextSplitter` breaks text at natural boundaries while respecting the chunk size limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 177, which is longer than the specified 150\n",
      "Created a chunk of size 229, which is longer than the specified 150\n",
      "Created a chunk of size 233, which is longer than the specified 150\n",
      "Created a chunk of size 206, which is longer than the specified 150\n",
      "Created a chunk of size 203, which is longer than the specified 150\n",
      "Created a chunk of size 299, which is longer than the specified 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 7 chunks\n"
     ]
    }
   ],
   "source": [
    "# Load the source document\n",
    "loader = TextLoader(\"./bella_vista.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks:\n",
    "# - chunk_size=150: Target size for each chunk (in characters)\n",
    "# - chunk_overlap=20: Characters shared between adjacent chunks (helps maintain context)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# See how many chunks were created\n",
    "print(f\"Document split into {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Q: What are the hours of operation for Bella Vista?\n",
      "A: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: What type of cuisine does Bella Vista serve?\n",
      "A: Bella Vista offers a delightful blend of Mediterranean and contemporary American cuisine. We pride ourselves on using the freshest ingredients, many of which are sourced locally.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Do you offer vegetarian or vegan options at Bella Vista?\n",
      "A: Absolutely! Bella Vista boasts a diverse menu that includes a variety of vegetarian and vegan dishes. Our chefs are also happy to customize dishes based on dietary needs.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Is Bella Vista family-friendly? sdoasdokasdoaskodosa\n",
      "A: Yes, Bella Vista is a family-friendly establishment. We have a dedicated kids' menu and offer high chairs and booster seats for our younger guests.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Can I book private events at Bella Vista?\n",
      "A: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: What's the ambiance like at Bella Vista?\n",
      "A: Bella Vista boasts a cozy and elegant setting, with ambient lighting, comfortable seating, and a stunning view of the city skyline. Whether you're looking for a romantic dinner or a casual meal with friends, Bella Vista provides the perfect atmosphere.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Do I need a reservation for Bella Vista?\n",
      "A: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.' metadata={'source': './bella_vista.txt'}\n"
     ]
    }
   ],
   "source": [
    "for elem in docs:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up the Record Manager\n",
    "\n",
    "The **Record Manager** is the brain of the Indexing API. It:\n",
    "- Maintains a registry of all indexed documents\n",
    "- Stores content hashes to detect changes\n",
    "- Tracks timestamps to determine document freshness\n",
    "\n",
    "We use `SQLRecordManager` which stores this metadata in a SQL database (same PostgreSQL instance in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Indexing API components\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "# SQLRecordManager: Tracks document state in a SQL database\n",
    "# index: The main function that syncs documents to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace uniquely identifies this index\n",
    "# Format: \"vectorstore_type/collection_name\" - helps manage multiple indices\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "\n",
    "# Create the record manager\n",
    "# It will create a table to track document hashes, sources, and timestamps\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,                    # Unique identifier for this index\n",
    "    db_url=CONNECTION_STRING      # Where to store the tracking metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Create the database schema (tables) for the record manager\n",
    "# This only needs to be done once - it's safe to call multiple times\n",
    "# Creates a table to store: document_id, hash, source, updated_at\n",
    "record_manager.create_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Initial Indexing with `cleanup=None`\n",
    "\n",
    "Now let's index our documents! We'll start with `cleanup=None` mode.\n",
    "\n",
    "**What happens with `cleanup=None`:**\n",
    "- New documents are added to the vector store\n",
    "- Existing documents (same content hash) are skipped\n",
    "- **Nothing is ever deleted** - old/stale documents remain\n",
    "\n",
    "This mode is useful when you only want to add new content without affecting existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 177, which is longer than the specified 150\n",
      "Created a chunk of size 229, which is longer than the specified 150\n",
      "Created a chunk of size 233, which is longer than the specified 150\n",
      "Created a chunk of size 206, which is longer than the specified 150\n",
      "Created a chunk of size 203, which is longer than the specified 150\n",
      "Created a chunk of size 299, which is longer than the specified 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Q: What are the hours of operation for Bella Vista?\n",
      "A: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: What type of cuisine does Bella Vista serve?\n",
      "A: Bella Vista offers a delightful blend of Mediterranean and contemporary American cuisine. We pride ourselves on using the freshest ingredients, many of which are sourced locally.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Do you offer vegetarian or vegan options at Bella Vista?\n",
      "A: Absolutely! Bella Vista boasts a diverse menu that includes a variety of vegetarian and vegan dishes. Our chefs are also happy to customize dishes based on dietary needs.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Is Bella Vista family-friendly? sdoasdokasdoaskodosa\n",
      "A: Yes, Bella Vista is a family-friendly establishment. We have a dedicated kids' menu and offer high chairs and booster seats for our younger guests.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Can I book private events at Bella Vista?\n",
      "A: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: What's the ambiance like at Bella Vista?\n",
      "A: Bella Vista boasts a cozy and elegant setting, with ambient lighting, comfortable seating, and a stunning view of the city skyline. Whether you're looking for a romantic dinner or a casual meal with friends, Bella Vista provides the perfect atmosphere.' metadata={'source': './bella_vista.txt'}\n",
      "page_content='Q: Do I need a reservation for Bella Vista?\n",
      "A: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.' metadata={'source': './bella_vista.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Reload and re-split the document to see what we're working with\n",
    "loader = TextLoader(\"./bella_vista.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preview each chunk - notice the 'source' in metadata\n",
    "# The source is critical for incremental cleanup mode!\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Index Run** - Adding 7 new documents with `cleanup=None`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'num_added': 7, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# Index the documents with cleanup=None\n",
    "result = index(\n",
    "    docs,                    # Documents to index\n",
    "    record_manager,          # Tracks what's been indexed\n",
    "    vectorstore,             # Where to store embeddings\n",
    "    cleanup=None,            # No deletion - only add new docs\n",
    "    source_id_key=\"source\",  # Metadata key that identifies the document source\n",
    ")\n",
    "\n",
    "# Result shows what happened:\n",
    "# - num_added: New documents embedded and stored\n",
    "# - num_updated: Documents with changed content (re-embedded)\n",
    "# - num_skipped: Unchanged documents (already indexed)\n",
    "# - num_deleted: Removed documents (only with cleanup modes)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulating Document Changes\n",
    "\n",
    "Let's simulate real-world scenarios where documents change:\n",
    "1. **Modify** an existing document (update content)\n",
    "2. **Delete** a document from the source\n",
    "3. **Add** a new document\n",
    "\n",
    "This demonstrates how the Indexing API handles each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 7 documents (1 deleted, 1 added = net 0 change)\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Simulate document changes:\n",
    "\n",
    "# 1. UPDATE: Modify the content of an existing document\n",
    "docs[1].page_content = \"updated\"\n",
    "\n",
    "# 2. DELETE: Remove a document from our source (index 6)\n",
    "del docs[6]\n",
    "\n",
    "# 3. ADD: Create a brand new document with a different source\n",
    "docs.append(Document(\n",
    "    page_content=\"new content\", \n",
    "    metadata={\"source\": \"important\"}  # Note: different source!\n",
    "))\n",
    "\n",
    "print(f\"Now we have {len(docs)} documents (1 deleted, 1 added = net 0 change)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Index Run** - Re-indexing after changes with `cleanup=None`\n",
    "\n",
    "Notice: \n",
    "- 2 added (1 updated doc + 1 new doc - both have new content hashes)\n",
    "- 5 skipped (unchanged documents)\n",
    "- 0 deleted (cleanup=None never deletes!)\n",
    "\n",
    "**Problem**: The old version of doc[1] and the deleted doc[6] are still in the vector store!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 2, 'num_updated': 0, 'num_skipped': 5, 'num_deleted': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-index with cleanup=None - observe what happens\n",
    "index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=None,            # Still no deletions\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "# Expected: num_added=2, num_skipped=5, num_deleted=0\n",
    "# The old/deleted docs are STILL in the vector store (duplicates possible!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Using `cleanup=\"incremental\"` Mode\n",
    "\n",
    "Now let's try **incremental cleanup** - this is the recommended mode for most use cases!\n",
    "\n",
    "**How incremental cleanup works:**\n",
    "- When a document with a given `source` is re-indexed, all OLD versions from that source are deleted\n",
    "- Only affects documents from sources that appear in the current batch\n",
    "- Documents from sources NOT in the batch are left untouched\n",
    "\n",
    "This is perfect for syncing specific source files without affecting others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After changes: 5 documents remain\n"
     ]
    }
   ],
   "source": [
    "# Make more changes to simulate ongoing document updates:\n",
    "\n",
    "# Update doc[1] again with new content\n",
    "docs[1].page_content = \"updated again\"\n",
    "\n",
    "# Delete several more documents from the source\n",
    "del docs[2]  # Remove another doc\n",
    "del docs[3]  # And another\n",
    "del docs[4]  # And one more\n",
    "\n",
    "# Add another new document (same source as the previous new doc)\n",
    "docs.append(Document(\n",
    "    page_content=\"more new content\", \n",
    "    metadata={\"source\": \"important\"}  # Same source as before\n",
    "))\n",
    "\n",
    "print(f\"After changes: {len(docs)} documents remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third Index Run** - Using `cleanup=\"incremental\"`\n",
    "\n",
    "Now we'll see deletions! The indexer will:\n",
    "1. Add the 2 new documents (updated content + new doc)\n",
    "2. Delete 6 old documents that are no longer in the source\n",
    "\n",
    "Notice: Only documents from sources present in this batch are cleaned up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'num_added': 2, 'num_updated': 0, 'num_skipped': 3, 'num_deleted': 6}\n"
     ]
    }
   ],
   "source": [
    "# Index with incremental cleanup - THIS IS THE RECOMMENDED MODE\n",
    "result = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",   # Delete old versions from same sources\n",
    "    source_id_key=\"source\",  # Groups documents by source for cleanup\n",
    ")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "# num_added: New/updated documents\n",
    "# num_deleted: Old versions removed (from sources in this batch only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Incremental Cleanup with Empty Batch\n",
    "\n",
    "What happens if we pass an **empty list** with `cleanup=\"incremental\"`?\n",
    "\n",
    "**Answer**: Nothing gets deleted! Because no sources are in the batch, no sources get cleaned up.\n",
    "\n",
    "This is a KEY DIFFERENCE from `cleanup=\"full\"` mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# Pass an empty list with incremental cleanup\n",
    "result = index(\n",
    "    [],                      # Empty document list!\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",   # Incremental mode\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "# All zeros! No sources in the batch = no cleanup happens\n",
    "# Existing documents are safe because no source was \"re-indexed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Using `cleanup=\"full\"` Mode\n",
    "\n",
    "**Full cleanup** is the nuclear option - use with caution!\n",
    "\n",
    "**How it works:**\n",
    "- Deletes ALL documents in the vector store that are NOT in the current batch\n",
    "- Regardless of source - if it's not in the batch, it's deleted\n",
    "\n",
    "**When to use:**\n",
    "- Complete re-sync from scratch\n",
    "- You have ALL your documents in the batch\n",
    "- You want to ensure the vector store exactly matches your source\n",
    "\n",
    "**Warning**: Passing an empty list with `cleanup=\"full\"` will DELETE EVERYTHING!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 5}\n"
     ]
    }
   ],
   "source": [
    "# DANGER: Full cleanup with empty list = DELETE EVERYTHING\n",
    "result = index(\n",
    "    [],                      # Empty list\n",
    "    record_manager, \n",
    "    vectorstore, \n",
    "    cleanup=\"full\",          # Full sync mode\n",
    "    source_id_key=\"source\"\n",
    ")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "# num_deleted will show ALL remaining documents were removed!\n",
    "# The vector store is now empty - use with extreme caution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Choosing the Right Cleanup Mode\n",
    "\n",
    "| Cleanup Mode | Best For | Deletes? | Safe with Empty Batch? |\n",
    "|--------------|----------|----------|------------------------|\n",
    "| `None` | Append-only workloads | Never | ✅ Yes |\n",
    "| `\"incremental\"` | Regular updates (recommended) | Only from sources in batch | ✅ Yes |\n",
    "| `\"full\"` | Complete re-sync | Everything not in batch | ❌ No (deletes all!) |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use `source_id_key`** - It's required for `incremental` cleanup and helps track document provenance\n",
    "2. **Start with `cleanup=\"incremental\"`** - It's the safest choice for most production use cases\n",
    "3. **Use `cleanup=\"full\"` carefully** - Only when you have ALL documents and want a complete sync\n",
    "4. **The Record Manager is essential** - It stores hashes to detect content changes efficiently\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "- Run the indexer on a schedule (cron job) to keep vectors in sync\n",
    "- Use meaningful source identifiers (file paths, URLs, database IDs)\n",
    "- Monitor the return values to track indexing health\n",
    "- Consider using separate namespaces for different document collections\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
