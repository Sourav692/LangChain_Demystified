{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbf7232",
   "metadata": {},
   "source": [
    "# ðŸ¤– Using Commercial LLMs Natively (Without Frameworks)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll learn how to interact with popular **Commercial Large Language Model (LLM) APIs** directly using their official Python SDKs â€” **without any abstraction layers like LangChain**.\n",
    "\n",
    "### Why Learn Native API Access?\n",
    "\n",
    "Understanding native API access is crucial because:\n",
    "1. **Foundation Knowledge**: It helps you understand what happens \"under the hood\" when using higher-level frameworks\n",
    "2. **Debugging**: When things go wrong, knowing the native API helps you troubleshoot effectively\n",
    "3. **Flexibility**: Some advanced features may only be available through native APIs\n",
    "4. **Performance**: Direct API calls can sometimes be more efficient for simple use cases\n",
    "\n",
    "### LLMs Covered in This Notebook\n",
    "\n",
    "| Provider | Model | Pricing |\n",
    "|----------|-------|---------|\n",
    "| OpenAI | GPT-4o, GPT-4o-mini, GPT-3.5-Turbo | Paid (Pay-per-token) |\n",
    "| Google | Gemini 2.5 Flash, Gemini 2.5 Pro | Free tier available |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- API keys from OpenAI and/or Google\n",
    "- Understanding of what LLMs are and their basic capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "969aaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REQUIRED PACKAGES INSTALLATION\n",
    "# ============================================================================\n",
    "# Uncomment the lines below to install the required packages\n",
    "# \n",
    "# openai: Official OpenAI Python client library for GPT models\n",
    "# google-generativeai: Google's Python SDK for Gemini models\n",
    "#\n",
    "# The -qq flag suppresses output for cleaner installation\n",
    "# ============================================================================\n",
    "\n",
    "# !pip install -qq openai==1.57.0\n",
    "# !pip install -qq google-generativeai==0.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945c667",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ”‘ Part 1: OpenAI GPT Setup\n",
    "\n",
    "## Step 1: Get Your OpenAI API Key\n",
    "\n",
    "> âš ï¸ **Important**: OpenAI API is a **paid service**. You'll need to set up billing before you can use it.\n",
    "\n",
    "### What is an API Key?\n",
    "\n",
    "An API key is like a password that:\n",
    "- Identifies you to the OpenAI service\n",
    "- Allows OpenAI to track your usage and bill you accordingly\n",
    "- Should be kept **secret** and never shared publicly\n",
    "\n",
    "### How to Get Your API Key\n",
    "\n",
    "**Step 1.1**: Create an OpenAI account at [platform.openai.com](https://platform.openai.com)\n",
    "\n",
    "**Step 1.2**: Set up billing\n",
    "- Go to [Settings â†’ Billing](https://platform.openai.com/settings/organization/billing/overview)\n",
    "- Add a payment method and top up with at least $5 (sufficient for thousands of API calls)\n",
    "\n",
    "![Billing Setup](https://i.imgur.com/pXgs31r.png)\n",
    "\n",
    "**Step 1.3**: Generate your API key\n",
    "- Navigate to [Dashboard â†’ API Keys](https://platform.openai.com/api-keys)\n",
    "- Click \"Create new secret key\"\n",
    "\n",
    "![Create API Key](https://i.imgur.com/YbIBBtc.png)\n",
    "\n",
    "**Step 1.4**: Save your key securely\n",
    "- âš ï¸ The key is shown **only once** â€” copy it immediately!\n",
    "- Store it in a secure location (we'll use a `.env` file)\n",
    "\n",
    "![Save API Key](https://i.imgur.com/myFXgZg.png)\n",
    "\n",
    "> ðŸ’¡ **Best Practice**: Never hardcode API keys in your code. Use environment variables or `.env` files instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d29f8",
   "metadata": {},
   "source": [
    "## Step 2: Load OpenAI API Credentials\n",
    "\n",
    "There are two common ways to load API credentials:\n",
    "\n",
    "1. **Interactive Input** (using `getpass`) - Good for notebooks, prompts user at runtime\n",
    "2. **Environment Variables** (using `.env` files) - Better for production, more secure\n",
    "\n",
    "We'll demonstrate both approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4589b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD 1: Interactive Input (Alternative approach)\n",
    "# ============================================================================\n",
    "# getpass() prompts for input without showing what you type (like a password)\n",
    "# This is useful when you don't want to store keys in files\n",
    "# Uncomment below to use this method\n",
    "# ============================================================================\n",
    "\n",
    "# from getpass import getpass\n",
    "# openai_key = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e53debea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# METHOD 2: Environment Variables (Recommended for production)\n",
    "# ============================================================================\n",
    "# python-dotenv loads variables from a .env file into environment variables\n",
    "# \n",
    "# Your .env file should look like this:\n",
    "# OPENAI_API_KEY=sk-your-api-key-here\n",
    "# GOOGLE_GENAI_API_KEY=your-gemini-key-here\n",
    "#\n",
    "# Note: Add .env to your .gitignore to prevent accidental commits!\n",
    "# ============================================================================\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load_dotenv() searches for .env file in current directory and parent directories\n",
    "# Returns True if file found and loaded, False otherwise\n",
    "load_dotenv()\n",
    "\n",
    "# os.getenv() retrieves the value of an environment variable\n",
    "# Returns None if the variable doesn't exist\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Optional: Verify the key was loaded (without printing the actual key!)\n",
    "if openai_key:\n",
    "    print(\"âœ… OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"âŒ Warning: OpenAI API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d668b5",
   "metadata": {},
   "source": [
    "## Step 3: Using ChatGPT Directly via API\n",
    "\n",
    "Now let's use the OpenAI API directly. This approach gives you:\n",
    "- **Full control** over API parameters\n",
    "- **Direct access** to all model features\n",
    "- **Better understanding** of how chat completions work\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Chat Completion** | The API endpoint for conversational AI (most common) |\n",
    "| **Messages** | A list of message objects with roles (system, user, assistant) |\n",
    "| **System Message** | Sets the behavior/personality of the AI |\n",
    "| **User Message** | The actual prompt/question from the user |\n",
    "| **Temperature** | Controls randomness (0 = deterministic, 2 = very random) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a11a6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT THE OPENAI LIBRARY\n",
    "# ============================================================================\n",
    "# The openai library provides a simple interface to interact with OpenAI's APIs\n",
    "# It handles HTTP requests, authentication, and response parsing for you\n",
    "# ============================================================================\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a86e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE OPENAI WITH YOUR API KEY\n",
    "# ============================================================================\n",
    "# This sets the API key globally for all subsequent OpenAI API calls\n",
    "# The library will automatically include this key in request headers\n",
    "# ============================================================================\n",
    "\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a889715",
   "metadata": {},
   "source": [
    "### Understanding API Pricing\n",
    "\n",
    "OpenAI charges based on **tokens** â€” pieces of words (roughly 4 characters = 1 token).\n",
    "\n",
    "#### What are Tokens?\n",
    "- \"Hello, world!\" â‰ˆ 4 tokens\n",
    "- 1000 tokens â‰ˆ 750 words\n",
    "- You pay for both **input** (your prompt) and **output** (AI response) tokens\n",
    "\n",
    "#### Model Recommendations\n",
    "\n",
    "| Use Case | Recommended Model | Why |\n",
    "|----------|------------------|-----|\n",
    "| Cost-effective | GPT-4o-mini | Best price-to-performance ratio |\n",
    "| High performance | GPT-4o | Most capable, best for complex tasks |\n",
    "| Legacy/Budget | GPT-3.5-Turbo | Stable, cheaper, good for simple tasks |\n",
    "\n",
    "ðŸ“Š [View current pricing here](https://openai.com/api/pricing/)\n",
    "\n",
    "![OpenAI Pricing](https://i.imgur.com/U0C1Xhx.png)\n",
    "\n",
    "> ðŸ’¡ **Tip**: Start with GPT-4o-mini for learning. Upgrade to GPT-4o only when you need superior reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87841e93",
   "metadata": {},
   "source": [
    "### Your First API Call: Chat Completion\n",
    "\n",
    "Let's make our first API call! We'll use the **Chat Completions** endpoint, which is designed for conversational interactions.\n",
    "\n",
    "#### Understanding the Message Structure\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"...\"},  # Sets AI behavior (optional but recommended)\n",
    "    {\"role\": \"user\", \"content\": \"...\"},    # Your question/prompt\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"} # AI's previous response (for multi-turn)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1fb1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Los Angeles Dodgers won the World Series in 2020. They defeated the Tampa Bay Rays, clinching the championship in six games. This victory marked the Dodgers' first World Series title since 1988.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAKING A CHAT COMPLETION REQUEST\n",
    "# ============================================================================\n",
    "\n",
    "# Step 1: Define the conversation as a list of messages\n",
    "# Each message has a \"role\" and \"content\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",           # System messages set the AI's behavior/personality\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",             # User messages are your prompts/questions\n",
    "        \"content\": \"Who won the world series in 2020?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 2: Make the API call\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",    # The model to use (affects cost and capability)\n",
    "    messages=messages,       # The conversation history\n",
    "    temperature=0            # 0 = deterministic (same input â†’ same output)\n",
    "                            # Higher values (0.7-1.0) = more creative/random\n",
    ")\n",
    "\n",
    "# Step 3: Extract the response\n",
    "# The response object contains metadata + the actual reply\n",
    "# - response.choices: List of possible completions (usually just 1)\n",
    "# - response.choices[0].message.content: The actual text response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e662e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATING A REUSABLE HELPER FUNCTION\n",
    "# ============================================================================\n",
    "# It's good practice to wrap API calls in functions for:\n",
    "# 1. Reusability - call the same logic with different prompts\n",
    "# 2. Abstraction - hide complexity from the rest of your code\n",
    "# 3. Error handling - add try/except in one place (shown below)\n",
    "# 4. Customization - easily switch models or parameters\n",
    "# ============================================================================\n",
    "\n",
    "def get_completion_chatgpt(prompt, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Get a completion from OpenAI's ChatGPT model.\n",
    "    \n",
    "    This is a simplified wrapper that handles the boilerplate of making\n",
    "    API calls, allowing you to focus on crafting good prompts.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the model.\n",
    "        model (str): The model to use. Options include:\n",
    "                    - \"gpt-4o-mini\" (default, cost-effective)\n",
    "                    - \"gpt-4o\" (most capable)\n",
    "                    - \"gpt-3.5-turbo\" (legacy, cheapest)\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response message.\n",
    "        \n",
    "    Example:\n",
    "        >>> response = get_completion_chatgpt(\"What is 2+2?\")\n",
    "        >>> print(response)\n",
    "        \"2 + 2 equals 4.\"\n",
    "    \"\"\"\n",
    "    # Construct the messages array with a default system message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Make the API call with temperature=0 for consistent outputs\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0  # Deterministic output - good for testing and demos\n",
    "    )\n",
    "    \n",
    "    # Return just the text content (not the full response object)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62816781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– GPT-4o-mini Response:\n",
      "--------------------------------------------------\n",
      "- **Definition and Functionality**: Generative AI refers to a class of artificial intelligence models that can create new content, such as text, images, music, or videos, by learning patterns and structures from existing data. These models, like GPT-3 for text or DALL-E for images, use deep learning techniques to generate outputs that resemble human-created content.\n",
      "\n",
      "- **Applications and Impact**: Generative AI has a wide range of applications, including content creation, design, gaming, and personalized experiences. It can enhance creativity, automate tasks, and provide innovative solutions across various industries, but it also raises ethical concerns regarding copyright, misinformation, and the potential for misuse.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 1: Using GPT-4o-mini (Cost-effective)\n",
    "# ============================================================================\n",
    "# GPT-4o-mini offers excellent performance at a fraction of the cost\n",
    "# Perfect for most use cases including learning, prototyping, and production\n",
    "# ============================================================================\n",
    "\n",
    "prompt = 'Explain Generative AI in 2 bullet points'\n",
    "\n",
    "# Call our helper function with the default model (gpt-4o-mini)\n",
    "response = get_completion_chatgpt(prompt=prompt, model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"ðŸ¤– GPT-4o-mini Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff64c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– GPT-3.5-Turbo Response:\n",
      "--------------------------------------------------\n",
      "- Generative AI refers to a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns and data it has been trained on.\n",
      "- It works by learning the underlying structure of the data it is trained on and then generating new content that is similar to the input data but not an exact copy, allowing for the creation of original and diverse outputs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 2: Using GPT-3.5-Turbo (Legacy/Budget option)\n",
    "# ============================================================================\n",
    "# GPT-3.5-Turbo is an older model but still capable and cheaper\n",
    "# Notice how the response quality may differ from GPT-4o-mini\n",
    "# This demonstrates how model choice affects output quality\n",
    "# ============================================================================\n",
    "\n",
    "prompt = 'Explain Generative AI in 2 bullet points'\n",
    "\n",
    "# Same prompt, different model - compare the outputs!\n",
    "response = get_completion_chatgpt(prompt=prompt, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(\"ðŸ¤– GPT-3.5-Turbo Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)\n",
    "\n",
    "# ðŸ’¡ Exercise: Compare the responses from both models. \n",
    "# Which one is more detailed? More accurate? Better structured?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1666e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ”‘ Part 2: Google Gemini Setup\n",
    "\n",
    "## Step 1: Get Your Google Gemini API Key\n",
    "\n",
    "> âœ… **Good News**: Google Gemini offers a **free tier** â€” no billing required for basic usage!\n",
    "\n",
    "### Why Use Gemini?\n",
    "\n",
    "- **Free tier available** â€” great for learning and experimentation\n",
    "- **Multimodal capabilities** â€” can process text, images, audio, and video\n",
    "- **Competitive performance** â€” Gemini 2.5 models rival GPT-4o in many benchmarks\n",
    "- **Integration with Google ecosystem** â€” works well with other Google services\n",
    "\n",
    "### How to Get Your API Key\n",
    "\n",
    "**Step 1.1**: Go to [Google AI Studio](https://aistudio.google.com/app/u/0/apikey)\n",
    "- Sign in with your Google account (Gmail works fine)\n",
    "\n",
    "**Step 1.2**: Create your API key\n",
    "- Click \"Create API Key\"\n",
    "- Select or create a Google Cloud project\n",
    "\n",
    "![Create Gemini API Key](https://i.imgur.com/UYVkKmK.png)\n",
    "\n",
    "**Step 1.3**: Save your key securely\n",
    "- Copy the key and store it in your `.env` file\n",
    "\n",
    "![Save Gemini Key](https://i.imgur.com/9JZyw2t.png)\n",
    "\n",
    "> ðŸ’¡ **Note**: Check [Google's pricing page](https://ai.google.dev/pricing) for current free tier limits and paid options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6904e",
   "metadata": {},
   "source": [
    "## Step 2: Load Gemini API Credentials\n",
    "\n",
    "Similar to OpenAI, we'll load the Gemini API key from our `.env` file.\n",
    "\n",
    "> ðŸ“ **Your `.env` file should now contain:**\n",
    "> ```\n",
    "> OPENAI_API_KEY=sk-your-openai-key-here\n",
    "> GOOGLE_GENAI_API_KEY=your-gemini-key-here\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee988186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD GEMINI API KEY FROM ENVIRONMENT\n",
    "# ============================================================================\n",
    "# Same approach as OpenAI - load from .env file for security\n",
    "# ============================================================================\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Reload .env file (in case it was updated after the OpenAI section)\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Gemini API key from environment variables\n",
    "gemini_key = os.getenv(\"GOOGLE_GENAI_API_KEY\")\n",
    "\n",
    "# Verify the key was loaded\n",
    "if gemini_key:\n",
    "    print(\"âœ… Gemini API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"âŒ Warning: Gemini API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525f1d9",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Import the Gemini SDK\n",
    "\n",
    "Now let's import Google's Generative AI library and configure it with our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "856171ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT GOOGLE'S GENERATIVE AI LIBRARY\n",
    "# ============================================================================\n",
    "# google-generativeai is the official Python SDK for Google's Gemini models\n",
    "# We import it as 'genai' by convention (shorter to type)\n",
    "# \n",
    "# Note: You may see a TqdmWarning - this is just about progress bar display\n",
    "# and doesn't affect functionality\n",
    "# ============================================================================\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e40289f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini SDK configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE THE GEMINI SDK WITH YOUR API KEY\n",
    "# ============================================================================\n",
    "# Unlike OpenAI, Gemini uses a configure() function to set the API key\n",
    "# This sets the key globally for all subsequent Gemini API calls\n",
    "# ============================================================================\n",
    "\n",
    "genai.configure(api_key=gemini_key)\n",
    "\n",
    "print(\"âœ… Gemini SDK configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3383f7",
   "metadata": {},
   "source": [
    "## Understanding Gemini Pricing & Models\n",
    "\n",
    "Google offers generous free tiers for Gemini, making it excellent for learning!\n",
    "\n",
    "### Gemini Model Recommendations\n",
    "\n",
    "| Model | Best For | Free Tier |\n",
    "|-------|----------|-----------|\n",
    "| Gemini 2.5 Flash | Fast responses, cost-effective | âœ… Yes |\n",
    "| Gemini 2.5 Pro | Complex reasoning, high quality | âœ… Limited |\n",
    "| Gemini 2.0 Flash | Balanced speed/quality | âœ… Yes |\n",
    "\n",
    "### Key Differences from OpenAI\n",
    "\n",
    "| Feature | OpenAI | Gemini |\n",
    "|---------|--------|--------|\n",
    "| Free Tier | âŒ No | âœ… Yes |\n",
    "| Context Window | Up to 128K tokens | Up to 1M tokens |\n",
    "| Multimodal | Yes (text, images) | Yes (text, images, audio, video) |\n",
    "| Native SDK | `openai` | `google-generativeai` |\n",
    "\n",
    "ðŸ“Š [View current Gemini pricing](https://ai.google.dev/pricing)\n",
    "\n",
    "![Gemini Pricing](https://i.imgur.com/8hR2Ti8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52a51968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Available Gemini Models with Content Generation:\n",
      "======================================================================\n",
      "models/gemini-2.5-flash -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp -----> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation -----> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts -----> ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts -----> ['countTokens', 'generateContent']\n",
      "models/gemma-3-1b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it -----> ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image-preview -----> ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image -----> ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-09-2025 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-preview -----> ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-image-preview -----> ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/nano-banana-pro-preview -----> ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview -----> ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-computer-use-preview-10-2025 -----> ['generateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIST AVAILABLE GEMINI MODELS\n",
    "# ============================================================================\n",
    "# One advantage of the Gemini SDK is that you can programmatically list\n",
    "# all available models and their capabilities\n",
    "#\n",
    "# Common supported methods:\n",
    "# - 'generateContent' - Can generate text responses (what we need)\n",
    "# - 'countTokens' - Can count tokens in a prompt\n",
    "# - 'createCachedContent' - Supports content caching for efficiency\n",
    "# - 'batchGenerateContent' - Can process multiple prompts at once\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ“‹ Available Gemini Models with Content Generation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model in genai.list_models():\n",
    "    # Filter to only show models that can generate content\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"{model.name} -----> {model.supported_generation_methods}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e0220b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Gemini Response:\n",
      "--------------------------------------------------\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# YOUR FIRST GEMINI API CALL\n",
    "# ============================================================================\n",
    "# The Gemini API has a different structure than OpenAI:\n",
    "# 1. Create a GenerativeModel instance with your model choice\n",
    "# 2. Call generate_content() with your prompt\n",
    "# 3. Access the text via response.text\n",
    "#\n",
    "# Note: Unlike OpenAI, you don't need to structure messages with roles\n",
    "# for simple single-turn queries (though you can for multi-turn chats)\n",
    "# ============================================================================\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Step 1: Create a model instance\n",
    "# GenerativeModel is a class that wraps a specific Gemini model\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# Step 2: Generate content\n",
    "# This makes the actual API call to Google's servers\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# Step 3: Access the response text\n",
    "# The response object contains metadata and the actual text reply\n",
    "print(\"ðŸ¤– Gemini Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07dede6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function 'get_completion_gemini' defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATING A REUSABLE HELPER FUNCTION FOR GEMINI\n",
    "# ============================================================================\n",
    "# Just like we did for OpenAI, let's create a simple wrapper function\n",
    "# This makes it easy to switch between models and reuse code\n",
    "# ============================================================================\n",
    "\n",
    "def get_completion_gemini(prompt, model_name=\"gemini-2.5-flash\"):\n",
    "    \"\"\"\n",
    "    Get a completion from Google's Gemini model.\n",
    "    \n",
    "    This is a simplified wrapper that handles the boilerplate of making\n",
    "    API calls to Google's Gemini service.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the model.\n",
    "        model_name (str): The name of the Gemini model to use. Options include:\n",
    "                         - \"gemini-2.5-flash\" (default, fast and capable)\n",
    "                         - \"gemini-2.5-pro\" (most capable)\n",
    "                         - \"gemini-2.0-flash\" (balanced)\n",
    "\n",
    "    Returns:\n",
    "        str: The text of the response.\n",
    "        \n",
    "    Example:\n",
    "        >>> response = get_completion_gemini(\"What is 2+2?\")\n",
    "        >>> print(response)\n",
    "        \"2 + 2 equals 4.\"\n",
    "    \"\"\"\n",
    "    # Create a model instance for the specified model\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    \n",
    "    # Generate content and return the text response\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "print(\"âœ… Helper function 'get_completion_gemini' defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "381bd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Gemini 2.5 Flash Response:\n",
      "--------------------------------------------------\n",
      "Here's an explanation of Generative AI in two bullet points:\n",
      "\n",
      "*   **Creates novel content:** Unlike traditional AI that analyzes or classifies existing data, Generative AI models are designed to produce entirely new and original content, such as text, images, audio, or video, that didn't exist before.\n",
      "*   **Learns patterns from vast datasets:** These models are trained on enormous amounts of existing data to understand the underlying patterns, structures, and styles, which they then use to generate new outputs that are coherent and often indistinguishable from human-created content.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TESTING THE GEMINI HELPER FUNCTION\n",
    "# ============================================================================\n",
    "# Let's use the same prompt we used with OpenAI to compare responses\n",
    "# This is a great way to evaluate different models for your use case\n",
    "# ============================================================================\n",
    "\n",
    "prompt = 'Explain Generative AI in 2 bullet points'\n",
    "\n",
    "# Use our helper function to get a response from Gemini 2.5 Flash\n",
    "response = get_completion_gemini(prompt=prompt, model_name=\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"ðŸ¤– Gemini 2.5 Flash Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)\n",
    "\n",
    "# ðŸ’¡ Exercise: Compare this response to the GPT-4o-mini and GPT-3.5-Turbo\n",
    "# responses from earlier. Notice any differences in style, detail, or accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486fb12",
   "metadata": {},
   "source": [
    "# ðŸ“ Summary & Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "In this notebook, we explored how to use commercial LLM APIs **natively** (without frameworks like LangChain):\n",
    "\n",
    "### OpenAI GPT\n",
    "- Uses the `openai` Python library\n",
    "- Requires a **paid API key** with billing set up\n",
    "- Uses a **messages array** structure with roles (system, user, assistant)\n",
    "- Main method: `openai.chat.completions.create()`\n",
    "\n",
    "### Google Gemini\n",
    "- Uses the `google-generativeai` Python library\n",
    "- Offers a **free tier** for learning and experimentation\n",
    "- Simpler API structure with `GenerativeModel` class\n",
    "- Main method: `model.generate_content()`\n",
    "\n",
    "## Comparison Quick Reference\n",
    "\n",
    "| Feature | OpenAI | Gemini |\n",
    "|---------|--------|--------|\n",
    "| Import | `import openai` | `import google.generativeai as genai` |\n",
    "| Configure | `openai.api_key = key` | `genai.configure(api_key=key)` |\n",
    "| API Call | `openai.chat.completions.create()` | `model.generate_content()` |\n",
    "| Response | `response.choices[0].message.content` | `response.text` |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Practice**: Try different prompts with both APIs\n",
    "2. **Experiment**: Change the `temperature` parameter and observe how outputs vary\n",
    "3. **Compare**: Test the same prompt across different models\n",
    "4. **Move On**: Learn how to use these same LLMs with **LangChain** in the next notebook!\n",
    "\n",
    "> ðŸŽ¯ **Coming Up Next**: In `2. Commercial_LLMs_with_LangChain.ipynb`, we'll see how LangChain simplifies working with these APIs and provides powerful abstractions for building LLM applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
