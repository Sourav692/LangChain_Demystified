{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84df7e99",
   "metadata": {},
   "source": [
    "# üéØ Introduction\n",
    "\n",
    "LangChain memory systems provide different strategies for managing conversation history in AI applications. Each memory type addresses specific challenges related to context retention, memory efficiency, and conversation management.\n",
    "\n",
    "# üìã Memory Architecture Overview\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "- **Memory Buffer**: Storage mechanism for conversation messages  \n",
    "- **Message History**: Sequence of user and AI interactions  \n",
    "- **Memory Variables**: Processed memory content for prompt templates  \n",
    "- **Context Window**: Active memory content sent to language models  \n",
    "\n",
    "## Memory System Layers\n",
    "\n",
    "- **Storage Layer**: Where messages are physically stored  \n",
    "- **Strategy Layer**: How messages are retrieved and managed  \n",
    "- **Processing Layer**: How memory is formatted for consumption  \n",
    "\n",
    "# üß† ConversationBufferMemory\n",
    "\n",
    "## Theory\n",
    "\n",
    "`ConversationBufferMemory` implements a simple linear storage approach where all conversation messages are retained in chronological order. This memory type prioritizes complete context preservation over memory efficiency.\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "- **Complete Retention**: Every message is preserved  \n",
    "- **Linear Access**: Messages stored in chronological sequence  \n",
    "- **No Filtering**: All messages available for retrieval  \n",
    "- **Unbounded Growth**: Memory size increases with conversation length  \n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Short to medium-length conversations  \n",
    "- Applications requiring complete conversation history  \n",
    "- Debugging and conversation analysis  \n",
    "- Prototyping and development environments  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1524ce",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e62599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Messages in Buffer ===\n",
      "1. human: Hi, I'm Sourav from Databricks\n",
      "2. ai: Hello Sourav! Nice to meet you.\n",
      "3. human: I work with Apache Spark\n",
      "4. ai: That's great! Spark is powerful for big data.\n",
      "5. human: What's my name?\n",
      "\n",
      "Total messages: 5\n",
      "\n",
      "Memory as string:\n",
      "Human: Hi, I'm Sourav from Databricks\n",
      "AI: Hello Sourav! Nice to meet you.\n",
      "Human: I work with Apache Spark\n",
      "AI: That's great! Spark is powerful for big data.\n",
      "Human: What's my name?\n",
      "\n",
      "Direct buffer access:\n",
      "Human: Hi, I'm Sourav from Databricks\n",
      "AI: Hello Sourav! Nice to meet you.\n",
      "Human: I work with Apache Spark\n",
      "AI: That's great! Spark is powerful for big data.\n",
      "Human: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/xkrl1q210t5_4t4hvbx286800000gp/T/ipykernel_23267/2196926004.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create memory instance\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add messages manually\n",
    "memory.chat_memory.add_user_message(\"Hi, I'm Sourav from Databricks\")\n",
    "memory.chat_memory.add_ai_message(\"Hello Sourav! Nice to meet you.\")\n",
    "memory.chat_memory.add_user_message(\"I work with Apache Spark\")\n",
    "memory.chat_memory.add_ai_message(\"That's great! Spark is powerful for big data.\")\n",
    "memory.chat_memory.add_user_message(\"What's my name?\")\n",
    "\n",
    "# Check what's stored\n",
    "print(\"=== All Messages in Buffer ===\")\n",
    "for i, message in enumerate(memory.chat_memory.messages):\n",
    "    print(f\"{i+1}. {message.type}: {message.content}\")\n",
    "\n",
    "print(f\"\\nTotal messages: {len(memory.chat_memory.messages)}\")\n",
    "\n",
    "# Get memory as variables (what would be passed to prompt)\n",
    "memory_vars = memory.load_memory_variables({})\n",
    "print(f\"\\nMemory as string:\\n{memory_vars['history']}\")\n",
    "\n",
    "# Get buffer content directly\n",
    "print(f\"\\nDirect buffer access:\\n{memory.buffer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75667d5e",
   "metadata": {},
   "source": [
    "## ‚úÖ Advantages & ‚ùå Limitations\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- ‚úÖ **Complete Context**: No information loss  \n",
    "- ‚úÖ **Simple Implementation**: Straightforward to use and understand  \n",
    "- ‚úÖ **Full History Access**: All messages available for analysis  \n",
    "- ‚úÖ **Deterministic Behavior**: Predictable memory retrieval  \n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- ‚ùå **Memory Growth**: Unbounded memory consumption  \n",
    "- ‚ùå **Performance Degradation**: Slower with long conversations  \n",
    "- ‚ùå **Token Limitations**: May exceed LLM context limits  \n",
    "- ‚ùå **Cost Implications**: More tokens sent to LLM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ddf38",
   "metadata": {},
   "source": [
    "# ü™ü ConversationBufferWindowMemory\n",
    "\n",
    "## Theory\n",
    "\n",
    "`ConversationBufferWindowMemory` implements a sliding window approach that maintains only the most recent **K** messages. This strategy balances context preservation with memory efficiency by using a fixed-size circular buffer.\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "- **Fixed Window Size**: Maintains exactly K messages  \n",
    "- **Recency Bias**: Prioritizes recent interactions  \n",
    "- **Automatic Eviction**: Oldest messages automatically removed  \n",
    "- **Bounded Memory**: Predictable memory consumption  \n",
    "\n",
    "### Mathematical Model:\n",
    "\n",
    "> Let `M = [m‚ÇÅ, m‚ÇÇ, ..., m‚Çô]` represent the message sequence.  \n",
    "> The memory buffer retains the last **K** messages:  \n",
    "> `Window(M, K) = [m‚Çô‚Çã‚Çñ‚Çä‚ÇÅ, ..., m‚Çô]`, where `n` is the total number of messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47236dc7",
   "metadata": {},
   "source": [
    "## üìå Use Cases\n",
    "\n",
    "- Long-running conversations  \n",
    "- Resource-constrained environments  \n",
    "- Applications where recent context is most important  \n",
    "- Customer support chatbots  \n",
    "- Real-time interactive systems  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f863a",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575ac6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Window Memory (k=4) ===\n",
      "Window size (k): 4\n",
      "Messages in window: 9\n",
      "1. human: Message 1: Hi, I'm Sourav\n",
      "2. ai: Response 1: Hello Sourav!\n",
      "3. human: Message 2: I work at Databricks\n",
      "4. ai: Response 2: Great company!\n",
      "5. human: Message 3: I use Apache Spark\n",
      "6. ai: Response 3: Spark is powerful!\n",
      "7. human: Message 4: I optimize pipelines\n",
      "8. ai: Response 4: Performance is key!\n",
      "9. human: Message 5: What's my name?\n",
      "\n",
      "Memory output (only window):\n",
      "AI: Response 1: Hello Sourav!\n",
      "Human: Message 2: I work at Databricks\n",
      "AI: Response 2: Great company!\n",
      "Human: Message 3: I use Apache Spark\n",
      "AI: Response 3: Spark is powerful!\n",
      "Human: Message 4: I optimize pipelines\n",
      "AI: Response 4: Performance is key!\n",
      "Human: Message 5: What's my name?\n",
      "\n",
      "=== Adding More Messages ===\n",
      "Messages after adding more: 11\n",
      "1. human: Message 1: Hi, I'm Sourav\n",
      "2. ai: Response 1: Hello Sourav!\n",
      "3. human: Message 2: I work at Databricks\n",
      "4. ai: Response 2: Great company!\n",
      "5. human: Message 3: I use Apache Spark\n",
      "6. ai: Response 3: Spark is powerful!\n",
      "7. human: Message 4: I optimize pipelines\n",
      "8. ai: Response 4: Performance is key!\n",
      "9. human: Message 5: What's my name?\n",
      "10. human: Message 6: Do you remember Message 1?\n",
      "11. ai: Response 6: I only remember recent messages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/xkrl1q210t5_4t4hvbx286800000gp/T/ipykernel_23267/353418606.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Create memory with window size = 4 messages\n",
    "memory = ConversationBufferWindowMemory(k=4)\n",
    "\n",
    "# Add many messages\n",
    "messages = [\n",
    "    (\"user\", \"Message 1: Hi, I'm Sourav\"),\n",
    "    (\"ai\", \"Response 1: Hello Sourav!\"),\n",
    "    (\"user\", \"Message 2: I work at Databricks\"),\n",
    "    (\"ai\", \"Response 2: Great company!\"),\n",
    "    (\"user\", \"Message 3: I use Apache Spark\"),\n",
    "    (\"ai\", \"Response 3: Spark is powerful!\"),\n",
    "    (\"user\", \"Message 4: I optimize pipelines\"),\n",
    "    (\"ai\", \"Response 4: Performance is key!\"),\n",
    "    (\"user\", \"Message 5: What's my name?\"),  # This will push out Message 1\n",
    "]\n",
    "\n",
    "for msg_type, content in messages:\n",
    "    if msg_type == \"user\":\n",
    "        memory.chat_memory.add_user_message(content)\n",
    "    else:\n",
    "        memory.chat_memory.add_ai_message(content)\n",
    "\n",
    "print(\"=== Window Memory (k=4) ===\")\n",
    "print(f\"Window size (k): {memory.k}\")\n",
    "print(f\"Messages in Total Memory: {len(memory.chat_memory.messages)}\")\n",
    "\n",
    "# Show what's currently in the window\n",
    "for i, message in enumerate(memory.chat_memory.messages):\n",
    "    print(f\"{i+1}. {message.type}: {message.content}\")\n",
    "\n",
    "# Get memory variables\n",
    "memory_vars = memory.load_memory_variables({})\n",
    "print(f\"\\nMemory output (only window):\\n{memory_vars['history']}\")\n",
    "\n",
    "# Demonstrate sliding window by adding more messages\n",
    "print(\"\\n=== Adding More Messages ===\")\n",
    "memory.chat_memory.add_user_message(\"Message 6: Do you remember Message 1?\")\n",
    "memory.chat_memory.add_ai_message(\"Response 6: I only remember recent messages\")\n",
    "\n",
    "print(f\"Messages after adding more: {len(memory.chat_memory.messages)}\")\n",
    "for i, message in enumerate(memory.chat_memory.messages):\n",
    "    print(f\"{i+1}. {message.type}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f09c6c",
   "metadata": {},
   "source": [
    "## üìè Window Size Selection\n",
    "\n",
    "### Factors to Consider:\n",
    "\n",
    "- **Conversation Complexity**: More complex topics need larger windows  \n",
    "- **Memory Constraints**: Available system memory  \n",
    "- **Response Quality**: Balance between context and focus  \n",
    "- **Performance Requirements**: Larger windows = more processing  \n",
    "\n",
    "### Recommended Window Sizes:\n",
    "\n",
    "- **Small (2‚Äì4 messages)**: Simple Q&A, quick interactions  \n",
    "- **Medium (6‚Äì10 messages)**: Standard conversations, customer support  \n",
    "- **Large (12‚Äì20 messages)**: Complex discussions, technical support  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Advantages & ‚ùå Limitations\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- ‚úÖ **Bounded Memory**: Predictable memory usage  \n",
    "- ‚úÖ **Recent Focus**: Maintains relevant recent context  \n",
    "- ‚úÖ **Performance**: Consistent performance regardless of conversation length  \n",
    "- ‚úÖ **Automatic Management**: No manual cleanup required  \n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- ‚ùå **Context Loss**: Early conversation information lost  \n",
    "- ‚ùå **Fixed Strategy**: Cannot adapt window size dynamically  \n",
    "- ‚ùå **Important Information Loss**: Critical early context may be evicted  \n",
    "- ‚ùå **Reference Failures**: Cannot reference old information  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9348e",
   "metadata": {},
   "source": [
    "# üìù ConversationSummaryMemory\n",
    "\n",
    "## Theory\n",
    "\n",
    "`ConversationSummaryMemory` implements an intelligent compression strategy that maintains conversation context through summarization rather than raw message storage. This approach uses natural language processing (typically LLMs) to distill the essence of the conversation while preserving important information.\n",
    "\n",
    "### Key Principles:\n",
    "\n",
    "- **Lossy Compression**: Reduces memory footprint through summarization  \n",
    "- **Context Preservation**: Maintains semantic meaning  \n",
    "- **Adaptive Length**: Summary size remains relatively stable  \n",
    "- **Intelligent Processing**: Uses LLM for content analysis  \n",
    "\n",
    "## üìä Information Theory Perspective\n",
    "\n",
    "- **Raw Messages**: High redundancy, complete information  \n",
    "- **Summary**: Low redundancy, essential information  \n",
    "- **Compression Ratio**: `Original_Size / Summary_Size`  \n",
    "- **Information Loss**: Details sacrificed for efficiency  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Use Cases\n",
    "\n",
    "- Very long conversations  \n",
    "- Memory-constrained environments  \n",
    "- Applications requiring historical context without details  \n",
    "- Knowledge-intensive conversations  \n",
    "- Multi-session continuity  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Implementation\n",
    "\n",
    "> Typically uses an LLM or summarization model to periodically update the summary as the conversation progresses.  \n",
    "> Summarization can be triggered based on message count, time, or interaction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499e1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Messages ===\n",
      "1. human: Hi, I'm Sourav Banerjee from Databricks\n",
      "2. ai: Hello Sourav! Nice to meet you. How can I help?\n",
      "3. human: I work on Apache Spark optimization and big data pipelines\n",
      "4. ai: That's fascinating! Spark optimization is crucial for performance.\n",
      "5. human: I also work with Delta Lake and MLflow\n",
      "6. ai: Great tools! Delta Lake provides ACID transactions for data lakes.\n",
      "7. human: I'm particularly interested in performance tuning\n",
      "8. ai: Performance tuning involves query optimization and resource management.\n",
      "\n",
      "Total original messages: 8\n",
      "\n",
      "=== Generated Summary ===\n",
      "Summary: [SystemMessage(content='', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "=== Buffer Content ===\n",
      "Buffer: \n",
      "\n",
      "=== Adding New Messages ===\n",
      "Updated memory:\n",
      "[SystemMessage(content='', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# NOTE: ConversationSummaryMemory needs an LLM ONLY for summarization\n",
    "# The memory functionality itself is separate from conversation chains\n",
    "llm_for_summary = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# Create summary memory\n",
    "memory = ConversationSummaryMemory(return_messages=True, llm=llm_for_summary)\n",
    "\n",
    "# Add messages (these will be summarized)\n",
    "conversation_history = [\n",
    "    (\"user\", \"Hi, I'm Sourav Banerjee from Databricks\"),\n",
    "    (\"ai\", \"Hello Sourav! Nice to meet you. How can I help?\"),\n",
    "    (\"user\", \"I work on Apache Spark optimization and big data pipelines\"),\n",
    "    (\"ai\", \"That's fascinating! Spark optimization is crucial for performance.\"),\n",
    "    (\"user\", \"I also work with Delta Lake and MLflow\"),\n",
    "    (\"ai\", \"Great tools! Delta Lake provides ACID transactions for data lakes.\"),\n",
    "    (\"user\", \"I'm particularly interested in performance tuning\"),\n",
    "    (\"ai\", \"Performance tuning involves query optimization and resource management.\"),\n",
    "]\n",
    "\n",
    "# Add all messages\n",
    "for msg_type, content in conversation_history:\n",
    "    if msg_type == \"user\":\n",
    "        memory.chat_memory.add_user_message(content)\n",
    "    else:\n",
    "        memory.chat_memory.add_ai_message(content)\n",
    "\n",
    "print(\"=== Original Messages ===\")\n",
    "for i, message in enumerate(memory.chat_memory.messages):\n",
    "    print(f\"{i+1}. {message.type}: {message.content}\")\n",
    "\n",
    "print(f\"\\nTotal original messages: {len(memory.chat_memory.messages)}\")\n",
    "\n",
    "# Get the summary (this is where LLM is used - ONLY for summarization)\n",
    "memory_vars = memory.load_memory_variables({})\n",
    "summary = memory_vars['history']\n",
    "\n",
    "print(f\"\\n=== Generated Summary ===\")\n",
    "print(f\"Summary: {summary}\")\n",
    "\n",
    "# Check the buffer (summary storage)\n",
    "print(f\"\\n=== Buffer Content ===\")\n",
    "print(f\"Buffer: {memory.buffer}\")\n",
    "\n",
    "# Add new messages after summarization\n",
    "print(f\"\\n=== Adding New Messages ===\")\n",
    "memory.chat_memory.add_user_message(\"What did we discuss about my work?\")\n",
    "memory.chat_memory.add_ai_message(\"We discussed your work with Spark optimization at Databricks.\")\n",
    "\n",
    "# Get updated memory (summary + new messages)\n",
    "updated_memory = memory.load_memory_variables({})\n",
    "print(f\"Updated memory:\\n{updated_memory['history']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74b64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
