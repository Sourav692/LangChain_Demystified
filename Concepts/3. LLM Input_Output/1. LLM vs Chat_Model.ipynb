{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddee9be",
   "metadata": {},
   "source": [
    "# Exploring LLMs and ChatModels for LLM Input / Output with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00846b9e",
   "metadata": {},
   "source": [
    "## Install OpenAI, HuggingFace and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3702f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain==0.3.11\n",
    "!pip install -qq langchain-openai==0.2.12\n",
    "!pip install -qq langchain-community==0.3.11\n",
    "!pip install -qq huggingface_hub==0.30.2\n",
    "!pip install -qq langchain-core==0.3.63 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88aef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run if you want to use only chatgpt\n",
    "# This is for accessing open LLMs from huggingface\n",
    "!pip install -qq transformers==4.46.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89626e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain_google_genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298ac1b",
   "metadata": {},
   "source": [
    "## Enter API Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5838b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5106e",
   "metadata": {},
   "source": [
    "# Model I/O\n",
    "\n",
    "In LangChain, the central part of any application is the language model. This module provides crucial tools for working effectively with any language model, ensuring it integrates smoothly and communicates well.\n",
    "\n",
    "### Key Components of Model I/O\n",
    "\n",
    "**LLMs and Chat Models (used interchangeably):**\n",
    "- **LLMs:**\n",
    "  - **Definition:** Pure text completion models.\n",
    "  - **Input/Output:** Receives a text string and returns a text string.\n",
    "- **Chat Models:**\n",
    "  - **Definition:** Based on a language model but with different input and output types.\n",
    "  - **Input/Output:** Takes a list of chat messages as input and produces a chat message as output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0e54b",
   "metadata": {},
   "source": [
    "## Chat Models and LLMs\n",
    "\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain does not implement or build its own LLMs. It provides a standard API for interacting with almost every LLM out there.\n",
    "\n",
    "There are lots of LLM providers (OpenAI, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001447c",
   "metadata": {},
   "source": [
    "## Accessing Commercial LLMs like ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e24fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Accessing ChatGPT as an LLM\n",
    "\n",
    "Here we will show how to access a basic ChatGPT Instruct LLM. However the ChatModel interface which we will see later, is better because the LLM API doesn't support the chat models like `gpt-3.5-turbo`and only support the `instruct`models which can respond to instructions but can't have a conversation with you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e543206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "chatgpt = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f954b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Generative AI is a subset of artificial intelligence that focuses on creating new and original content, rather than just analyzing and processing existing data.\n",
      "\n",
      "2. It uses algorithms and machine learning techniques to generate new ideas, designs, or solutions based on a set of input data or parameters.\n",
      "\n",
      "3. Generative AI has a wide range of applications, including creating art, music, and text, as well as assisting in product design and optimization. It has the potential to revolutionize industries by automating creative tasks and providing innovative solutions.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
    "response = chatgpt.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41fe0c",
   "metadata": {},
   "source": [
    "### Accessing ChatGPT as an Chat Model LLM\n",
    "\n",
    "Here we will show how to access the more advanced ChatGPT Turbo Chat-based LLM. The ChatModel interface is better because this supports the chat models like `gpt-3.5-turbo`which can respond to instructions as well as have a conversation with you. We will look at the conversation aspect slightly later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63dc0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='- Generative AI is a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns and data it has been trained on.\\n- It uses algorithms and neural networks to generate new content that is similar to the input data it has been trained on, but with variations and creativity.\\n- Generative AI has applications in various fields, including art, design, music composition, and even creating realistic deepfake videos.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 19, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--1a28f2ae-e2a0-498f-ae11-f653d71dcc1b-0' usage_metadata={'input_tokens': 19, 'output_tokens': 94, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
    "response = chatgpt.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d34c8b",
   "metadata": {},
   "source": [
    "## Accessing Gemini as an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd382e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dbaa184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Creates new content:** Generative AI uses algorithms to produce various forms of content, including text, images, audio, and video, rather than just analyzing or classifying existing data.\n",
      "\n",
      "* **Learns from existing data:**  It's trained on massive datasets to learn patterns and structures, allowing it to generate outputs that resemble the training data but are novel and unique.\n",
      "\n",
      "* **Uses various techniques:**  Different generative AI models employ techniques like transformers, generative adversarial networks (GANs), and variational autoencoders (VAEs) to achieve content generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Explain what is Generative AI in 3 bullet points\"\"\"\n",
    "response = gemini.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dac2b6",
   "metadata": {},
   "source": [
    "## Accessing Local LLMs with HuggingFacePipeline API\n",
    "\n",
    "Hugging Face models can be run locally through the `HuggingFacePipeline` class. However remember you need a good GPU to get fast inference\n",
    "\n",
    "The Hugging Face Model Hub hosts over 500k models, 90K+ open LLMs\n",
    "\n",
    "These can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the `HuggingFaceEndpoint` API we saw earlier.\n",
    "\n",
    "To use, you should have the `transformers` python package installed, as well as `pytorch`.\n",
    "\n",
    "Advantages include the model being completely local, high privacy and security. Disadvantages are basically the necessity of a good compute infrastructure, preferably with a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2afec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4069e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma_params = {\n",
    "#                   \"do_sample\": False, # greedy decoding - temperature = 0\n",
    "#                   \"return_full_text\": False, # don't return input prompt\n",
    "#                   \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
    "#                 }\n",
    "\n",
    "# local_llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=gemma_params,\n",
    "#     # device=0 # when running on Colab selects the GPU, you can change this if you run it on your own instance if needed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "467935d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a48a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gemma2B when used locally expects input prompt to be formatted in a specific way\n",
    "# # check more details here: https://huggingface.co/google/gemma-1.1-2b-it#chat-template\n",
    "# gemma_prompt = \"\"\"<bos><start_of_turn>user\\n\"\"\" + prompt + \"\"\"\\n<end_of_turn>\n",
    "# <start_of_turn>model\n",
    "# \"\"\"\n",
    "# print(gemma_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f4e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = local_llm.invoke(gemma_prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40c0b3",
   "metadata": {},
   "source": [
    "### Accessing Open LLMs in HuggingFace as a Chat Model LLM\n",
    "\n",
    "Here we will show how to access open LLMs from HuggingFace like Google Gemma 2B and make them have a conversation with you. We will look at the conversation aspect slightly later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi3_params = {\n",
    "                  \"wait_for_model\": True, # waits if model is not available in Hugginface serve\n",
    "                  \"do_sample\": False, # greedy decoding - temperature = 0\n",
    "                  \"return_full_text\": False, # don't return input prompt\n",
    "                  \"max_new_tokens\": 1000, # max tokens answer can go upto\n",
    "                }\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    # max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=\"\",\n",
    "   **phi3_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1f5b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "chat_gemma = ChatHuggingFace(llm=llm,\n",
    "                             model_id='google/gemma-1.1-2b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70bc385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Creates new content:** Generative AI uses algorithms to produce various forms of content, including text, images, audio, and video, rather than just analyzing or classifying existing data.\n",
      "\n",
      "* **Learns from existing data:**  It's trained on massive datasets to learn patterns and structures, allowing it to generate outputs that resemble the training data but are novel and unique.\n",
      "\n",
      "* **Uses various techniques:**  Different generative AI models employ techniques like transformers, generative adversarial networks (GANs), and variational autoencoders (VAEs) to achieve content generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8055076",
   "metadata": {},
   "source": [
    "## Message Types for ChatModels and Conversational Prompting\n",
    "\n",
    "Conversational prompting is basically you, the user, having a full conversation with the LLM. The conversation history is typically represented as a list of messages.\n",
    "\n",
    "ChatModels process a list of messages, receiving them as input and responding with a message. Messages are characterized by a few distinct types and properties:\n",
    "\n",
    "- **Role:** Indicates who is speaking in the message. LangChain offers different message classes for various roles.\n",
    "- **Content:** The substance of the message, which can vary:\n",
    "  - A string (commonly handled by most models)\n",
    "  - A list of dictionaries (for multi-modal inputs, where each dictionary details the type and location of the input)\n",
    "\n",
    "Additionally, messages have an `additional_kwargs` property, used for passing extra information specific to the message provider, not typically general. A well-known example is `function_call` from OpenAI.\n",
    "\n",
    "### Specific Message Types\n",
    "\n",
    "- **HumanMessage:** A user-generated message, usually containing only content.\n",
    "- **AIMessage:** A message from the model, potentially including `additional_kwargs`, like `tool_calls` for invoking OpenAI tools.\n",
    "- **SystemMessage:** A message from the system instructing model behavior, typically containing only content. Not all models support this type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efbb5b",
   "metadata": {},
   "source": [
    "## Conversational Prompting with ChatGPT\n",
    "\n",
    "Here we use the `ChatModel` API in `ChatOpenAI` to have a full conversation with ChatGPT while maintaining a full flow of the historical conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "034576fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de477ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage,SystemMessage\n",
    "\n",
    "prompt = \"\"\"Can you explain what is Generative AI in 3 bullet points?\"\"\"\n",
    "sys_prompt = \"\"\"Act as a helpful assistant and give meaningful examples in your responses.\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = chatgpt.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a80d8d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Act as a helpful assistant and give meaningful examples in your responses.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you explain what is Generative AI in 3 bullet points?'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cd7ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generative AI refers to a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns it has learned from existing data.\n",
      "2. One popular example of generative AI is GPT-3 (Generative Pre-trained Transformer 3), a language model developed by OpenAI that can generate human-like text based on the input it receives.\n",
      "3. Another example is StyleGAN, a generative adversarial network (GAN) that can generate highly realistic images of faces, animals, and other objects.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2cfb3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"1. Generative AI is a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns it has learned from existing data.\\n2. It uses techniques like neural networks and deep learning to generate realistic and original outputs that mimic human creativity.\\n3. Examples of generative AI include text generators like GPT-3, image generators like StyleGAN, and music generators like OpenAI's MuseNet.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 38, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c8960707-af3b-4144-a55f-cd84b1946123-0', usage_metadata={'input_tokens': 38, 'output_tokens': 92, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc6f8b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generative AI refers to a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns it has learned from existing data.\n",
      "2. One popular example of generative AI is GPT-3 (Generative Pre-trained Transformer 3), a language model developed by OpenAI that can generate human-like text based on the input it receives.\n",
      "3. Another example is StyleGAN, a generative adversarial network (GAN) that can generate highly realistic images of faces, animals, and other objects.\n"
     ]
    }
   ],
   "source": [
    "# Another way to define msg\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content = sys_prompt),\n",
    "    HumanMessage(content = prompt)\n",
    "\n",
    "]\n",
    "response = chatgpt.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c74c542e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Act as a helpful assistant and give meaningful examples in your responses.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you explain what is Generative AI in 3 bullet points?'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74f56f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Act as a helpful assistant and give meaningful examples in your responses.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you explain what is Generative AI in 3 bullet points?'},\n",
       " AIMessage(content=\"1. Generative AI is a type of artificial intelligence that is capable of creating new content, such as images, text, or music, based on patterns it has learned from existing data.\\n2. It uses techniques like neural networks and deep learning to generate realistic and original outputs that mimic human creativity.\\n3. Examples of generative AI include text generators like GPT-3, image generators like StyleGAN, and music generators like OpenAI's MuseNet.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 38, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c8960707-af3b-4144-a55f-cd84b1946123-0', usage_metadata={'input_tokens': 38, 'output_tokens': 92, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='What did we discuss so far?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the past conversation history into messages\n",
    "message.append(response)\n",
    "# add the new prompt to the conversation history list\n",
    "prompt = \"\"\"What did we discuss so far?\"\"\"\n",
    "message.append(HumanMessage(content=prompt))\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53949a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So far, we have discussed the concept of Generative AI in three main points:\\n1. Generative AI's ability to create new content based on learned patterns.\\n2. The use of neural networks and deep learning in generative AI.\\n3. Examples of generative AI applications such as text generators, image generators, and music generators.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent the conversation history along with the new prompt to chatgpt\n",
    "response = chatgpt.invoke(message)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7b51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
