{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# LangChain Runnables: A Comprehensive Guide\n",
    "\n",
    "## Overview\n",
    "LangChain Runnables are the fundamental building blocks that enable you to create complex, composable AI workflows. This notebook explores the core runnable types and demonstrates how to build sophisticated data processing pipelines.\n",
    "\n",
    "## What You'll Learn:\n",
    "- **RunnablePassthrough**: Preserving input context\n",
    "- **RunnableLambda**: Converting functions to runnables\n",
    "- **RunnableParallel**: Concurrent execution patterns\n",
    "- **Sequential Chaining**: Building step-by-step workflows\n",
    "- **Conditional Logic**: Dynamic routing based on content\n",
    "- **Streaming & Batch Processing**: Handling different execution modes\n",
    "\n",
    "## Key Concepts:\n",
    "- **Composability**: Combining simple runnables to create complex workflows\n",
    "- **Type Safety**: Ensuring data flows correctly between components\n",
    "- **Performance**: Parallel execution for independent operations\n",
    "- **Flexibility**: Adapting to different input types and processing needs\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and Initial Configuration\n",
    "\n",
    "Let's start by clearing any previous outputs and importing the essential runnable components from LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any previous output from the notebook for a clean start\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the core runnable classes from LangChain\n",
    "# RunnablePassthrough: Passes input through unchanged\n",
    "# RunnableLambda: Wraps any Python function to make it a runnable\n",
    "# RunnableParallel: Executes multiple runnables in parallel\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. RunnablePassthrough: Basic Input Handling\n",
    "\n",
    "**RunnablePassthrough** is the simplest runnable that takes an input and returns it unchanged. This is useful when you want to preserve the original input in a chain while also performing other operations on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "# Create a RunnablePassthrough instance\n",
    "# This will simply return whatever input it receives\n",
    "chain = RunnablePassthrough()\n",
    "\n",
    "# Test with a simple string - it returns the same string\n",
    "print(chain.invoke(\"abcd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. RunnableLambda: Converting Functions to Runnables\n",
    "\n",
    "**RunnableLambda** allows you to convert any Python function into a runnable that can be chained with other components. This is extremely powerful for integrating custom business logic into your LangChain workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Define a custom function that calculates the length of input text\n",
    "def output_length(input: str):\n",
    "    \"\"\"Calculate the length of the input string\"\"\"\n",
    "    output = len(input)\n",
    "    return output\n",
    "\n",
    "# Convert the function to a runnable using RunnableLambda\n",
    "chain = RunnableLambda(output_length)\n",
    "\n",
    "# Test the chain - it should return the length of the input string\n",
    "print(chain.invoke(\"input to output\"))  # Should print 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function that works with dictionary input\n",
    "def sum(item: dict):\n",
    "    \"\"\"Add two numbers from a dictionary\"\"\"\n",
    "    return item[\"a\"] + item[\"b\"]\n",
    "\n",
    "# Convert to runnable and test with dictionary input\n",
    "chain = RunnableLambda(sum)\n",
    "chain.invoke({\"a\": 1, \"b\": 2})  # Should return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Alternative approach: Define a function with separate parameters\n",
    "def sum_values(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers passed as separate parameters\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Use lambda to adapt the function to work with dictionary input\n",
    "# This demonstrates how to bridge different function signatures\n",
    "chain = RunnableLambda(lambda item: sum_values(item[\"a\"], item[\"b\"]))\n",
    "\n",
    "# Test the adapted function\n",
    "result = chain.invoke({\"a\": 1, \"b\": 2})\n",
    "print(result)  # Should print 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. RunnableParallel: Concurrent Processing\n",
    "\n",
    "**RunnableParallel** executes multiple runnables concurrently on the same input, collecting their outputs into a dictionary. This is perfect for scenarios where you need to extract different types of information from the same input simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': 'start-tech academy', 'length': 18}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parallel chain that:\n",
    "# 1. Preserves the original text using RunnablePassthrough\n",
    "# 2. Calculates the length using our custom function\n",
    "chain = RunnableParallel(\n",
    "    text1=RunnablePassthrough(),      # Keep original input\n",
    "    length=RunnableLambda(output_length)  # Calculate length\n",
    ")\n",
    "\n",
    "# Test with a sample text - returns both original text and its length\n",
    "chain.invoke(\"start-tech academy\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Integration with Large Language Models (LLMs)\n",
    "\n",
    "Now let's explore how to integrate runnables with LLMs like OpenAI's GPT models. This section demonstrates how to set up the OpenAI client and create chains that combine custom processing with AI capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and OpenAI API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Make sure you have a .env file with OPENAI_API_KEY=your_api_key_here\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from environment variables\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key as an environment variable\n",
    "# This is required for the OpenAI client to authenticate\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI chat model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a ChatOpenAI instance with specific configuration\n",
    "# gpt-4o-mini: A cost-effective model for most tasks\n",
    "# temperature=0: Ensures deterministic, consistent responses\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template and chain it with the LLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a simple prompt template that accepts user queries\n",
    "prompt_txt = \"{query}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# Create a chain by connecting the prompt template to the LLM\n",
    "# The pipe operator (|) creates a sequential chain\n",
    "# Data flows: input → prompt_template → chatgpt → output\n",
    "llmchain = (prompt_template\n",
    "              |\n",
    "           chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to algorithms that can create new content, such as text, images, or music, by learning patterns from existing data.\n"
     ]
    }
   ],
   "source": [
    "# Test the chain with a sample query\n",
    "response = llmchain.invoke({'query': 'Explain Generative AI in 1 line'})\n",
    "\n",
    "# Print the AI's response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RunnableLambda Patterns\n",
    "\n",
    "### Custom Data Processing with RunnableLambda\n",
    "\n",
    "This section demonstrates how to create more sophisticated data processing pipelines using RunnableLambda to handle complex data transformations before sending to the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Sourav | Role: Solution Architect | Location: Bengaluru\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for custom function wrapping\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a custom function to format user data\n",
    "def format_user_data(user_info):\n",
    "    \"\"\"Process user information for downstream components\"\"\"\n",
    "    # Create a formatted string with user details\n",
    "    return f\"User: {user_info['name']} | Role: {user_info['role']} | Location: {user_info['location']}\"\n",
    "\n",
    "# Convert the function to a Runnable for use in chains\n",
    "formatter = RunnableLambda(format_user_data)\n",
    "\n",
    "# Test the formatter with sample user data\n",
    "user_data = {\"name\": \"Sourav\", \"role\": \"Solution Architect\", \"location\": \"Bengaluru\"}\n",
    "result = formatter.invoke(user_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RunnablePassthrough - Preserving Input Context\n",
    "\n",
    "### Complex Data Processing with Context Preservation\n",
    "\n",
    "This example shows how to use RunnablePassthrough to maintain the original input while simultaneously processing it through other functions. This is crucial for maintaining context in complex AI workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text \"Building scalable data platforms requires careful architecture planning\" emphasizes the importance of strategic design in the development of data platforms that can grow and adapt to increasing demands. \n",
      "\n",
      "### Analysis:\n",
      "\n",
      "1. **Keywords**:\n",
      "   - **Building**: This suggests an active process of creation and development, indicating that constructing data platforms is not a passive task but requires effort and expertise.\n",
      "   - **Scalable**: This is a critical term in technology and data management, referring to the ability of a system to handle growth, whether in terms of data volume, user load, or functionality. Scalability is essential for ensuring that a platform can evolve without requiring a complete redesign.\n",
      "   - **Platforms**: This term indicates a foundational technology or framework that supports various applications or services. In the context of data, it implies a comprehensive system that manages data storage, processing, and analysis.\n",
      "\n",
      "2. **Key Themes**:\n",
      "   - **Architecture Planning**: The phrase highlights the necessity of thoughtful and strategic planning in the architecture of data platforms. This involves considering various factors such as data flow, storage solutions, processing capabilities, and user access.\n",
      "   - **Scalability Challenges**: The text implies that without careful planning, a data platform may struggle to scale effectively, leading to performance issues or the need for costly overhauls in the future.\n",
      "\n",
      "3. **Implications**:\n",
      "   - Organizations looking to build data platforms must invest time and resources into the architectural design phase to ensure long-term success and adaptability.\n",
      "   - The focus on scalability suggests that businesses should anticipate future growth and design their platforms accordingly, rather than merely addressing current needs.\n",
      "\n",
      "### Conclusion:\n",
      "Overall, the text serves as a reminder of the complexities involved in creating data platforms and the critical role of architecture in ensuring that these platforms can scale effectively to meet future demands.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary components for context preservation\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a function to extract keywords from text\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Simple keyword extraction that filters words longer than 4 characters\"\"\"\n",
    "    words = text.lower().split()\n",
    "    keywords = [word for word in words if len(word) > 4]\n",
    "    return keywords[:3]  # Return top 3 keywords\n",
    "\n",
    "# Create a prompt template that uses both original text and keywords\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this text: {original_text}\\nKeywords found: {keywords}\"\n",
    ")\n",
    "\n",
    "# Build a chain that preserves original input and adds keyword analysis\n",
    "# This demonstrates parallel processing: \n",
    "# - RunnablePassthrough keeps the original text\n",
    "# - RunnableLambda processes the text to extract keywords\n",
    "chain = {\n",
    "    \"original_text\": RunnablePassthrough(),      # Preserve original input\n",
    "    \"keywords\": RunnableLambda(extract_keywords)    # Extract keywords\n",
    "} | prompt | chatgpt\n",
    "\n",
    "# Test the chain with a technical text\n",
    "sample_text = \"Building scalable data platforms requires careful architecture planning\"\n",
    "result = chain.invoke(sample_text)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequential Chaining with Pipe Operator (|)\n",
    "\n",
    "### Building Multi-Step Data Processing Pipelines\n",
    "\n",
    "Sequential chaining allows you to build complex workflows where data flows through multiple processing steps in order. Each step receives the output of the previous step as its input, creating a powerful data transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Analysis Summary:\n",
      "- Characters: 119\n",
      "- Words: 18  \n",
      "- Sentences: 3\n",
      "- Avg words per sentence: 6.0\n",
      "Original: \"Data engineering is crucial for ML success. It inv...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for creating processing steps\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Step 1: Input validation and cleaning\n",
    "def validate_input(data):\n",
    "    \"\"\"Validate and clean input data\"\"\"\n",
    "    if not isinstance(data, str) or len(data.strip()) == 0:\n",
    "        raise ValueError(\"Input must be a non-empty string\")\n",
    "    return data.strip()\n",
    "\n",
    "# Step 2: Extract various text metrics\n",
    "def extract_metrics(text):\n",
    "    \"\"\"Extract comprehensive text metrics from the input\"\"\"\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"character_count\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?')\n",
    "    }\n",
    "\n",
    "# Step 3: Generate a formatted summary report\n",
    "def generate_summary(metrics):\n",
    "    \"\"\"Generate a comprehensive summary report from metrics\"\"\"\n",
    "    avg_words_per_sentence = metrics['word_count'] / max(metrics['sentence_count'], 1)\n",
    "    return f\"\"\"\n",
    "Text Analysis Summary:\n",
    "- Characters: {metrics['character_count']}\n",
    "- Words: {metrics['word_count']}  \n",
    "- Sentences: {metrics['sentence_count']}\n",
    "- Avg words per sentence: {avg_words_per_sentence:.1f}\n",
    "Original: \"{metrics['original_text'][:50]}...\"\n",
    "\"\"\"\n",
    "\n",
    "# Create a sequential processing chain\n",
    "# Data flows: input → validate → extract_metrics → generate_summary → output\n",
    "analysis_chain = (\n",
    "    RunnableLambda(validate_input) |     # Step 1: Clean input\n",
    "    RunnableLambda(extract_metrics) |    # Step 2: Extract metrics\n",
    "    RunnableLambda(generate_summary)     # Step 3: Generate report\n",
    ")\n",
    "\n",
    "# Test the sequential chain with sample text\n",
    "sample_text = \"Data engineering is crucial for ML success. It involves building robust pipelines. Quality data leads to better models.\"\n",
    "result = analysis_chain.invoke(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Parallel Execution with RunnableParallel\n",
    "\n",
    "### Concurrent Multi-Analysis Pipeline\n",
    "\n",
    "This advanced example demonstrates how to perform multiple types of analysis simultaneously on the same input. This is particularly useful for extracting different insights from text data in parallel, significantly improving performance compared to sequential processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Analysis Results:\n",
      "  technical_analysis: {'technical_terms': ['data', 'platform', 'architecture', 'pipeline', 'model'], 'technical_density': 27.77777777777778}\n",
      "  sentiment: Positive\n",
      "  entities: ['The', 'Databricks']\n",
      "  word_count: 18\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for parallel processing\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "import re\n",
    "\n",
    "# Analysis Function 1: Technical Content Analysis\n",
    "def analyze_technical_content(text):\n",
    "    \"\"\"Analyze technical aspects of text and calculate technical density\"\"\"\n",
    "    tech_terms = ['data', 'platform', 'architecture', 'pipeline', 'model', 'algorithm']\n",
    "    found_terms = [term for term in tech_terms if term.lower() in text.lower()]\n",
    "    technical_density = len(found_terms) / len(text.split()) * 100\n",
    "    return {\n",
    "        \"technical_terms\": found_terms,\n",
    "        \"technical_density\": technical_density\n",
    "    }\n",
    "\n",
    "# Analysis Function 2: Sentiment Analysis\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Simple rule-based sentiment analysis\"\"\"\n",
    "    positive_words = ['good', 'great', 'excellent', 'success', 'efficient', 'robust']\n",
    "    negative_words = ['bad', 'poor', 'failed', 'problem', 'issue', 'difficult']\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    pos_count = sum(1 for word in positive_words if word in text.lower())\n",
    "    neg_count = sum(1 for word in negative_words if word in text.lower())\n",
    "    \n",
    "    # Determine overall sentiment\n",
    "    if pos_count > neg_count:\n",
    "        return \"Positive\"\n",
    "    elif neg_count > pos_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Analysis Function 3: Entity Extraction\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract potential named entities using simple pattern matching\"\"\"\n",
    "    # Find capitalized words (potential proper nouns/entities)\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "    return list(set(entities))  # Remove duplicates\n",
    "\n",
    "# Create a parallel analysis pipeline that runs all analyses simultaneously\n",
    "# All functions receive the same input text and execute concurrently\n",
    "parallel_analyzer = RunnableParallel(\n",
    "    technical_analysis=RunnableLambda(analyze_technical_content),  # Technical analysis\n",
    "    sentiment=RunnableLambda(analyze_sentiment),                   # Sentiment analysis\n",
    "    entities=RunnableLambda(extract_entities),                     # Entity extraction\n",
    "    word_count=RunnableLambda(lambda x: len(x.split()))            # Simple word count\n",
    ")\n",
    "\n",
    "# Test the parallel execution with sample text\n",
    "sample_text = \"Databricks provides an excellent platform for building robust data pipelines. The architecture supports efficient model training and deployment.\"\n",
    "\n",
    "# Execute all analyses in parallel\n",
    "result = parallel_analyzer.invoke(sample_text)\n",
    "\n",
    "# Display results from all parallel analyses\n",
    "print(\"Parallel Analysis Results:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conditional Logic with RunnableBranch\n",
    "\n",
    "### Intelligent Query Routing System\n",
    "\n",
    "RunnableBranch enables dynamic routing of inputs based on conditions. This example demonstrates how to create an intelligent system that routes different types of technical queries to specialized handlers, similar to how a support system might route tickets to different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I optimize Spark pipelines for better performance?\n",
      "Response: 🔧 Data Engineering Response: This query about 'How do I optimize Spark pipelines for better performance?' relates to building and maintaining data pipelines and infrastructure.\n",
      "\n",
      "Query: What's the best algorithm for classification problems?\n",
      "Response: 🤖 ML Response: This query about 'What's the best algorithm for classification problems?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: How to deploy models on Azure ML?\n",
      "Response: 🤖 ML Response: This query about 'How to deploy models on Azure ML?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: What's the weather like today?\n",
      "Response: 💡 General Response: This is a general query about 'What's the weather like today?'. Please provide more specific context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for conditional logic\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "# Condition Functions: These determine which branch to take\n",
    "def is_data_engineering_query(text):\n",
    "    \"\"\"Check if query is about data engineering topics\"\"\"\n",
    "    de_keywords = ['pipeline', 'etl', 'data warehouse', 'spark', 'kafka', 'airflow']\n",
    "    return any(keyword in text.lower() for keyword in de_keywords)\n",
    "\n",
    "def is_ml_query(text):\n",
    "    \"\"\"Check if query is about machine learning topics\"\"\"\n",
    "    ml_keywords = ['model', 'algorithm', 'training', 'prediction', 'classification', 'regression']\n",
    "    return any(keyword in text.lower() for keyword in ml_keywords)\n",
    "\n",
    "def is_cloud_query(text):\n",
    "    \"\"\"Check if query is about cloud platforms and services\"\"\"\n",
    "    cloud_keywords = ['aws', 'azure', 'gcp', 'cloud', 'kubernetes', 'docker']\n",
    "    return any(keyword in text.lower() for keyword in cloud_keywords)\n",
    "\n",
    "# Handler Functions: These process queries based on their type\n",
    "def handle_data_engineering(text):\n",
    "    \"\"\"Specialized handler for data engineering queries\"\"\"\n",
    "    return f\"🔧 Data Engineering Response: This query about '{text}' relates to building and maintaining data pipelines and infrastructure.\"\n",
    "\n",
    "def handle_ml_query(text):\n",
    "    \"\"\"Specialized handler for ML queries\"\"\"\n",
    "    return f\"🤖 ML Response: This query about '{text}' involves machine learning models and algorithms.\"\n",
    "\n",
    "def handle_cloud_query(text):\n",
    "    \"\"\"Specialized handler for cloud queries\"\"\"\n",
    "    return f\"☁️ Cloud Response: This query about '{text}' concerns cloud platforms and services.\"\n",
    "\n",
    "def handle_general_query(text):\n",
    "    \"\"\"Default handler for general queries that don't match specific categories\"\"\"\n",
    "    return f\"💡 General Response: This is a general query about '{text}'. Please provide more specific context.\"\n",
    "\n",
    "# Create conditional routing system using RunnableBranch\n",
    "# Structure: (condition, handler) pairs, with default handler at the end\n",
    "query_router = RunnableBranch(\n",
    "    (RunnableLambda(is_data_engineering_query), RunnableLambda(handle_data_engineering)),\n",
    "    (RunnableLambda(is_ml_query), RunnableLambda(handle_ml_query)),\n",
    "    (RunnableLambda(is_cloud_query), RunnableLambda(handle_cloud_query)),\n",
    "    RunnableLambda(handle_general_query)  # Default case when no conditions match\n",
    ")\n",
    "\n",
    "# Test the routing system with different types of queries\n",
    "test_queries = [\n",
    "    \"How do I optimize Spark pipelines for better performance?\",  # Data Engineering\n",
    "    \"What's the best algorithm for classification problems?\",      # Machine Learning\n",
    "    \"How to deploy models on Azure ML?\",                          # Cloud (contains both ML and cloud keywords)\n",
    "    \"What's the weather like today?\"                              # General (no specific keywords)\n",
    "]\n",
    "\n",
    "# Execute routing for each test query\n",
    "for query in test_queries:\n",
    "    result = query_router.invoke(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Streaming Processing\n",
    "\n",
    "### Real-time Data Processing with Streaming\n",
    "\n",
    "Streaming allows you to process data incrementally and yield results as they become available. This is particularly useful for long-running processes or when you want to provide progressive updates to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming processing:\n",
      "  Processed: BUILDING\n",
      "  Processed: BUILDING SCALABLE\n",
      "  Processed: BUILDING SCALABLE DATA\n",
      "  Processed: BUILDING SCALABLE DATA SOLUTIONS\n"
     ]
    }
   ],
   "source": [
    "# Import required modules for streaming\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import time\n",
    "\n",
    "def streaming_processor(text):\n",
    "    \"\"\"Process text word by word and yield incremental results\"\"\"\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    # Process each word incrementally\n",
    "    for word in words:\n",
    "        processed_words.append(word.upper())\n",
    "        \n",
    "        # Simulate processing time (remove in production)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Yield intermediate result after processing each word\n",
    "        yield f\"Processed: {' '.join(processed_words)}\"\n",
    "\n",
    "# Create a streaming runnable from the generator function\n",
    "streaming_runnable = RunnableLambda(streaming_processor)\n",
    "\n",
    "# Execute streaming processing - results are yielded incrementally\n",
    "print(\"Streaming processing:\")\n",
    "for chunk in streaming_runnable.stream(\"building scalable data solutions\"):\n",
    "    print(f\"  {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing\n",
    "\n",
    "### Efficient Processing of Multiple Inputs\n",
    "\n",
    "Batch processing allows you to process multiple inputs simultaneously, which is more efficient than processing them one by one. This is particularly useful when you need to analyze large volumes of data or multiple queries at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Analysis Results:\n",
      "  Query 1: {'query': 'Hello', 'word_count': 1, 'complexity': 'Simple'}\n",
      "  Query 2: {'query': 'How do I setup Databricks?', 'word_count': 5, 'complexity': 'Medium'}\n",
      "  Query 3: {'query': 'What are the best practices fo...', 'word_count': 21, 'complexity': 'Complex'}\n",
      "  Query 4: {'query': 'Spark optimization tips', 'word_count': 3, 'complexity': 'Simple'}\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for batch processing\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def analyze_query_complexity(query):\n",
    "    \"\"\"Analyze the complexity of a user query based on word count\"\"\"\n",
    "    word_count = len(query.split())\n",
    "    char_count = len(query)\n",
    "    \n",
    "    # Categorize complexity based on word count\n",
    "    if word_count <= 3:\n",
    "        complexity = \"Simple\"\n",
    "    elif word_count <= 10:\n",
    "        complexity = \"Medium\"\n",
    "    else:\n",
    "        complexity = \"Complex\"\n",
    "    \n",
    "    return {\n",
    "        \"query\": query[:30] + \"...\" if len(query) > 30 else query,  # Truncate long queries\n",
    "        \"word_count\": word_count,\n",
    "        \"complexity\": complexity\n",
    "    }\n",
    "\n",
    "# Create analyzer runnable\n",
    "complexity_analyzer = RunnableLambda(analyze_query_complexity)\n",
    "\n",
    "# Prepare multiple queries for batch processing\n",
    "queries = [\n",
    "    \"Hello\",                                    # Simple query\n",
    "    \"How do I setup Databricks?\",              # Medium query\n",
    "    \"What are the best practices for designing a data lake architecture that can handle both batch and streaming data processing workloads?\",  # Complex query\n",
    "    \"Spark optimization tips\"                   # Simple query\n",
    "]\n",
    "\n",
    "# Batch process all queries simultaneously\n",
    "# This is more efficient than processing them one by one\n",
    "results = complexity_analyzer.batch(queries)\n",
    "\n",
    "# Display results from batch processing\n",
    "print(\"Batch Analysis Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  Query {i}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Examples with LLM Integration\n",
    "\n",
    "### Building Production-Ready AI Workflows\n",
    "\n",
    "This section demonstrates how to combine all the concepts we've learned to build sophisticated AI workflows that integrate custom processing with Large Language Models. These examples show real-world patterns for building production-ready AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Custom Data Processing with LLM Integration\n",
    "\n",
    "This example demonstrates how to clean and format user input before sending it to the LLM. Data preprocessing is crucial for getting consistent, high-quality responses from AI models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Approach 1: Dictionary Input Processing\n",
    "\n",
    "This approach processes dictionary input, useful when your function expects structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In this approach, the `clean_user_input()` function expects a dictionary input with a \"topic\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'apache spark'}\n",
      "AI Response: Apache Spark is an open-source, distributed computing system that is designed for big data processing and analytics. It provides a fast and general-purpose cluster computing framework for large-scale data processing tasks. Spark is known for its speed and ease of use, as well as its ability to handle a wide range of workloads, including batch processing, real-time streaming, machine learning, and graph processing.\n",
      "\n",
      "Spark uses a concept called Resilient Distributed Datasets (RDDs) to store and process data across multiple nodes in a cluster. RDDs are fault-tolerant, immutable collections of objects that can be operated on in parallel. Spark also provides a rich set of APIs in multiple programming languages, such as Scala, Java, Python, and R, making it accessible to a wide range of developers.\n",
      "\n",
      "One of the key features of Apache Spark is its ability to perform in-memory processing, which allows it to achieve high performance by caching data in memory and reusing it across multiple computations. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n",
      "\n",
      "Overall, Apache Spark is a powerful and versatile tool for processing large volumes of data efficiently and quickly, making it a popular choice for organizations looking to harness the power of big data analytics.\n"
     ]
    }
   ],
   "source": [
    "# Import string output parser for clean text responses\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Function that processes dictionary input\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    clean_text = text[\"topic\"].strip().lower()  # Extract and clean the topic\n",
    "    return {\"topic\": clean_text}\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "\n",
    "# Test the cleaner with dictionary input\n",
    "print(\"Cleaned output:\", input_cleaner.invoke({\"topic\": \"  APACHE SPARK  \"}))\n",
    "\n",
    "# Create a complete AI chain with input cleaning\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = input_cleaner | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the complete chain with dictionary input\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke({\"topic\": user_input})\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Direct String Input Processing\n",
    "\n",
    "This approach processes string input directly, which is more common when dealing with simple text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache spark\n",
      "AI Response: Apache Spark is an open-source, distributed computing system that is designed for big data processing and analytics. It provides a fast and general-purpose cluster computing framework for large-scale data processing. Spark allows users to write applications in Java, Scala, Python, and R, and provides high-level APIs in these languages for ease of use.\n",
      "\n",
      "One of the key features of Apache Spark is its ability to perform in-memory processing, which allows it to process data much faster than traditional disk-based systems. Spark also supports a wide range of data processing tasks, including batch processing, real-time streaming, machine learning, and graph processing.\n",
      "\n",
      "Spark is built around the concept of Resilient Distributed Datasets (RDDs), which are fault-tolerant collections of data that can be operated on in parallel across a cluster of machines. Spark automatically distributes the data and computation across the cluster, making it easy to scale up and down as needed.\n",
      "\n",
      "Overall, Apache Spark is a powerful and flexible tool for processing and analyzing large volumes of data, making it a popular choice for organizations looking to harness the power of big data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import string output parser for clean text responses\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Function that processes string input directly\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    return text.strip().lower()  # Remove whitespace and convert to lowercase\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "\n",
    "# Test the cleaner with string input\n",
    "print(\"Cleaned output:\", input_cleaner.invoke(\"  APACHE SPARK  \"))\n",
    "\n",
    "# Create a complete AI chain with input cleaning\n",
    "# Note: We use dictionary syntax to map the cleaned input to the \"topic\" key\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = {\"topic\": input_cleaner} | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the complete chain with string input\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke(user_input)\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Context Preservation with RunnablePassthrough\n",
    "\n",
    "This example demonstrates how to preserve the original input context while simultaneously processing it through other functions. This pattern is essential for maintaining context in complex AI workflows where you need both the original data and processed insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: How do I build data pipelines using Spark for cloud architecture?\n",
      "Keywords found: ['data', 'spark', 'pipeline', 'cloud', 'architecture']\n",
      "AI Analysis: To build data pipelines using Spark for cloud architecture, you can leverage the capabilities of Spark's distributed processing framework to efficiently process and analyze large volumes of data in a cloud environment. \n",
      "\n",
      "First, you would need to design a pipeline that outlines the flow of data from source to destination, including any transformations or processing steps along the way. Spark provides a high-level API for building data pipelines, allowing you to easily define and execute complex data processing workflows.\n",
      "\n",
      "Next, you can deploy your Spark application on a cloud platform such as A...\n"
     ]
    }
   ],
   "source": [
    "# Function to extract technical keywords from text\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract key technical terms from text\"\"\"\n",
    "    technical_terms = ['data', 'spark', 'pipeline', 'model', 'cloud', 'architecture']\n",
    "    words = text.lower().split()\n",
    "    found_terms = [term for term in technical_terms if term in ' '.join(words)]\n",
    "    return found_terms\n",
    "\n",
    "# Create a prompt template that uses both original text and extracted keywords\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Original query: \"{original_text}\"\n",
    "Technical terms found: {keywords}\n",
    "\n",
    "Provide a focused technical explanation based on these key terms.\n",
    "\"\"\")\n",
    "\n",
    "# Create a chain that preserves original input and adds keyword analysis\n",
    "# RunnablePassthrough preserves the original text\n",
    "# RunnableLambda extracts keywords from the same text\n",
    "context_chain = {\n",
    "    \"original_text\": RunnablePassthrough(),      # Preserve original input\n",
    "    \"keywords\": RunnableLambda(extract_keywords)    # Extract keywords\n",
    "} | analysis_prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the context preservation chain\n",
    "tech_query = \"How do I build data pipelines using Spark for cloud architecture?\"\n",
    "result = context_chain.invoke(tech_query)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Original: {tech_query}\")\n",
    "print(f\"Keywords found: {extract_keywords(tech_query)}\")\n",
    "print(f\"AI Analysis: {result[:600]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chaining with Pipe Operator (|)\n",
    "The pipe operator creates sequential workflows where data flows from left to right through multiple processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: 1. Partitioning: Ensure that your data is properly partitioned to distribute the workload evenly across the cluster. Use the `repartition()` or `coale...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: The best algorithm for classification can vary depending on the specific characteristics of the dataset and the problem at hand. Some commonly used al...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: To deploy on Azure Kubernetes Service (AKS) as a cloud architect, you would follow these steps:\n",
      "\n",
      "1. Create an Azure account: If you don't already have...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules for sequential chaining\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def categorize_query(text):\n",
    "    \"\"\"Categorize technical queries based on keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in ['spark', 'hadoop', 'etl', 'pipeline']):\n",
    "        return \"Data Engineering\"\n",
    "    elif any(word in text_lower for word in ['model', 'algorithm', 'training']):\n",
    "        return \"Machine Learning\"\n",
    "    elif any(word in text_lower for word in ['azure', 'aws', 'cloud']):\n",
    "        return \"Cloud Platform\"\n",
    "    return \"General\"\n",
    "\n",
    "def create_expert_prompt(data):\n",
    "    \"\"\"Create specialized prompts based on category\"\"\"\n",
    "    category = data['category']\n",
    "    question = data['question']\n",
    "    \n",
    "    expert_prompts = {\n",
    "        \"Data Engineering\": f\"As a data engineering expert, provide technical guidance for: {question}\",\n",
    "        \"Machine Learning\": f\"As an ML specialist, explain the concepts related to: {question}\",\n",
    "        \"Cloud Platform\": f\"As a cloud architect, describe the solution for: {question}\",\n",
    "        \"General\": f\"Provide a comprehensive answer to: {question}\"\n",
    "    }\n",
    "    \n",
    "    return expert_prompts[category]\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    RunnableLambda(lambda x: {\"question\": x, \"category\": categorize_query(x)}) |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: 1. Partitioning: Ensure that your data is properly partitioned before running a Spark job. This helps in distributing the workload evenly across the c...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: The best algorithm for classification depends on various factors such as the nature of the data, the size of the dataset, the complexity of the proble...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: To deploy on Azure Kubernetes Service (AKS) as a cloud architect, you would follow these steps:\n",
      "\n",
      "1. Create an Azure account: If you don't already have...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to create the chain\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    {\"question\": RunnablePassthrough(),\"category\": RunnableLambda(categorize_query)} |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing with RunnableParallel\n",
    "RunnableParallel executes multiple operations simultaneously on the same input, perfect for extracting different types of information concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Simple word count function\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of this text (positive/negative/neutral): {text}\"\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Provide a one-sentence summary of: {text}\"\n",
    ")\n",
    "\n",
    "# Create parallel analysis chains\n",
    "sentiment_chain = sentiment_prompt | chatgpt | StrOutputParser()\n",
    "summary_chain = summary_prompt | chatgpt | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Databricks provides excellent tools for building scalable data pipelines and machine learning solutions efficiently.\"\n",
    "# RunnableLambda(count_words).invoke({\"text\": sample_text})\n",
    "\n",
    "RunnableLambda(count_words).invoke( sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand RunnableLambda\n",
    "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def sum(a):\n",
    "    return a + 3\n",
    "\n",
    "print(RunnableLambda(sum).invoke(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "print(RunnableLambda(add_one).invoke(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
     ]
    }
   ],
   "source": [
    "print(type(RunnableLambda(add_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for elem in RunnableLambda(add_one).stream(2):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for elem in RunnableLambda(add_one).batch([1,2,3]):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand RunnablePassthrough\n",
    "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin': 1, 'modified': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "print(runnable.invoke(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return prompt+\" completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parsed': 'noitelpmoc olleh'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain that combines a fake LLM with text reversal\n",
    "# Step 1: RunnableLambda(fake_llm) - Wraps our fake LLM function\n",
    "# Step 2: {'parsed': lambda text: text[::-1]} - Creates a dictionary with a key 'parsed'\n",
    "#         that contains a lambda function to reverse the text\n",
    "# The pipe operator (|) chains these operations together\n",
    "chain = RunnableLambda(fake_llm) | {'parsed': lambda text: text[::-1]}\n",
    "\n",
    "# Invoke the chain with input \"hello\"\n",
    "# This will: 1) Pass \"hello\" to fake_llm -> returns \"hello completion\"\n",
    "#           2) Pass \"hello completion\" to the lambda function -> reverses it to \"noitelpmoc olleh\"\n",
    "#           3) Return result as {'parsed': 'noitelpmoc olleh'}\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'hello completion', 'parsed': 'noitelpmoc olleh'}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] # Reverse the output\n",
    "}\n",
    "\n",
    "print(chain.invoke('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of above code\n",
    "\n",
    "1. RunnableLambda(fake_llm) Converts the fake_llm function into a LangChain Runnable. This makes it compatible with LangChain pipelines\n",
    "2. This pipes the output of RunnableLambda(fake_llm) into the next processing step.\n",
    "3. The output of fake_llm (\"completion\") is passed into both\n",
    "    * 'original': RunnablePassthrough(), which simply forwards the input as is.\n",
    "    * 'parsed': lambda text: text[::-1], which reverses the output string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input 'hello' is passed into fake_llm\n",
    "    * fake_llm('hello')  # Returns \"completion\"\n",
    "2. \"completion\" is passed into the dictionary processor, which does:\n",
    "    * 'original': Keeps \"completion\" as is.\n",
    "    * 'parsed': Reverses \"completion\" to \"noitelpmoc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello completion'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableLambda(fake_llm).invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Assign in RunnablePassThrough\n",
    "# In some cases, it may be useful to pass the input through while adding some keys to the output. In this case, you can use the assign method.\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "This notebook covered the fundamental concepts of LangChain Runnables and how to build sophisticated AI workflows:\n",
    "\n",
    "#### Core Runnable Types:\n",
    "1. **RunnablePassthrough**: Preserves input unchanged - useful for maintaining context\n",
    "2. **RunnableLambda**: Wraps Python functions for custom processing\n",
    "3. **RunnableParallel**: Executes multiple operations simultaneously\n",
    "4. **RunnableBranch**: Enables conditional logic and routing\n",
    "\n",
    "#### Key Patterns:\n",
    "- **Sequential Processing**: Using the pipe operator (`|`) to chain operations\n",
    "- **Parallel Processing**: Executing multiple analyses simultaneously\n",
    "- **Context Preservation**: Maintaining original input while processing\n",
    "- **Conditional Routing**: Directing inputs to specialized handlers\n",
    "\n",
    "#### Best Practices:\n",
    "- Use parallel processing for independent operations to improve performance\n",
    "- Preserve context when building complex workflows\n",
    "- Implement proper input validation and cleaning\n",
    "- Design modular, reusable components\n",
    "- Combine custom logic with LLM capabilities effectively\n",
    "\n",
    "#### Real-world Applications:\n",
    "- Content analysis and categorization\n",
    "- Technical query routing systems\n",
    "- Data preprocessing pipelines\n",
    "- Multi-stage AI workflows\n",
    "- Batch processing of large datasets\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand the fundamentals of LangChain Runnables, you can:\n",
    "- Build more complex multi-agent systems\n",
    "- Implement sophisticated data processing pipelines\n",
    "- Create intelligent routing systems\n",
    "- Develop production-ready AI applications\n",
    "\n",
    "**Remember**: The power of LangChain Runnables lies in their composability - you can combine these simple building blocks to create highly sophisticated AI workflows that solve real-world problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
