{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîî LangChain Callbacks Deep Dive\n",
    "\n",
    "Callbacks are a powerful mechanism in LangChain that allow you to **hook into various stages** of your LLM application's execution. They enable:\n",
    "\n",
    "- üìä **Monitoring** - Track token usage, costs, and performance\n",
    "- üêõ **Debugging** - Inspect prompts, responses, and intermediate steps\n",
    "- üìù **Logging** - Record events for audit trails and analytics\n",
    "- ‚ö° **Streaming** - Handle real-time token-by-token output\n",
    "- üîÑ **Custom Actions** - Trigger side effects during execution\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Built-in Callbacks](#built-in-callbacks)\n",
    "3. [Custom Callback Handlers](#custom-callback-handlers)\n",
    "4. [Streaming Callbacks](#streaming-callbacks)\n",
    "5. [Cost & Token Tracking](#cost-tracking)\n",
    "6. [Advanced: Logging to File](#file-logging)\n",
    "7. [Async Callbacks](#async-callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "\n",
    "First, let's load our environment variables and import the necessary modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "from uuid import UUID\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler, get_openai_callback\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"built-in-callbacks\"></a>\n",
    "## 2. Built-in Callbacks\n",
    "\n",
    "LangChain provides several built-in callback handlers. The most common is `StdOutCallbackHandler` which prints execution details to the console.\n",
    "\n",
    "### 2.1 StdOutCallbackHandler (Using Modern LCEL)\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: We're using LCEL (LangChain Expression Language) with the pipe `|` operator instead of the deprecated `LLMChain`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "üì§ Final Result:\n",
      "What do you call a line of rabbits hopping backward? \n",
      "\n",
      "A receding hare-line!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# Create a chain using LCEL (modern approach)\n",
    "chain = prompt | llm\n",
    "\n",
    "# Use StdOutCallbackHandler to see execution details\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "# Invoke with callbacks in config\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"rabbits\"}, \n",
    "    config={\"callbacks\": [handler]}\n",
    ")\n",
    "\n",
    "print(\"\\nüì§ Final Result:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"custom-callback-handlers\"></a>\n",
    "## 3. Custom Callback Handlers\n",
    "\n",
    "You can create custom handlers by extending `BaseCallbackHandler`. This gives you fine-grained control over what happens at each stage of execution.\n",
    "\n",
    "### Available Callback Methods\n",
    "\n",
    "| Method | Triggered When |\n",
    "|--------|----------------|\n",
    "| `on_llm_start` | LLM begins processing |\n",
    "| `on_llm_end` | LLM finishes generating |\n",
    "| `on_llm_error` | LLM encounters an error |\n",
    "| `on_chain_start` | Chain begins execution |\n",
    "| `on_chain_end` | Chain completes |\n",
    "| `on_chain_error` | Chain encounters an error |\n",
    "| `on_llm_new_token` | New token generated (streaming) |\n",
    "\n",
    "### 3.1 Comprehensive Custom Handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Chain Started with inputs: {'topic': 'programming'}\n",
      "\n",
      "üîó Chain Started with inputs: {'topic': 'programming'}\n",
      "üîó Chain Completed\n",
      "==================================================\n",
      "üöÄ LLM STARTED\n",
      "‚è∞ Time: 16:40:40\n",
      "üìù Prompt: Human: Tell me a joke about programming...\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "‚úÖ LLM COMPLETED\n",
      "‚è±Ô∏è  Duration: 2.01s\n",
      "üìä Tokens - Input: 13, Output: 12, Total: 25\n",
      "==================================================\n",
      "üîó Chain Completed\n",
      "\n",
      "üì§ Response:\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "class DetailedCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"A comprehensive callback handler that tracks all major events.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        \n",
    "    def on_llm_start(\n",
    "        self, \n",
    "        serialized: Dict[str, Any], \n",
    "        prompts: List[str], \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Called when LLM starts processing.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üöÄ LLM STARTED\")\n",
    "        print(f\"‚è∞ Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"üìù Prompt: {prompts[0][:100]}...\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        \"\"\"Called when LLM finishes generating.\"\"\"\n",
    "        elapsed = time.time() - self.start_time if self.start_time else 0\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"‚úÖ LLM COMPLETED\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {elapsed:.2f}s\")\n",
    "        \n",
    "        # Extract token usage if available\n",
    "        if response.llm_output:\n",
    "            token_usage = response.llm_output.get('token_usage', {})\n",
    "            print(f\"üìä Tokens - Input: {token_usage.get('prompt_tokens', 'N/A')}, \"\n",
    "                  f\"Output: {token_usage.get('completion_tokens', 'N/A')}, \"\n",
    "                  f\"Total: {token_usage.get('total_tokens', 'N/A')}\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs) -> None:\n",
    "        \"\"\"Called when LLM encounters an error.\"\"\"\n",
    "        print(f\"‚ùå LLM ERROR: {error}\")\n",
    "    \n",
    "    def on_chain_start(\n",
    "        self, \n",
    "        serialized: Dict[str, Any], \n",
    "        inputs: Dict[str, Any], \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Called when chain starts.\"\"\"\n",
    "        print(f\"\\nüîó Chain Started with inputs: {inputs}\")\n",
    "    \n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Called when chain completes.\"\"\"\n",
    "        print(f\"üîó Chain Completed\")\n",
    "\n",
    "\n",
    "# Test the detailed handler\n",
    "detailed_handler = DetailedCallbackHandler()\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"programming\"},\n",
    "    config={\"callbacks\": [detailed_handler]}\n",
    ")\n",
    "\n",
    "print(f\"\\nüì§ Response:\\n{result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"streaming-callbacks\"></a>\n",
    "## 4. Streaming Callbacks\n",
    "\n",
    "Streaming allows you to receive tokens as they're generated, creating a more responsive user experience. The `on_llm_new_token` callback is essential for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Streaming Response:\n",
      "\n",
      "Why did the robot go on a diet?\n",
      "\n",
      "Because it had too many bytes!\n",
      "\n",
      "‚úÖ Streaming complete! Generated 18 tokens.\n"
     ]
    }
   ],
   "source": [
    "class StreamingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Handler for streaming tokens as they're generated.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.token_count = 0\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \"\"\"Called for each new token generated.\"\"\"\n",
    "        self.tokens.append(token)\n",
    "        self.token_count += 1\n",
    "        # Print token without newline for streaming effect\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        \"\"\"Called when generation completes.\"\"\"\n",
    "        print(f\"\\n\\n‚úÖ Streaming complete! Generated {self.token_count} tokens.\")\n",
    "\n",
    "\n",
    "# Create a streaming-enabled LLM\n",
    "streaming_llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "streaming_chain = prompt | streaming_llm\n",
    "\n",
    "# Create handler and invoke\n",
    "stream_handler = StreamingCallbackHandler()\n",
    "\n",
    "print(\"üé¨ Streaming Response:\\n\")\n",
    "result = streaming_chain.invoke(\n",
    "    {\"topic\": \"artificial intelligence\"},\n",
    "    config={\"callbacks\": [stream_handler]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cost-tracking\"></a>\n",
    "## 5. Cost & Token Tracking\n",
    "\n",
    "LangChain provides a convenient `get_openai_callback` context manager to track API usage and costs automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üìä USAGE REPORT\n",
      "==================================================\n",
      "üî¢ Total Tokens:      101\n",
      "   ‚îú‚îÄ Prompt Tokens:  39\n",
      "   ‚îî‚îÄ Output Tokens:  62\n",
      "üí∞ Total Cost:        $0.000043\n",
      "üìû Successful Calls:  3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Track costs and token usage with context manager\n",
    "with get_openai_callback() as cb:\n",
    "    # Run multiple calls to accumulate costs\n",
    "    result1 = chain.invoke({\"topic\": \"cats\"})\n",
    "    result2 = chain.invoke({\"topic\": \"dogs\"})\n",
    "    result3 = chain.invoke({\"topic\": \"birds\"})\n",
    "\n",
    "# Print comprehensive usage report\n",
    "print(\"=\" * 50)\n",
    "print(\"üìä USAGE REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üî¢ Total Tokens:      {cb.total_tokens:,}\")\n",
    "print(f\"   ‚îú‚îÄ Prompt Tokens:  {cb.prompt_tokens:,}\")\n",
    "print(f\"   ‚îî‚îÄ Output Tokens:  {cb.completion_tokens:,}\")\n",
    "print(f\"üí∞ Total Cost:        ${cb.total_cost:.6f}\")\n",
    "print(f\"üìû Successful Calls:  {cb.successful_requests}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the cat sitting on the computer? \\n\\nBecause it wanted to keep an eye on the mouse!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_11f3029f6b', 'id': 'chatcmpl-ClvMITmw9c74UtC9xpufcXH1Svj3A', 'finish_reason': 'stop', 'logprobs': None}, id='run-00db6cf9-00f3-4a5c-a346-c9c954bf2b0d-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"file-logging\"></a>\n",
    "## 6. Advanced: Logging to File\n",
    "\n",
    "For production applications, you'll often want to log callback events to a file for auditing and debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "class FileLoggingHandler(BaseCallbackHandler):\n",
    "    \"\"\"Logs all LLM events to a JSON file for auditing.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str = \"llm_logs.json\"):\n",
    "        self.log_file = log_file\n",
    "        self.logs = []\n",
    "        \n",
    "    def _log_event(self, event_type: str, data: dict):\n",
    "        \"\"\"Helper to log events with timestamp.\"\"\"\n",
    "        event = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"event_type\": event_type,\n",
    "            \"data\": data\n",
    "        }\n",
    "        self.logs.append(event)\n",
    "        \n",
    "        # Append to file\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(json.dumps(event) + \"\\n\")\n",
    "    \n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self._log_event(\"llm_start\", {\n",
    "            \"prompts\": prompts,\n",
    "            \"model\": serialized.get(\"kwargs\", {}).get(\"model_name\", \"unknown\")\n",
    "        })\n",
    "    \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        token_usage = {}\n",
    "        if response.llm_output:\n",
    "            token_usage = response.llm_output.get(\"token_usage\", {})\n",
    "        \n",
    "        self._log_event(\"llm_end\", {\n",
    "            \"response\": response.generations[0][0].text if response.generations else \"\",\n",
    "            \"token_usage\": token_usage\n",
    "        })\n",
    "    \n",
    "    def on_llm_error(self, error: Exception, **kwargs):\n",
    "        self._log_event(\"llm_error\", {\"error\": str(error)})\n",
    "\n",
    "\n",
    "# Test file logging\n",
    "file_handler = FileLoggingHandler(\"llm_audit_log.json\")\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"space exploration\"},\n",
    "    config={\"callbacks\": [file_handler]}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Response generated and logged!\")\n",
    "print(f\"üìÅ Logs written to: llm_audit_log.json\")\n",
    "print(f\"\\nüìÑ Latest log entry:\")\n",
    "print(json.dumps(file_handler.logs[-1], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"async-callbacks\"></a>\n",
    "## 7. Async Callbacks\n",
    "\n",
    "For high-performance applications, you can use async callback handlers with `AsyncCallbackHandler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "class AsyncStreamHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async handler for non-blocking streaming.\"\"\"\n",
    "    \n",
    "    async def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \"\"\"Handle each new token asynchronously.\"\"\"\n",
    "        print(token, end=\"\", flush=True)\n",
    "        # Simulate async work (e.g., sending to websocket)\n",
    "        await asyncio.sleep(0.01)\n",
    "    \n",
    "    async def on_llm_start(self, serialized, prompts, **kwargs) -> None:\n",
    "        print(\"üöÄ Async LLM started...\\n\")\n",
    "    \n",
    "    async def on_llm_end(self, response, **kwargs) -> None:\n",
    "        print(\"\\n\\n‚úÖ Async streaming complete!\")\n",
    "\n",
    "\n",
    "async def run_async_example():\n",
    "    \"\"\"Run async streaming example.\"\"\"\n",
    "    async_handler = AsyncStreamHandler()\n",
    "    async_llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "    async_chain = prompt | async_llm\n",
    "    \n",
    "    result = await async_chain.ainvoke(\n",
    "        {\"topic\": \"quantum computing\"},\n",
    "        config={\"callbacks\": [async_handler]}\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Run the async example\n",
    "print(\"üé¨ Async Streaming Demo:\\n\")\n",
    "await run_async_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combining Multiple Callbacks\n",
    "\n",
    "You can use multiple callback handlers simultaneously to achieve different purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple handlers for different purposes\n",
    "timing_handler = DetailedCallbackHandler()  # For timing and debug info\n",
    "log_handler = FileLoggingHandler(\"combined_logs.json\")  # For persistent logs\n",
    "\n",
    "# Use multiple handlers together\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"machine learning\"},\n",
    "    config={\"callbacks\": [timing_handler, log_handler]}\n",
    ")\n",
    "\n",
    "print(f\"\\nüì§ Final response:\\n{result.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "| Callback Type | Use Case | Key Method |\n",
    "|---------------|----------|------------|\n",
    "| `StdOutCallbackHandler` | Quick debugging | Built-in |\n",
    "| `BaseCallbackHandler` | Custom sync handlers | `on_llm_start`, `on_llm_end`, etc. |\n",
    "| `AsyncCallbackHandler` | Non-blocking async ops | `async on_llm_*` methods |\n",
    "| `get_openai_callback` | Cost/token tracking | Context manager |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use LCEL** (`prompt | llm`) instead of deprecated `LLMChain`\n",
    "2. **Callbacks are composable** - use multiple handlers for different concerns\n",
    "3. **Streaming callbacks** provide real-time UX with `on_llm_new_token`\n",
    "4. **Always track costs** in production using `get_openai_callback`\n",
    "5. **Log to files** for auditing and debugging in production\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [LangChain Callbacks Documentation](https://python.langchain.com/docs/concepts/callbacks/)\n",
    "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
